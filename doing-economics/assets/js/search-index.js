index.addDoc({
    id: 0,
    title: "Doing Economics: Half-title",
    content: "The CORE team Doing Economics Empirical Projects"
});
index.addDoc({
    id: 1,
    title: "Doing Economics: Title page",
    content: "The CORE team Doing Economics"
});
index.addDoc({
    id: 2,
    title: "Doing Economics: Copyright",
    content: "Copyright Doing Economics Text © The CORE team ISBN (Digital download): 978-1-5272-1209-1 ISBN (Digital online): 978-1-5272-4375-0 All rights reserved. No part of this book may be reproduced or transmitted in any form or by any electronic or mechanical means, including photocopying and recording, or any other information storage or retrieval system, without written permission from the publisher."
});
index.addDoc({
    id: 3,
    title: "Doing Economics: Contents",
    content: "Contents Preface A note to instructors Producing Doing Economics List of resources 1 Measuring climate change Learning objectives Introduction Working in Excel Part 1.1 The behaviour of average surface temperature over time Part 1.2 Variation in temperature over time Part 1.3 Carbon emissions and the environment Working in R Getting started in R Part 1.1 The behaviour of average surface temperature over time Part 1.2 Variation in temperature over time Part 1.3 Carbon emissions and the environment Working in Google Sheets Part 1.1 The behaviour of average surface temperature over time Part 1.2 Variation in temperature over time Part 1.3 Carbon emissions and the environment Solutions Part 1.1 The behaviour of average surface temperature over time Part 1.2 Variation in temperature over time Part 1.3 Carbon emissions and the environment 2 Collecting and analysing data from experiments Learning objectives Introduction Working in Excel Part 2.1 Collecting data by playing a public goods game Part 2.2 Describing the data Part 2.3 Did changing the rules of the game affect behaviour? Working in R Getting started in R Part 2.1 Collecting data by playing a public goods game Part 2.2 Describing the data Part 2.3 Did changing the rules of the game affect behaviour? Working in Google Sheets Part 2.1 Collecting data by playing a public goods game Part 2.2 Describing the data Part 2.3 Did changing the rules of the game affect behaviour? Solutions Part 2.1 Collecting data by playing a public goods game Part 2.2 Describing the data Part 2.3 Did changing the rules of the game affect behaviour? 3 Measuring the effect of a sugar tax Learning objectives Introduction Working in Excel Excel-specific learning objectives Part 3.1 Before-and-after comparisons of retail prices Part 3.2 Before-and-after comparisons with prices in other areas Working in R Getting started in R Part 3.1 Before-and-after comparisons of retail prices Part 3.2 Before-and-after comparisons with prices in other areas Working in Google Sheets Google Sheets-specific learning objectives Part 3.1 Before-and-after comparisons of retail prices Part 3.2 Before-and-after comparisons with prices in other areas Solutions Part 3.1 Before-and-after comparisons of retail prices Part 3.2 Before-and-after comparisons with prices in other areas 4 Measuring wellbeing Learning objectives Introduction Working in Excel Excel-specific learning objectives Part 4.1 GDP and its components as a measure of material wellbeing Part 4.2 The HDI as a measure of wellbeing Working in R R-specific learning objectives Getting started in R Part 4.1 GDP and its components as a measure of material wellbeing Part 4.2 The HDI as a measure of wellbeing Working in Google Sheets Google Sheets-specific learning objectives Part 4.1 GDP and its components as a measure of material wellbeing Part 4.2 The HDI as a measure of wellbeing Solutions Part 4.1 GDP and its components as a measure of material wellbeing Part 4.2 The HDI as a measure of wellbeing 5 Measuring inequality: Lorenz curves and Gini coefficients Learning objectives Introduction Working in Excel Part 5.1 Measuring income inequality Part 5.2 Measuring other kinds of inequality Working in R R-specific learning objectives Getting started in R Part 5.1 Measuring income inequality Part 5.2 Measuring other kinds of inequality Working in Google Sheets Part 5.1 Measuring income inequality Part 5.2 Measuring other kinds of inequality Solutions Part 5.1 Measuring income inequality Part 5.2 Measuring other kinds of inequality 6 Measuring management practices Learning objectives Introduction Working in Excel Part 6.1 Looking for patterns in the survey data Part 6.2 Are differences in management practices statistically significant? Part 6.3 What factors affect the quality of management? Working in R Getting started in R Part 6.1 Looking for patterns in the survey data Part 6.2 Are differences in management practices statistically significant? Part 6.3 What factors affect the quality of management? Working in Google Sheets Part 6.1 Looking for patterns in the survey data Part 6.2 Are differences in management practices statistically significant? Part 6.3 What factors affect the quality of management? Solutions Part 6.1 Looking for patterns in the survey data Part 6.2 Are differences in management practices statistically significant? Part 6.3 What factors affect the quality of management? 7 Supply and demand Learning objectives Introduction Working in Excel Part 7.1 Drawing supply and demand diagrams Part 7.2 Interpreting supply and demand curves Working in R Getting started in R Part 7.1 Drawing supply and demand diagrams Part 7.2 Interpreting supply and demand curves Working in Google Sheets Part 7.1 Drawing supply and demand diagrams Part 7.2 Interpreting supply and demand curves Solutions Part 7.1 Drawing supply and demand diagrams Part 7.2 Interpreting supply and demand curves 8 Measuring the non-monetary cost of unemployment Learning objectives Introduction Working in Excel Part 8.1 Cleaning and summarizing the data Part 8.2 Visualizing the data Part 8.3 Confidence intervals for difference in the mean Working in R Getting started in R Part 8.1 Cleaning and summarizing the data Part 8.2 Visualizing the data Part 8.3 Confidence intervals for difference in the mean Working in Google Sheets Part 8.1 Cleaning and summarizing the data Part 8.2 Visualizing the data Part 8.3 Confidence intervals for difference in the mean Solutions Part 8.1 Cleaning and summarizing the data Part 8.2 Visualizing the data Part 8.3 Confidence intervals for difference in the mean 9 Credit-excluded households in a developing country Learning objectives Introduction Working in Excel Excel-specific learning objectives Part 9.1 Households that did not get a loan Part 9.2 Households that got a loan Working in R Getting started in R Part 9.1 Households that did not get a loan Part 9.2 Households that got a loan Working in Google Sheets Google Sheets-specific learning objectives Part 9.1 Households that did not get a loan Part 9.2 Households that got a loan Solutions Part 9.1 Households that did not get a loan Part 9.2 Households that got a loan 10 Characteristics of banking systems around the world Learning objectives Introduction Working in Excel Part 10.1 Summarizing the data Part 10.2 Comparing financial stability before and after the 2008 global financial crisis Working in R Getting started in R Part 10.1 Summarizing the data Part 10.2 Comparing financial stability before and after the 2008 global financial crisis Working in Google Sheets Part 10.1 Summarizing the data Part 10.2 Comparing financial stability before and after the 2008 global financial crisis Solutions Part 10.1 Summarizing the data Part 10.2 Comparing financial stability before and after the 2008 global financial crisis 11 Measuring willingness to pay for climate change mitigation Learning objectives Introduction Working in Excel Part 11.1 Summarizing the data Part 11.2 Comparing willingness to pay across methods and individual characteristics Working in R Getting started in R Part 11.1 Summarizing the data Part 11.2 Comparing willingness to pay across methods and individual characteristics Working in Google Sheets Part 11.1 Summarizing the data Part 11.2 Comparing willingness to pay across methods and individual characteristics Solutions Part 11.1 Summarizing the data Part 11.2 Comparing willingness to pay across methods and individual characteristics 12 Government policies and popularity: Hong Kong cash handout Learning objectives Introduction Working in Excel Excel-specific learning objectives Part 12.1 Inequality Part 12.2 Government popularity Working in R Getting started in R Part 12.1 Inequality Part 12.2 Government popularity Working in Google Sheets Google Sheets-specific learning objectives Part 12.1 Inequality Part 12.2 Government popularity Solutions Part 12.1 Inequality Part 12.2 Government popularity Technical Reference Glossary Bibliography Copyright acknowledgements"
});
index.addDoc({
    id: 4,
    title: "Doing Economics: Preface",
    content: "Preface Welcome to Doing Economics. Our aim is to introduce students to the art, practice, and excitement of using data to understand economics and policy analysis. Doing Economics is a unique empirical resource created by a worldwide collaboration of economists and social scientists. As with all projects created by the CORE Team, it is free, and open-access. We designed it primarily to complement our books Economy, Society, and Public Policy and The Economy, but you can use it in any way you choose: if you are studying on your own and want to learn new skills, or you are a teacher who wants to apply it to a course in which the quantitative understanding of policy plays a role, or you are a student and think this is a skill that will help with your course or future employment, you have an open invitation to try or adapt any of these projects. Doing Economics gives you the opportunity to gain hands-on experience with real-world data in areas of pressing importance to contemporary societies. If you are interested in environmental sustainability, inequality and wellbeing, or policies to address public health problems such as obesity, you will find something relevant. In our experience, students are drawn to study economics precisely because they are interested in these topics—but leave disappointed by abstract and theoretical courses. In recent years, students, citizens, and teachers have all complained loudly about the gap between what is commonly taught in economics and these important and fascinating empirical questions. This deficiency is not solely academic. Policymakers, journalists, and decision-makers who fail to understand how economics can be employed in the real world are ill-equipped to address the major economic challenges we face. Doing Economics seeks to close this gap. It has a set of empirical projects based on carefully curated data sets and publicly available data. Each project takes students on a step-by-step journey of investigation using easily-available software. Three tracks are available—using a spreadsheet application (Excel or Google Sheets), or a programming language (R). Through Doing Economics, students gain a first-hand appreciation of why they are learning economics, and the relevance and power of the economics they have learned. They gain a valuable toolkit of data handling, data cleaning, software and statistical skills that they can transfer to other courses they might take and, afterwards, to the workplace. Having looked at many different ways in which data is collected, and the problems of finding a good fit between data and the economic question at hand, as well as having worked with the data, users of Doing Economics will be more critical consumers of data, and more appreciative of the efforts of researchers and statistical agencies collecting and analysing it. The expanding CORE team of educators, researchers, students, and teachers that produced these resources believes that economic understanding and data literacy should be taught to as many people as possible, and that the democracies we inhabit benefit from a population that is not easily confused and misled by data. That is why we created Doing Economics as a standalone, free resource. We thank the Nuffield Foundation for financial support and all of the contributors to CORE around the world. As with all CORE resources, we depend on our users for improvements. Please send your corrections and suggestions. We dedicate Doing Economics to the memory of James Lincoln of Manchester University, who generously contributed to this project and passionately helped his students to wrestle with data … never losing sight of the joy of doing so. The CORE Team January 2020"
});
index.addDoc({
    id: 5,
    title: "Doing Economics: A note to instructors",
    content: "Note to instructors Target audience Our target audience includes: students at undergraduate and postgraduate level who are not taking economics as a major subject anyone who wants to learn how to use economics to understand and articulate reasoned views on some of the most pressing policy problems facing our societies: inequality, financial instability, the future of work, wealth creation, and environmental degradation anyone who wants practical training in understanding and using data to measure the economy and policy effectiveness anyone interested in social and economic policy, who is taking a degree related to policy, or is hoping to have a policy-related job in the future. Prerequisites no prior courses in economics or statistics are required no knowledge of statistical programs (such as R, Excel, or Google Sheets) is required, except a familiarity with the interface and how to enter and clear data familiarity with basic mathematical operations, percentages, decimals, 2-D graphs. The purpose of the empirical projects provide hands-on experience, using real-world data, to investigate important policy problems strengthen the link between real-world phenomena and economic concepts and models help students to develop skills that are transferable to other courses and to the workplace. The structure of the empirical projects Empirical projects are designed to be completed in Excel, R, or Google Sheets. Each project contains: clearly defined learning objectives related to statistical/economic concepts and data presentation an introduction to the project and the economic topics it addresses two to three parts, each containing multiple questions on a specific subtopic. Unless indicated otherwise, these can be done independently, or at the same time step-by-step walk-throughs for conceptually difficult or challenging tasks in Excel, R, and Google Sheets. These are videos and annotated screenshots (Excel and Google Sheets) and code with explanations (R). The walk-throughs are designed so that beginners in Excel, R, or Google Sheets will be able to learn the skills required to complete the project. Students who come to these projects without any prior experience with the software tools should start with Project 1 as this will provide the easiest entry to the required software skills. This is essential for those using R. Solutions for all empirical projects are available, and the walk-throughs will also help students confirm that they have got the correct output. There are brief notes for some of the more interpretive questions. Note that these are not model answers. They are included to help students, including those doing the project outside a formal class, to check their progress working through the steps using Excel, Google Sheets, or R. Students taking courses using Doing Economics should follow the guidance of their instructors. Instructors are encouraged to develop additional tasks that help students critically evaluate data sources and definitions. Contact us if you’d like to share these with other users of CORE resources. How to use the empirical projects Each empirical project in Doing Economics is divided into two or three parts. Each part can be completed independently, or together (unless specified otherwise). The time needed to complete one part will depend on the structure and pace of the course, though as a rough guide, one project can be a term-long assignment. In each part, students will be guided through the steps to produce the charts or tables that can form the basis of a report. The projects can be done independently, since the same key concepts are repeated in a number of projects. Each project has an introduction with information about the concepts that are prerequisites, for that project, as well as those that will be introduced in the project. The empirical projects can be used to supplement units in Economy, Society, and Public Policy and The Economy, although this is not essential. Each project contains information about the unit to which the material is related, and has links to sections in the ebooks that may help students to understand the project. ESPP and Doing Economics as part of a connected curriculum If you are teaching a social science, engineering, business studies, or public policy program in which students have to take an economics course and a quantitative methods course, you can use ESPP and Doing Economics empirical projects to connect these parts of the curriculum. ESPP Unit Title Doing Economics 1 Capitalism: Affluence, inequality, and the environment Empirical Project 1: Measuring climate change (datasets: Goddard Institute for Space Studies temperature data; US National Oceanic and Atmospheric Administration CO2 data) 2 Social interactions and economic outcomes Empirical Project 2: Collecting and analysing data from experiments (datasets: student-generated experimental data; Hermann et al. 2008) 3 Public policy for fairness and efficiency. Empirical Project 3: Measuring the effect of a sugar tax (datasets: Global Food Research Program’s Berkeley Store Price Survey; Silver et al. 2017) 4 Work, wellbeing, and scarcity. Empirical Project 4: Measuring wellbeing (datasets: UN GDP data; Human Development Index) 5 Institutions, power, and inequality Empirical Project 5: Measuring inequality: Lorenz curves and Gini coefficients (datasets: the Global Consumption and Income Project’s income data; Chartbook of Economic Inequality; Our world in data) 6 The firm: Employees, managers, and owners Empirical Project 6: Measuring management practices (dataset: World Management Survey) 7 Firms and markets for goods and services Empirical Project 7: Supply and demand (dataset: US market for watermelons (1930–1951); taken from Stewart (2018)) 8 The labour market and the product market: Unemployment and inequality Empirical Project 8: Measuring the non-monetary cost of unemployment (dataset: European Values Study) 9 The credit market: Borrowers, lenders, and the rate of interest Empirical Project 9: Credit-excluded households in a developing country (dataset: Ethiopian Socioeconomic Survey) 10 Banks, money, housing, and financial assets Empirical Project 10: Characteristics of banking systems around the world (dataset: World Bank Global Financial Development Database) 11 Market successes and failures Empirical Project 11: Measuring willingness to pay for climate change mitigation (dataset: German survey data, taken from Uehleke (2016)) 12 Governments and markets in a democratic society Empirical Project 12: Government policies and popularity: Hong Kong cash handout (datasets: University of Hong Kong Public Opinion Programme and the Hong Kong poverty situation report (published by the Hong Kong Census and Statistics Department))"
});
index.addDoc({
    id: 6,
    title: "Doing Economics: Producing Doing Economics",
    content: "Producing Doing Economics Doing Economics was conceived of and coordinated by Eileen Tipoe, University of Oxford and researcher for the CORE EQuSS project, and co-authored with Ralf Becker, University of Manchester, who devised the ‘Working in R’ sections of the book. Ralf was assisted by the late James Lincoln, University of Manchester, in preparing the R walk-throughs. Tim Phillips of the CORE project created the Excel video walk-throughs. Oliver Yimeng Zhang drafted the project solutions, and Clemens Blab drafted the Google Sheets walk-throughs. They were advised by the empirical projects working group, a team of experts in the teaching and use of economic and social data. Stella Yarrow was the project manager. Contributors Peter Backus, Lucy Barnes, Wendy Carlin, Wing Chan, Carlos Cortinhas, Marion Dumas, Stefania Paredes Fuentes, Thaana Galia, Georg von Graevenitz, Arthur Grimes, David Hope, Girol Karacaoglu, Dunli Li, Deborah Mabbett, Marta Martinez Matute, Anand Murugesan, Michael Muthukrishna, Mary O’Mahony, Mark Schaffer, Christian Spielmann, Margaret Stevens, Guglielmo Volpe, Stephen Wright. Editorial, design, and software development Doing Economics is produced and maintained by the Electric Book Works team: Christina Tromp, Derika van Biljon, Louise Steward, Klara Skinner, Nazley Samsodien, Alexandra Turner, Alison Paulin, Dione Mentis, Jennifer Jacobs, Lauren Ellwood, and Arthur Attwell. Funding <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/nuffield-green.png'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/nuffield-green-320.png 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/nuffield-green-640.png 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/nuffield-green-1024.png 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/nuffield-green.png 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt='' '' /> The project has been funded by the Nuffield Foundation, but the views expressed are those of the authors and not necessarily the Foundation. Visit www.nuffieldfoundation.org."
});
index.addDoc({
    id: 7,
    title: "Doing Economics: Empirical Project 1: Measuring climate change",
    content: "Empirical Project 1 Measuring climate change Learning objectives In this project you will: use charts and summary measures to discuss the extent of climate change and its possible causes use line charts to describe the behaviour of real-world variables over time (Part 1.1) summarize data in a frequency table, and visualize distributions with column charts (Part 1.2) describe a distribution using mean and variance (Part 1.2) use scatterplots and the correlation coefficient to assess the degree of association between two variables (Part 1.3) explain what correlation measures and the limitations of correlation (Part 1.3). Key concepts Concepts needed for this project: mean, median, and decile. Concepts introduced in this project: variance, frequency table, correlation and correlation coefficient, causation, and spurious correlation. Introduction CORE projects This empirical project is related to material in: Unit 1 of Economy, Society, and Public Policy Unit 1 and Unit 20 of The Economy. Climate change is one of the effects of the rapid economic growth that has occurred in most countries since the Industrial Revolution. It is an important issue for policymaking, since governments need to assess how serious the problem is and then decide how to mitigate it. To find out more about climate change and its effects, visit the Met Office’s webpage. Suppose you are a policy advisor for a small island nation. The government would like to know more about the extent of climate change and its possible causes. They ask you the following questions: How can we tell whether climate change is actually happening or not? If it is real, how can we measure the extent of climate change and determine what is causing it? To answer the first question, we look at the behaviour of environmental variables over time to see whether there are general patterns in environmental conditions that could be indicative of climate change. In this project, we focus on temperature-related variables. To answer the second question, we examine the degree of association between temperature and another variable, CO2 emissions, and consider whether there is a plausible relationship between the two, or whether there are other explanations for what we observe. Working in Excel Working in R Working in Google Sheets"
});
index.addDoc({
    id: 8,
    title: "Doing Economics: Empirical Project 1: Working in Excel",
    content: "Empirical Project 1 Working in Excel Part 1.1 The behaviour of average surface temperature over time Learning objectives for this part use line charts to describe the behaviour of real-world variables over time. In the questions below, we look at data from NASA about land-ocean temperature anomalies in the northern hemisphere. Figure 1.1 is constructed using this data, and shows temperatures in the northern hemisphere over the period 1880–2016, expressed as differences from the average temperature from 1951 to 1980. We start by creating charts similar to Figure 1.1, in order to visualize the data and spot patterns more easily. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Northern hemisphere temperatures (1880–2016). '' /> Figure 1.1 Northern hemisphere temperatures (1880–2016). Before plotting any charts, download the data and make sure you understand how temperature is measured: Go to NASA’s Goddard Institute for Space Studies website. Under the subheading ‘Combined Land-Surface Air and Sea-Surface Water Temperature Anomalies’, select the CSV version of ‘Northern Hemisphere-mean monthly, seasonal, and annual means’ (right-click and select ‘Save Link As…’). The default name of this file is NH.Ts+dSST.csv. Give it a suitable name and save it in an easily accessible location, such as a folder on your Desktop or in your personal folder. In this dataset, temperature is measured as ‘anomalies’ rather than absolute temperature. Using NASA’s Frequently Asked Questions section as a reference, explain in your own words what temperature ‘anomalies’ means. Why have researchers chosen this particular measure over other measures (such as absolute temperature)? Now create some line charts using monthly, seasonal, and annual data, which help us look for general patterns over time. Choose one month and plot a line chart with average temperature anomaly on the vertical axis and time (from 1880 to the latest year available) on the horizontal axis. Label each axis appropriately and give your chart a suitable title (Refer to Figure 1.1 as an example.) Excel walk-through 1.1 Drawing a line chart of temperature and time <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to draw a line chart of temperature and time. '' /> Figure 1.2 How to draw a line chart of temperature and time. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the temperature data looks like. Column A has time (in years), Column B has temperature deviations, and Column C contains the average northern hemisphere temperature. We will be using Columns A and B to make the line chart. '' /> The data This is what the temperature data looks like. Column A has time (in years), Column B has temperature deviations, and Column C contains the average northern hemisphere temperature. We will be using Columns A and B to make the line chart. Figure 1.2a This is what the temperature data looks like. Column A has time (in years), Column B has temperature deviations, and Column C contains the average northern hemisphere temperature. We will be using Columns A and B to make the line chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a line chart : After completing step 4, your line chart will look similar to this. Temperature deviation is on the vertical axis and time is on the horizontal axis. Notice that the numbers for time are not correct (they should be years). '' /> Draw a line chart After completing step 4, your line chart will look similar to this. Temperature deviation is on the vertical axis and time is on the horizontal axis. Notice that the numbers for time are not correct (they should be years). Figure 1.2b After completing step 4, your line chart will look similar to this. Temperature deviation is on the vertical axis and time is on the horizontal axis. Notice that the numbers for time are not correct (they should be years). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis variable to years : To change the horizontal axis labels to years, we need to add the values in Column A to the line chart. '' /> Change the horizontal axis variable to years To change the horizontal axis labels to years, we need to add the values in Column A to the line chart. Figure 1.2c To change the horizontal axis labels to years, we need to add the values in Column A to the line chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis variable to years : The current horizontal axis labels are the numbers 1, 2, 3, and so on. To change these labels to years, we need to edit the labels. '' /> Change the horizontal axis variable to years The current horizontal axis labels are the numbers 1, 2, 3, and so on. To change these labels to years, we need to edit the labels. Figure 1.2d The current horizontal axis labels are the numbers 1, 2, 3, and so on. To change these labels to years, we need to edit the labels. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis variable to years : After completing step 10, the horizontal axis will be in time (years). '' /> Change the horizontal axis variable to years After completing step 10, the horizontal axis will be in time (years). Figure 1.2e After completing step 10, the horizontal axis will be in time (years). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Reposition the horizontal axis : After completing step 12, the horizontal axis will be at the bottom of the chart. '' /> Reposition the horizontal axis After completing step 12, the horizontal axis will be at the bottom of the chart. Figure 1.2f After completing step 12, the horizontal axis will be at the bottom of the chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-02-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add titles : Give the axes and the chart appropriate titles. '' /> Add titles Give the axes and the chart appropriate titles. Figure 1.2g Give the axes and the chart appropriate titles. Extra practice: The columns labelled ‘DJF’, ‘MAM’, ‘JJA’, and ‘SON’ contain seasonal averages (means). For example, the ‘MAM’ column contains the average of the March, April, and May columns for each year. Plot a separate line chart for each season, using average temperature anomaly for that season on the vertical axis and time (from 1880 to the latest year available) on the horizontal axis. The column labelled ‘J–D’ contains the average temperature anomaly for each year. Plot a line chart with annual average temperature anomaly on the vertical axis and time (from 1880 to the latest year available) on the horizontal axis. Your chart should look like Figure 1.1. Extension: Add a horizontal line that intersects the vertical axis at 0, and label it ‘1951–1980 average’. What do your charts from Questions 2 to 4(a) suggest about the relationship between temperature and time? Excel walk-through 1.2 Plotting a line chart and adding a horizontal line <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to plot a line chart and add a horizontal line. '' /> Figure 1.3 How to plot a line chart and add a horizontal line. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the data looks like. Column A contains years (1880–Present), Columns B to M contain monthly temperature anomalies, and Columns N to S contain the mean temperature anomaly for the specified months (e.g. DJF is the mean over Dec, Jan, and Feb). We are going to draw a line chart for the January to December mean (Column N). '' /> The data This is what the data looks like. Column A contains years (1880–Present), Columns B to M contain monthly temperature anomalies, and Columns N to S contain the mean temperature anomaly for the specified months (e.g. DJF is the mean over Dec, Jan, and Feb). We are going to draw a line chart for the January to December mean (Column N). Figure 1.3a This is what the data looks like. Column A contains years (1880–Present), Columns B to M contain monthly temperature anomalies, and Columns N to S contain the mean temperature anomaly for the specified months (e.g. DJF is the mean over Dec, Jan, and Feb). We are going to draw a line chart for the January to December mean (Column N). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a line chart of mean temperature anomaly over time : After completing step 3, your line chart will look similar to this. Temperature deviation is on the vertical axis and time is on the horizontal axis. Notice that the numbers for time are not correct (they should be years). '' /> Draw a line chart of mean temperature anomaly over time After completing step 3, your line chart will look similar to this. Temperature deviation is on the vertical axis and time is on the horizontal axis. Notice that the numbers for time are not correct (they should be years). Figure 1.3b After completing step 3, your line chart will look similar to this. Temperature deviation is on the vertical axis and time is on the horizontal axis. Notice that the numbers for time are not correct (they should be years). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add a horizontal line showing the 1951–1980 average : To add a horizontal line, we will create a new variable (called ‘1951–1980 average’, shown in Column T) and add it to our line chart. This variable will have the value 0 so it will show up as a horizontal line on the chart. '' /> Add a horizontal line showing the 1951–1980 average To add a horizontal line, we will create a new variable (called ‘1951–1980 average’, shown in Column T) and add it to our line chart. This variable will have the value 0 so it will show up as a horizontal line on the chart. Figure 1.3c To add a horizontal line, we will create a new variable (called ‘1951–1980 average’, shown in Column T) and add it to our line chart. This variable will have the value 0 so it will show up as a horizontal line on the chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add a horizontal line showing the 1951–1980 average : Now, we have to add the values in Column T as a new series in the line chart. '' /> Add a horizontal line showing the 1951–1980 average Now, we have to add the values in Column T as a new series in the line chart. Figure 1.3d Now, we have to add the values in Column T as a new series in the line chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add a horizontal line showing the 1951–1980 average : The section on the left lists all the data series that are currently plotted on the chart as vertical axis values. We need to add the values in Column T to this list, so that a horizontal line with vertical axis values of 0 will appear. '' /> Add a horizontal line showing the 1951–1980 average The section on the left lists all the data series that are currently plotted on the chart as vertical axis values. We need to add the values in Column T to this list, so that a horizontal line with vertical axis values of 0 will appear. Figure 1.3e The section on the left lists all the data series that are currently plotted on the chart as vertical axis values. We need to add the values in Column T to this list, so that a horizontal line with vertical axis values of 0 will appear. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add a horizontal line showing the 1951–1980 average : The following box will pop up. You need to fill the two spaces with information about the new series. The first space is for the series name, and the second space is for the cells containing the vertical axis values. '' /> Add a horizontal line showing the 1951–1980 average The following box will pop up. You need to fill the two spaces with information about the new series. The first space is for the series name, and the second space is for the cells containing the vertical axis values. Figure 1.3f The following box will pop up. You need to fill the two spaces with information about the new series. The first space is for the series name, and the second space is for the cells containing the vertical axis values. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add a horizontal line showing the 1951–1980 average : The cells in Column T contain the vertical axis values we need to add to the chart, so we need to select them all. '' /> Add a horizontal line showing the 1951–1980 average The cells in Column T contain the vertical axis values we need to add to the chart, so we need to select them all. Figure 1.3g The cells in Column T contain the vertical axis values we need to add to the chart, so we need to select them all. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-h.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-h-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-h-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-h-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-h.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add a horizontal line showing the 1951–1980 average : After step 11, if both spaces in the box have been filled with the correct information, a horizontal line will appear on the chart. '' /> Add a horizontal line showing the 1951–1980 average After step 11, if both spaces in the box have been filled with the correct information, a horizontal line will appear on the chart. Figure 1.3h After step 11, if both spaces in the box have been filled with the correct information, a horizontal line will appear on the chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-i.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-i-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-i-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-i-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-i.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis variable to time (in years) : There are two remaining things to change in the chart. First, the temperature series (Column N) needs to be given an appropriate name. (This name will show in the chart legend). Second, the horizontal axis labels should be changed to the years 1880–2016. '' /> Change the horizontal axis variable to time (in years) There are two remaining things to change in the chart. First, the temperature series (Column N) needs to be given an appropriate name. (This name will show in the chart legend). Second, the horizontal axis labels should be changed to the years 1880–2016. Figure 1.3i There are two remaining things to change in the chart. First, the temperature series (Column N) needs to be given an appropriate name. (This name will show in the chart legend). Second, the horizontal axis labels should be changed to the years 1880–2016. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-j.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-j-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-j-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-j-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-j.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis variable to time (in years) : After step 15, if the correct cells have been selected, the horizontal axis labels will change to the years in your data. '' /> Change the horizontal axis variable to time (in years) After step 15, if the correct cells have been selected, the horizontal axis labels will change to the years in your data. Figure 1.3j After step 15, if the correct cells have been selected, the horizontal axis labels will change to the years in your data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-k.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-k-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-k-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-k-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-k.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis variable to time (in years) : After step 16, the box should look like this, with appropriate names for each data series (shown on the left), and correct horizontal axis values (shown on the right). '' /> Change the horizontal axis variable to time (in years) After step 16, the box should look like this, with appropriate names for each data series (shown on the left), and correct horizontal axis values (shown on the right). Figure 1.3k After step 16, the box should look like this, with appropriate names for each data series (shown on the left), and correct horizontal axis values (shown on the right). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-l.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-l-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-l-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-l-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-l.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Label the different lines on the line chart : After step 18, there will be a legend containing the names of each data series, in your chosen place on the chart. '' /> Label the different lines on the line chart After step 18, there will be a legend containing the names of each data series, in your chosen place on the chart. Figure 1.3l After step 18, there will be a legend containing the names of each data series, in your chosen place on the chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-m.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-m-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-m-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-m-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-03-m.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add titles and reposition the horizontal axis : By default, the horizontal axis is positioned at the vertical axis value of 0. To move it to the bottom of the chart (as in Figure 1.1), we have to change the axis position. After step 23, your chart will look similar to Figure 1.1. '' /> Add titles and reposition the horizontal axis By default, the horizontal axis is positioned at the vertical axis value of 0. To move it to the bottom of the chart (as in Figure 1.1), we have to change the axis position. After step 23, your chart will look similar to Figure 1.1. Figure 1.3m By default, the horizontal axis is positioned at the vertical axis value of 0. To move it to the bottom of the chart (as in Figure 1.1), we have to change the axis position. After step 23, your chart will look similar to Figure 1.1. You now have charts for three different time intervals: month (Question 2), season (Question 3), and year (Question 4). For each time interval, discuss what we can learn about patterns in temperature over time that we might not be able to learn from the charts of other time intervals. Compare your chart from Question 4 to Figure 1.4 which also shows the behaviour of temperature over time using data taken from the National Academy of Sciences. Discuss the similarities and differences between the charts. (For example, are the horizontal and vertical axes variables the same, or do the lines have the same shape?) Looking at the behaviour of temperature over time from 1000 to 1900 in Figure 1.4, are the observed patterns in your chart unusual? Based on your answers to Questions 4 and 5, do you think the government should be concerned about climate change? <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Northern hemisphere temperatures over the long run (1000–2006). '' /> Figure 1.4 Northern hemisphere temperatures over the long run (1000–2006). Part 1.2 Variation in temperature over time Learning objectives for this part summarize data in a frequency table, and visualize distributions with column charts describe a distribution using mean and variance. Aside from changes in the mean temperature, the government is also worried that climate change will result in more frequent extreme weather events. The island has experienced a few major storms and severe heat waves in the past, both of which caused serious damage and disruption to economic activity. Will weather become more extreme and vary more as a result of climate change? A New York Times article uses the same temperature dataset you have been using to investigate the distribution of temperatures and temperature variability over time. Read through the article, paying close attention to the descriptions of the temperature distributions. We can use the mean and median to describe distributions, and we can use deciles to describe parts of distributions. To visualize distributions, we can use column charts in Excel. (For some practice on using these concepts and creating column charts in Excel, see Section 1.3 of Economy, Society, and Public Policy). We are now going to create similar charts of temperature distributions to the ones in the New York Times article, and look at different ways of summarizing distributions. frequency tableA record of how many observations in a dataset have a particular value, range of values, or belong to a particular category. In order to create a column chart using the temperature data we have, we first need to summarize the data using a frequency table. Instead of using deciles to group the data, we use intervals of 0.05, so that temperature anomalies with a value from −0.3 to −0.025 will be in one group, a value greater than −0.025 up until 0.02 in another group, and so on. The frequency table shows us how many values belong to a particular group. Using the monthly data for June, July, and August (columns G to I in your spreadsheet), create two frequency tables similar to Figure 1.5 for the years 1951–1980 and 1981–2010, respectively. The values in the first column should range from −0.3 to 1.05, in intervals of 0.05. Range of temperature anomaly (T) Frequency −0.30 −0.25 … 1.00 1.05 Figure 1.5 A frequency table. Excel walk-through 1.3 Creating a frequency table Follow the walk-through in the CORE video, or in Figure 1.6, to find out how to make a frequency table in Excel. How to make a frequency table <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create a table : How to create a frequency table in Excel. '' /> Create a table Figure 1.6 How to create a frequency table in Excel. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create a table : In this example, we will make a frequency table for the years 1951–1980 in Columns A and B. It’s a good idea to put all the tables in a separate place from the data. '' /> Create a table In this example, we will make a frequency table for the years 1951–1980 in Columns A and B. It’s a good idea to put all the tables in a separate place from the data. Figure 1.6a In this example, we will make a frequency table for the years 1951–1980 in Columns A and B. It’s a good idea to put all the tables in a separate place from the data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create a table : After step 2, your table will look like Figure 1.5. '' /> Create a table After step 2, your table will look like Figure 1.5. Figure 1.6b After step 2, your table will look like Figure 1.5. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the data : It is easier to make a frequency table if you have filtered the data to show only the values you need for the table (the years 1951–1980 in this case). '' /> Filter the data It is easier to make a frequency table if you have filtered the data to show only the values you need for the table (the years 1951–1980 in this case). Figure 1.6c It is easier to make a frequency table if you have filtered the data to show only the values you need for the table (the years 1951–1980 in this case). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the data : After completing step 6, only the data for the selected years is shown in the spreadsheet. Data for the other years is still there, but it’s hidden. '' /> Filter the data After completing step 6, only the data for the selected years is shown in the spreadsheet. Data for the other years is still there, but it’s hidden. Figure 1.6d After completing step 6, only the data for the selected years is shown in the spreadsheet. Data for the other years is still there, but it’s hidden. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use the FREQUENCY function to fill in the rest of the table : Now that the data is filtered, we will use Excel’s FREQUENCY function to fill in Column B. First, select the cells that need to be filled in. '' /> Use the FREQUENCY function to fill in the rest of the table Now that the data is filtered, we will use Excel’s FREQUENCY function to fill in Column B. First, select the cells that need to be filled in. Figure 1.6e Now that the data is filtered, we will use Excel’s FREQUENCY function to fill in Column B. First, select the cells that need to be filled in. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use the FREQUENCY function to fill in the rest of the table : Excel will fill in the frequency table based on the values in the cells selected. '' /> Use the FREQUENCY function to fill in the rest of the table Excel will fill in the frequency table based on the values in the cells selected. Figure 1.6f Excel will fill in the frequency table based on the values in the cells selected. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-06-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use the FREQUENCY function to fill in the rest of the table : Note: The values you get will be slightly different to those shown here, because this station temperature data is slightly different. '' /> Use the FREQUENCY function to fill in the rest of the table Note: The values you get will be slightly different to those shown here, because this station temperature data is slightly different. Figure 1.6g Note: The values you get will be slightly different to those shown here, because this station temperature data is slightly different. Using the frequency tables from Question 1: Plot two separate column charts for 1951–1980 and 1981–2010 to show the distribution of temperatures, with frequency on the vertical axis and the range of temperature anomaly on the horizontal axis. Your charts should look similar to those in the New York Times article. Using your charts, describe the similarities and differences (if any) between the distributions of temperature anomalies in 1951–1980 and 1981–2010. varianceA measure of dispersion in a frequency distribution, equal to the mean of the squares of the deviations from the arithmetic mean of the distribution. The variance is used to indicate how ‘spread out’ the data is. A higher variance means that the data is more spread out. Example: The set of numbers 1, 1, 1 has zero variance (no variation), while the set of numbers 1, 1, 999 has a high variance of 221,334 (large spread). Now we will use our data to look at different aspects of distributions. First, we will learn how to use deciles to determine which observations are ‘normal’ and ‘abnormal’, and then learn how to use variance to describe the shape of a distribution. The New York Times article considers the bottom third (the lowest or coldest one-third) of temperature anomalies in 1951–1980 as ‘cold’ and the top third (the highest or hottest one-third) of anomalies as ‘hot’. In decile terms, temperatures in the 1st to 3rd decile are ‘cold’ and temperatures in the 7th to 10th decile or above are ‘hot’ (rounded to the nearest decile). Use Excel’s PERCENTILE.INC function to determine what values correspond to the 3rd and 7th decile, across all months in 1951–1980. Excel walk-through 1.4 Calculating percentiles Follow the walk-through in the CORE video, or in Figure 1.7, to find out how to calculate percentiles in Excel. How to calculate percentiles <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to use Excel’s PERCENTILE.INC function. '' /> Figure 1.7 How to use Excel’s PERCENTILE.INC function. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : We will be using the same data as in Excel walk-through 1.3. '' /> The data We will be using the same data as in Excel walk-through 1.3. Figure 1.7a We will be using the same data as in Excel walk-through 1.3. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use PERCENTILE.INC to get the value for the 3rd decile : The PERCENTILE.INC function will find the value corresponding to the chosen percentile in the cells you selected. The value 0.3 refers to the 30th percentile, also known as the 3rd decile. '' /> Use PERCENTILE.INC to get the value for the 3rd decile The PERCENTILE.INC function will find the value corresponding to the chosen percentile in the cells you selected. The value 0.3 refers to the 30th percentile, also known as the 3rd decile. Figure 1.7b The PERCENTILE.INC function will find the value corresponding to the chosen percentile in the cells you selected. The value 0.3 refers to the 30th percentile, also known as the 3rd decile. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-07-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use PERCENTILE.INC to get the value for the 7th decile : Note: The values you get may be slightly different to those shown here if you are using the latest data. '' /> Use PERCENTILE.INC to get the value for the 7th decile Note: The values you get may be slightly different to those shown here if you are using the latest data. Figure 1.7c Note: The values you get may be slightly different to those shown here if you are using the latest data. Based on the values you found in Question 3, count the number of anomalies that are considered ‘hot’ in 1981–2010, and express this as a percentage of all the temperature observations in that period. Does your answer suggest that we are experiencing hotter weather more frequently in 1981–2010? (Remember that each decile represents 10% of observations, so 30% of temperatures were considered ‘hot’ in 1951–1980.) Excel walk-through 1.5 Using Excel’s COUNTIF function <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to use Excel’s COUNTIF function. '' /> Figure 1.8 How to use Excel’s COUNTIF function. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the data, keeping the years 1981–2010 : We will be using the years 1981–2010 only. To make the data easier to view, we will filter the data so that only the years 1981–2010 are visible. Note: You should use the same data as you did in Excel walk-through 1.4. The values in your dataset may be slightly different to those shown here if you are using the latest data. '' /> Filter the data, keeping the years 1981–2010 We will be using the years 1981–2010 only. To make the data easier to view, we will filter the data so that only the years 1981–2010 are visible. Note: You should use the same data as you did in Excel walk-through 1.4. The values in your dataset may be slightly different to those shown here if you are using the latest data. Figure 1.8a We will be using the years 1981–2010 only. To make the data easier to view, we will filter the data so that only the years 1981–2010 are visible. Note: You should use the same data as you did in Excel walk-through 1.4. The values in your dataset may be slightly different to those shown here if you are using the latest data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the data, keeping the years 1981–2010 : Now only the data for your selected years are shown in the spreadsheet. Data for the other years is still there, but hidden. '' /> Filter the data, keeping the years 1981–2010 Now only the data for your selected years are shown in the spreadsheet. Data for the other years is still there, but hidden. Figure 1.8b Now only the data for your selected years are shown in the spreadsheet. Data for the other years is still there, but hidden. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use COUNTIF to get the number of cells with a value less than the 3rd decile of 1951–1980 : The COUNTIF function counts the number of cells you selected that satisfy a given condition (in this case, having a value less than or equal to the value of the 3rd decile in 1951–1980). '' /> Use COUNTIF to get the number of cells with a value less than the 3rd decile of 1951–1980 The COUNTIF function counts the number of cells you selected that satisfy a given condition (in this case, having a value less than or equal to the value of the 3rd decile in 1951–1980). Figure 1.8c The COUNTIF function counts the number of cells you selected that satisfy a given condition (in this case, having a value less than or equal to the value of the 3rd decile in 1951–1980). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use COUNTIF to get the number of cells with a value greater than the 7th decile of 1951–1980 : Now, the condition is that values should be greater than or equal to the value of the 7th decile in 1951–1980. '' /> Use COUNTIF to get the number of cells with a value greater than the 7th decile of 1951–1980 Now, the condition is that values should be greater than or equal to the value of the 7th decile in 1951–1980. Figure 1.8d Now, the condition is that values should be greater than or equal to the value of the 7th decile in 1951–1980. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-08-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use the numbers obtained to calculate percentages : COUNTIF gives us numbers, but to convert these into percentages we need to divide the numbers from COUNTIF by the total number of observations. Note: The values you get may be slightly different to those shown here if you are using the latest data. '' /> Use the numbers obtained to calculate percentages COUNTIF gives us numbers, but to convert these into percentages we need to divide the numbers from COUNTIF by the total number of observations. Note: The values you get may be slightly different to those shown here if you are using the latest data. Figure 1.8e COUNTIF gives us numbers, but to convert these into percentages we need to divide the numbers from COUNTIF by the total number of observations. Note: The values you get may be slightly different to those shown here if you are using the latest data. The New York Times article discusses whether temperatures have become more variable over time. One way to measure temperature variability is by calculating the variance of the temperature distribution. For each season (‘DJF’, ‘MAM’, ‘JJA’, and ‘SON’): Calculate the mean (average) and variance separately for the following time periods: 1921–1950, 1951–1980, and 1981–2010. For each season, compare the variances in different periods, and explain whether or not temperature appears to be more variable in later periods. Excel walk-through 1.6 Calculating and understanding the variance <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to calculate variance. '' /> Figure 1.9 How to calculate variance. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The temperature data : This data is temperature anomalies for 1981–2010, showing the months March to May only. The column chart shows what the data looks like. Each column shows how many values fall within the ranges shown on the horizontal axis. For example, the leftmost column tells us that 3 values are greater than 0.1 and less than or equal to 0.3. Note: The values in your dataset may be slightly different to those shown here, if you are using the latest data. '' /> The temperature data This data is temperature anomalies for 1981–2010, showing the months March to May only. The column chart shows what the data looks like. Each column shows how many values fall within the ranges shown on the horizontal axis. For example, the leftmost column tells us that 3 values are greater than 0.1 and less than or equal to 0.3. Note: The values in your dataset may be slightly different to those shown here, if you are using the latest data. Figure 1.9a This data is temperature anomalies for 1981–2010, showing the months March to May only. The column chart shows what the data looks like. Each column shows how many values fall within the ranges shown on the horizontal axis. For example, the leftmost column tells us that 3 values are greater than 0.1 and less than or equal to 0.3. Note: The values in your dataset may be slightly different to those shown here, if you are using the latest data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Made up data that is less spread out : This data on the right is made up. From this chart, you can see that the values are all quite close together, with the smallest value being 0.6 and the largest being 0.8. Comparing the charts, the real temperature data looks more spread out than the made up data. '' /> Made up data that is less spread out This data on the right is made up. From this chart, you can see that the values are all quite close together, with the smallest value being 0.6 and the largest being 0.8. Comparing the charts, the real temperature data looks more spread out than the made up data. Figure 1.9b This data on the right is made up. From this chart, you can see that the values are all quite close together, with the smallest value being 0.6 and the largest being 0.8. Comparing the charts, the real temperature data looks more spread out than the made up data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-09-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculating and interpreting the variance : The variance is a measure of how spread out the data is. Just looking at the charts, we would expect the real temperature data to have a higher variance than the made up data. Note: There is a similar function in Excel called VAR.S, which is used to calculate the variance for other types of data. For this temperature data though, we will use VAR.P. '' /> Calculating and interpreting the variance The variance is a measure of how spread out the data is. Just looking at the charts, we would expect the real temperature data to have a higher variance than the made up data. Note: There is a similar function in Excel called VAR.S, which is used to calculate the variance for other types of data. For this temperature data though, we will use VAR.P. Figure 1.9c The variance is a measure of how spread out the data is. Just looking at the charts, we would expect the real temperature data to have a higher variance than the made up data. Note: There is a similar function in Excel called VAR.S, which is used to calculate the variance for other types of data. For this temperature data though, we will use VAR.P. Using the findings of the New York Times article and your answers to Questions 1 to 5, discuss whether temperature appears to be more variable over time. Would you advise the government to spend more money on mitigating the effects of extreme weather events? Part 1.3 Carbon emissions and the environment Learning objectives for this part use scatterplots and the correlation coefficient to assess the degree of association between two variables explain what correlation measures and the limitations of correlation. correlationA measure of how closely related two variables are. Two variables are correlated if knowing the value of one variable provides information on the likely value of the other, for example high values of one variable being commonly observed along with high values of the other variable. Correlation can be positive or negative. It is negative when high values of one variable are observed with low values of the other. Correlation does not mean that there is a causal relationship between the variables. Example: When the weather is hotter, purchases of ice cream are higher. Temperature and ice cream sales are positively correlated. On the other hand, if purchases of hot beverages decrease when the weather is hotter, we say that temperature and hot beverage sales are negatively correlated.correlation coefficientA numerical measure, ranging between 1 and −1, of how closely associated two variables are—whether they tend to rise and fall together, or move in opposite directions. A positive coefficient indicates that when one variable takes a high (low) value, the other tends to be high (low) too, and a negative coefficient indicates that when one variable is high the other is likely to be low. A value of 1 or −1 indicates that knowing the value of one of the variables would allow you to perfectly predict the value of the other. A value of 0 indicates that knowing one of the variables provides no information about the value of the other. The government has heard that carbon emissions could be responsible for climate change, and has asked you to investigate whether this is the case. To do so, we are now going to look at carbon emissions over time, and use another type of chart (scatter charts) to show their relationship with temperature anomalies. One way to measure the relationship between two variables is correlation. Excel walk-through 1.7 explains what correlation is and how to calculate it in Excel. In the questions below, we will make charts using the CO2 data from the US National Oceanic and Atmospheric Administration. Download the Excel spreadsheet containing this data. The CO2 data was recorded from one Observatory in Mauna Loa. Using an Earth System Research Laboratory article as a reference, explain whether or not you think this data is a reliable representation of the global atmosphere. The variables ‘trend’ and ‘interpolated’ are similar, but not identical. In your own words, explain the difference between these two measures of CO2 levels. Why might there be seasonal variation in CO2 levels? Now we will use a line chart to look for general patterns over time. Plot a line chart with interpolated and trend CO2 levels on the vertical axis and time (starting from January 1960) on the horizontal axis. Label the axes and the chart legend, and give your chart an appropriate title. What does this chart suggest about the relationship between CO2 and time? We will now combine the CO2 data with the temperature data from Part 1.1, and then examine the relationship between these two variables visually, using scatterplots, and statistically, using the correlation coefficient. If you have not yet downloaded the temperature data, follow the instructions in Part 1.1. Choose one month and add the CO2 trend data to the temperature dataset from Part 1.1, making sure that the data corresponds to the correct year. Make a scatterplot of CO2 level on the vertical axis and temperature anomaly on the horizontal axis. Calculate and interpret the (Pearson) correlation coefficient between these two variables. Discuss the shortcomings of using this coefficient to summarize the relationship between variables. Excel walk-through 1.7 Calculating correlation and drawing a scatterplot Follow the walk-through in the CORE video, or in Figure 1.10, to find out how to calculate correlation and draw a scatterplot in Excel. How to calculate correlation and draw a scatterplot <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to create scatterplots and calculate the correlation coefficient. '' /> Figure 1.10 How to create scatterplots and calculate the correlation coefficient. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The CO2 data : This is what the monthly CO2 data looks like. Column A has the years (1958–present), Column B has the months (1–12), and Columns C to E have different measures of CO2 emissions. We will use the Trend data (Column E) to make a scatterplot of CO2 emissions and temperature anomalies for January. You will need to have the temperature anomaly spreadsheet open because we will be using it later. '' /> The CO2 data This is what the monthly CO2 data looks like. Column A has the years (1958–present), Column B has the months (1–12), and Columns C to E have different measures of CO2 emissions. We will use the Trend data (Column E) to make a scatterplot of CO2 emissions and temperature anomalies for January. You will need to have the temperature anomaly spreadsheet open because we will be using it later. Figure 1.10a This is what the monthly CO2 data looks like. Column A has the years (1958–present), Column B has the months (1–12), and Columns C to E have different measures of CO2 emissions. We will use the Trend data (Column E) to make a scatterplot of CO2 emissions and temperature anomalies for January. You will need to have the temperature anomaly spreadsheet open because we will be using it later. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the CO2 data according to your chosen month : After completing step 3, only the data for the selected months is shown in the spreadsheet. Data for the other months is still there, but it’s hidden. '' /> Filter the CO2 data according to your chosen month After completing step 3, only the data for the selected months is shown in the spreadsheet. Data for the other months is still there, but it’s hidden. Figure 1.10b After completing step 3, only the data for the selected months is shown in the spreadsheet. Data for the other months is still there, but it’s hidden. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the CO2 data according to your chosen month : We need to copy and paste the data in Column E into the temperature anomalies spreadsheet. Filtering the data first means that only these values will be copied (rather than all values in Column E). '' /> Filter the CO2 data according to your chosen month We need to copy and paste the data in Column E into the temperature anomalies spreadsheet. Filtering the data first means that only these values will be copied (rather than all values in Column E). Figure 1.10c We need to copy and paste the data in Column E into the temperature anomalies spreadsheet. Filtering the data first means that only these values will be copied (rather than all values in Column E). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the temperature data to correspond to the years present in the CO2 data : The values in Column A (years) need to match with the years available in the CO2 data. We will filter the data so that temperature data is only visible for the years that CO2 data is available. '' /> Filter the temperature data to correspond to the years present in the CO2 data The values in Column A (years) need to match with the years available in the CO2 data. We will filter the data so that temperature data is only visible for the years that CO2 data is available. Figure 1.10d The values in Column A (years) need to match with the years available in the CO2 data. We will filter the data so that temperature data is only visible for the years that CO2 data is available. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the temperature data to correspond to the years present in the CO2 data : Pay particular attention to the year in the first space. For March to December, the first year in the CO2 data is 1958. For January or February, the first year in the CO2 data is 1959. '' /> Filter the temperature data to correspond to the years present in the CO2 data Pay particular attention to the year in the first space. For March to December, the first year in the CO2 data is 1958. For January or February, the first year in the CO2 data is 1959. Figure 1.10e Pay particular attention to the year in the first space. For March to December, the first year in the CO2 data is 1958. For January or February, the first year in the CO2 data is 1959. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Paste the CO2 data into the temperature data spreadsheet : If the data has been filtered correctly, after step 9 the CO2 data will be matched with the corresponding year. '' /> Paste the CO2 data into the temperature data spreadsheet If the data has been filtered correctly, after step 9 the CO2 data will be matched with the corresponding year. Figure 1.10f If the data has been filtered correctly, after step 9 the CO2 data will be matched with the corresponding year. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a scatterplot for temperature anomaly and CO2 level : After completing step 13, your scatterplot will look similar to this chart, with CO2 level on the vertical axis and temperature anomaly on the horizontal axis. '' /> Draw a scatterplot for temperature anomaly and CO2 level After completing step 13, your scatterplot will look similar to this chart, with CO2 level on the vertical axis and temperature anomaly on the horizontal axis. Figure 1.10g After completing step 13, your scatterplot will look similar to this chart, with CO2 level on the vertical axis and temperature anomaly on the horizontal axis. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-h.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-h-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-h-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-h-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-h.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the range of the vertical axis : Currently the points are clustered in the top half of the graph. To see them more clearly, we need to change the range of the vertical axis. '' /> Change the range of the vertical axis Currently the points are clustered in the top half of the graph. To see them more clearly, we need to change the range of the vertical axis. Figure 1.10h Currently the points are clustered in the top half of the graph. To see them more clearly, we need to change the range of the vertical axis. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-i.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-i-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-i-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-i-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-i.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the range of the vertical axis : After completing step 15, you will see the individual data points more clearly. '' /> Change the range of the vertical axis After completing step 15, you will see the individual data points more clearly. Figure 1.10i After completing step 15, you will see the individual data points more clearly. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-j.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-j-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-j-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-j-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-j.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add titles : Give the axes and the chart appropriate titles. '' /> Add titles Give the axes and the chart appropriate titles. Figure 1.10j Give the axes and the chart appropriate titles. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-k.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-k-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-k-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-k-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-k.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate and interpret the correlation coefficient : The correlation coefficient we used here tells us how close the data is to resembling an upward- or downward-sloping straight line on a scatterplot. The correlation coefficient ranges from −1 to 1. A coefficient of 1 or −1 means that there is a perfect upward- or downward-sloping linear relationship between the two variables, while a coefficient of 0 means that there is no systematic upward- or downward-sloping linear relationship between the two variables. Note: The coefficient you get may be slightly different if you are using the latest data. '' /> Calculate and interpret the correlation coefficient The correlation coefficient we used here tells us how close the data is to resembling an upward- or downward-sloping straight line on a scatterplot. The correlation coefficient ranges from −1 to 1. A coefficient of 1 or −1 means that there is a perfect upward- or downward-sloping linear relationship between the two variables, while a coefficient of 0 means that there is no systematic upward- or downward-sloping linear relationship between the two variables. Note: The coefficient you get may be slightly different if you are using the latest data. Figure 1.10k The correlation coefficient we used here tells us how close the data is to resembling an upward- or downward-sloping straight line on a scatterplot. The correlation coefficient ranges from −1 to 1. A coefficient of 1 or −1 means that there is a perfect upward- or downward-sloping linear relationship between the two variables, while a coefficient of 0 means that there is no systematic upward- or downward-sloping linear relationship between the two variables. Note: The coefficient you get may be slightly different if you are using the latest data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-l.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-l-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-l-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-l-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-l.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate and interpret the correlation coefficient : Here are some more examples of correlation coefficients and how to interpret them. Note: The word ‘strong’ is used for coefficients that are close to 1 or −1, and ‘weak’ is used for coefficients that are close to 0, though there is no precise range of values that are considered ‘strong’ or ‘weak’. '' /> Calculate and interpret the correlation coefficient Here are some more examples of correlation coefficients and how to interpret them. Note: The word ‘strong’ is used for coefficients that are close to 1 or −1, and ‘weak’ is used for coefficients that are close to 0, though there is no precise range of values that are considered ‘strong’ or ‘weak’. Figure 1.10l Here are some more examples of correlation coefficients and how to interpret them. Note: The word ‘strong’ is used for coefficients that are close to 1 or −1, and ‘weak’ is used for coefficients that are close to 0, though there is no precise range of values that are considered ‘strong’ or ‘weak’. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-m.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-m-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-m-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-m-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-m.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Limitations of the correlation coefficient : One limitation of this correlation measure is that it only tells us about the strength of the linear relationship between two variables. The correlation coefficient cannot tell us if the two variables have a different kind of relationship (e.g. a wavy line). For example, suppose your electricity bill is high in winter but low in summer (the U-shaped pattern shown in the chart). This correlation measure cannot detect the U-shaped pattern, understating the actual association between the two variables. Note: There are other measures of correlation that can deal with this issue, but we will only look at this correlation measure (called Pearson correlation) for now. '' /> Limitations of the correlation coefficient One limitation of this correlation measure is that it only tells us about the strength of the linear relationship between two variables. The correlation coefficient cannot tell us if the two variables have a different kind of relationship (e.g. a wavy line). For example, suppose your electricity bill is high in winter but low in summer (the U-shaped pattern shown in the chart). This correlation measure cannot detect the U-shaped pattern, understating the actual association between the two variables. Note: There are other measures of correlation that can deal with this issue, but we will only look at this correlation measure (called Pearson correlation) for now. Figure 1.10m One limitation of this correlation measure is that it only tells us about the strength of the linear relationship between two variables. The correlation coefficient cannot tell us if the two variables have a different kind of relationship (e.g. a wavy line). For example, suppose your electricity bill is high in winter but low in summer (the U-shaped pattern shown in the chart). This correlation measure cannot detect the U-shaped pattern, understating the actual association between the two variables. Note: There are other measures of correlation that can deal with this issue, but we will only look at this correlation measure (called Pearson correlation) for now. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-n.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-n-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-n-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-n-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-10-n.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Limitations of the correlation coefficient : Remember that correlation is a measure of association and does not imply that one variable causes the other. Spurious correlation is when two variables are strongly correlated but there is no direct relationship between them. In this example, the number of insurance scams in your city is strongly correlated with your monthly electricity bill (almost a perfectly straight line), but it is difficult to argue that your electricity consumption caused the scams, or vice versa. Similarly, we cannot use correlation or scatterplots to predict the value of either variable outside the range shown. For example, from this chart, we cannot conclude that when there are 45 insurance scams, your electricity bill will be higher than $37. We can only use correlation and scatterplots to summarize and look at the data we have. '' /> Limitations of the correlation coefficient Remember that correlation is a measure of association and does not imply that one variable causes the other. Spurious correlation is when two variables are strongly correlated but there is no direct relationship between them. In this example, the number of insurance scams in your city is strongly correlated with your monthly electricity bill (almost a perfectly straight line), but it is difficult to argue that your electricity consumption caused the scams, or vice versa. Similarly, we cannot use correlation or scatterplots to predict the value of either variable outside the range shown. For example, from this chart, we cannot conclude that when there are 45 insurance scams, your electricity bill will be higher than $37. We can only use correlation and scatterplots to summarize and look at the data we have. Figure 1.10n Remember that correlation is a measure of association and does not imply that one variable causes the other. Spurious correlation is when two variables are strongly correlated but there is no direct relationship between them. In this example, the number of insurance scams in your city is strongly correlated with your monthly electricity bill (almost a perfectly straight line), but it is difficult to argue that your electricity consumption caused the scams, or vice versa. Similarly, we cannot use correlation or scatterplots to predict the value of either variable outside the range shown. For example, from this chart, we cannot conclude that when there are 45 insurance scams, your electricity bill will be higher than $37. We can only use correlation and scatterplots to summarize and look at the data we have. Extra practice: Choose two months and add the CO2 trend data to the temperature dataset from Part 1.1, making sure that the data corresponds to the correct year. Create a separate chart for each month. What do your charts and the correlation coefficients suggest about the relationship between CO2 levels and temperature anomalies? causationA direction from cause to effect, establishing that a change in one variable produces a change in another. While a correlation gives an indication of whether two variables move together (either in the same or opposite directions), causation means that there is a mechanism that explains this association. Example: We know that higher levels of CO2 in the atmosphere lead to a greenhouse effect, which warms the Earth’s surface. Therefore we can say that higher CO2 levels are the cause of higher surface temperatures.spurious correlationA strong linear association between two variables that does not result from any direct relationship, but instead may be due to coincidence or to another unseen factor. Even though two variables are strongly correlated with each other, it is not necessarily the case that one variable’s behaviour is the result of the other (a characteristic known as causation). The two variables could be spuriously correlated. The following example illustrates spurious correlation: A child’s academic performance may be positively correlated with the number of rooms in their house or house size, but could we conclude that building an extra room would make a child smarter, or doing well at school would make your house bigger? It is more plausible that income or wealth, which determines the size of home that a family can afford and the resources available for studying, is the ‘unseen factor’ in this relationship. We could also determine whether income is the reason for this spurious correlation by comparing exam scores for children whose parents have similar income but different house sizes. If there is no correlation between exam scores and house size, then we can deduce that house size was not ‘causing’ exam scores (or vice versa). See this TEDx Talk for more examples of the dangers of confusing correlation with causation. Consider the example of spurious correlation described above. In your own words, explain spurious correlation and the difference between correlation and causation. Give an example of spurious correlation, similar to the one above, for either CO2 levels or temperature anomalies. Choose an example of spurious correlation from Tyler Vigen’s website. Explain whether you think it is a coincidence, or whether this correlation could be due to one or more other variables. Find out more What makes some correlations spurious? In the spurious correlations website given in Question 6(c), most of the examples you will see involve data series that are trending (meaning that they tend to increase or decrease over time). If you calculate a correlation between two series that are trending, you are bound to find a large positive or negative correlation coefficient, even if there is no plausible explanation for a relationship between the two series. For example, ‘per capita cheese consumption’ (which increases over time due to increased disposable incomes or greater availability of cheese) has a correlation coefficient of 0.95 with the ‘number of people who die from becoming tangled in their bedsheets’ (which also increases over time due to a growing population and a growing availability of bedsheets). natural experimentAn empirical study exploiting naturally occurring statistical controls in which researchers do not have the ability to assign participants to treatment and control groups, as is the case in conventional experiments. Instead, differences in law, policy, weather, or other events can offer the opportunity to analyse populations as if they had been part of an experiment. The validity of such studies depends on the premise that the assignment of subjects to the naturally occurring treatment and control groups can be plausibly argued to be random. The case for our example (the relationship between temperature and CO2 emissions) is slightly different. There is a well-known chemical link between the two. So we understand how CO2 emissions could potentially cause changes in temperature. But in general, do not be tempted to conclude that there is a causal link just because a high correlation coefficient can be seen. Be very cautious when attaching too much meaning to high correlation coefficients when data displays trending behaviour. This part shows that summary statistics, such as the correlation coefficient, can help identify possible patterns or relationships between variables, but we cannot make conclusions about causation from them alone. It is also important to think about other explanations for what we see in the data, and whether we would expect there to be a relationship between the two variables. However, there are ways to determine whether there is a causal relationship between two variables, for example, by looking at the scientific processes that connect the variables (as with CO2 and temperature anomalies), or by using a natural experiment. To read more about how natural experiments help evaluate whether one variable causes another, see Section 1.8 of Economy, Society, and Public Policy. In Empirical Project 3, we will take a closer look at natural experiments and how we can use them to identify causal links between variables."
});
index.addDoc({
    id: 9,
    title: "Doing Economics: Empirical Project 1: Working in R",
    content: "Empirical Project 1 Working in R Download the code To download the code chunks used in this project, right-click on the download link and select ‘Save Link As…’. You’ll need to save the code download to your working directory, and open it in RStudio. Don’t forget to also download the data into your working directory by following the steps in this project. Getting started in R If you have worked with R and have the software installed on your machine, you can begin this project. If you have not, watch the videos ‘Installing R and RStudio’ and ‘RStudio orientation’ for guidance on installing and using RStudio. Installing R and RStudio RStudio orientation Part 1.1 The behaviour of average surface temperature over time Learning objectives for this part use line charts to describe the behaviour of real-world variables over time. In the questions below, we look at data from NASA about land–ocean temperature anomalies in the northern hemisphere. Figure 1.1 is constructed using this data, and shows temperatures in the northern hemisphere over the period 1880–2016, expressed as differences from the average temperature from 1951 to 1980. We start by creating charts similar to Figure 1.1, in order to visualize the data and spot patterns more easily. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Northern hemisphere temperatures (1880–2016). '' /> Northern hemisphere temperatures (1880–2016). Figure 1.1 Northern hemisphere temperatures (1880–2016). Before plotting any charts, download the data and make sure you understand how temperature is measured: Go to NASA’s Goddard Institute for Space Studies website. Under the subheading ‘Combined Land-Surface Air and Sea-Surface Water Temperature Anomalies’, select the CSV version of ‘Northern Hemisphere-mean monthly, seasonal, and annual means’ (right-click and select ‘Save Link As…’). The default name of this file is NH.Ts+dSST.csv. Give it a suitable name and save it in an easily accessible location, such as a folder on your Desktop or in your personal folder. In this dataset, temperature is measured as ‘anomalies’ rather than as absolute temperature. Using NASA’s Frequently Asked Questions section as a reference, explain in your own words what temperature ‘anomalies’ means. Why have researchers chosen this particular measure over other measures (such as absolute temperature)? First we have to import the data into R. R walk-through 1.1 Importing the datafile into R We want to import the datafile called ‘NH.Ts+dSST.csv’ into R. We start by setting our working directory using the setwd command. This command tells R where your datafiles are stored. In the code below, replace ‘YOURFILEPATH’ with the full filepath that indicates the folder in which you have saved the datafile. If you don’t know how to find the path to your working folder, see the ‘Technical Reference’ section. setwd(''YOURFILEPATH'') Since our data is in csv format, we use the read.csv function to import the data into R. We will call our file ‘tempdata’ (short for ‘temperature data’). Here you can see commands to R which are spread across two lines. You can spread a command across multiple lines, but you must adhere to the following two rules for this to work. First, the line break should come inside a set of parenthesis (i.e. between ( and ) or straight after the assignment operator (<-). Second, the line break must not be inside a string (whatever is inside quotes) or in the middle of a word or number. tempdata <- read.csv(''NH.Ts+dSST.csv'', skip = 1, na.strings = ''***'') When using this function, we added two options. If you open the spreadsheet in Excel, you will see that the real data table only starts in Row 2, so we use the skip = 1 option to skip the first row when importing the data. When looking at the spreadsheet, you can see that missing temperature data is coded as ''***''. In order for R to recognize the non-missing temperature data as numbers, we use the na.strings = ''***'' option to indicate that missing observations in the spreadsheet are coded as ''***''. To check that the data has been imported correctly, you can use the head function to view the first six rows of the dataset, and confirm that they correspond to the columns in the csv file. head(tempdata) ## Year Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov ## 1 1880 -0.57 -0.41 -0.28 -0.38 -0.12 -0.24 -0.25 -0.26 -0.29 -0.34 -0.39 ## 2 1881 -0.21 -0.27 -0.01 -0.04 -0.07 -0.38 -0.07 -0.04 -0.25 -0.42 -0.44 ## 3 1882 0.21 0.20 -0.01 -0.38 -0.33 -0.39 -0.38 -0.15 -0.18 -0.54 -0.34 ## 4 1883 -0.61 -0.70 -0.17 -0.29 -0.34 -0.27 -0.10 -0.27 -0.35 -0.23 -0.42 ## 5 1884 -0.24 -0.13 -0.67 -0.64 -0.43 -0.53 -0.49 -0.52 -0.46 -0.42 -0.49 ## 6 1885 -1.01 -0.37 -0.21 -0.54 -0.57 -0.49 -0.38 -0.46 -0.33 -0.31 -0.29 ## Dec J.D D.N DJF MAM JJA SON ## 1 -0.51 -0.34 NA NA -0.26 -0.25 -0.34 ## 2 -0.30 -0.21 -0.23 -0.33 -0.04 -0.16 -0.37 ## 3 -0.43 -0.23 -0.22 0.04 -0.24 -0.31 -0.35 ## 4 -0.27 -0.34 -0.35 -0.58 -0.27 -0.21 -0.33 ## 5 -0.40 -0.45 -0.44 -0.22 -0.58 -0.51 -0.45 ## 6 0.00 -0.41 -0.45 -0.60 -0.44 -0.44 -0.31 Before working with the important data, we use the str function to check that the data is formatted correctly. str(tempdata) ## 'data.frame': 138 obs. of 19 variables: ## $ Year: int 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 ... ## $ Jan : num -0.57 -0.21 0.21 -0.61 -0.24 -1.01 -0.69 -1.08 -0.54 -0.33 ... ## $ Feb : num -0.41 -0.27 0.2 -0.7 -0.13 -0.37 -0.69 -0.6 -0.61 0.32 ... ## $ Mar : num -0.28 -0.01 -0.01 -0.17 -0.67 -0.21 -0.58 -0.37 -0.59 0.05 ... ## $ Apr : num -0.38 -0.04 -0.38 -0.29 -0.64 -0.54 -0.35 -0.43 -0.26 0.12 ... ## $ May : num -0.12 -0.07 -0.33 -0.34 -0.43 -0.57 -0.36 -0.28 -0.18 -0.07 ... ## $ Jun : num -0.24 -0.38 -0.39 -0.27 -0.53 -0.49 -0.44 -0.21 -0.06 -0.15 ... ## $ Jul : num -0.25 -0.07 -0.38 -0.1 -0.49 -0.38 -0.21 -0.23 0.01 -0.12 ... ## $ Aug : num -0.26 -0.04 -0.15 -0.27 -0.52 -0.46 -0.48 -0.53 -0.21 -0.19 ... ## $ Sep : num -0.29 -0.25 -0.18 -0.35 -0.46 -0.33 -0.35 -0.18 -0.15 -0.28 ... ## $ Oct : num -0.34 -0.42 -0.54 -0.23 -0.42 -0.31 -0.33 -0.41 0.01 -0.37 ... ## $ Nov : num -0.39 -0.44 -0.34 -0.42 -0.49 -0.29 -0.46 -0.21 -0.04 -0.63 ... ## $ Dec : num -0.51 -0.3 -0.43 -0.27 -0.4 0 -0.18 -0.45 -0.28 -0.58 ... ## $ J.D : num -0.34 -0.21 -0.23 -0.34 -0.45 -0.41 -0.43 -0.42 -0.24 -0.19 ... ## $ D.N : num NA -0.23 -0.22 -0.35 -0.44 -0.45 -0.41 -0.39 -0.26 -0.16 ... ## $ DJF : num NA -0.33 0.04 -0.58 -0.22 -0.6 -0.46 -0.62 -0.53 -0.1 ... ## $ MAM : num -0.26 -0.04 -0.24 -0.27 -0.58 -0.44 -0.43 -0.36 -0.34 0.03 ... ## $ JJA : num -0.25 -0.16 -0.31 -0.21 -0.51 -0.44 -0.38 -0.32 -0.09 -0.15 ... ## $ SON : num -0.34 -0.37 -0.35 -0.33 -0.45 -0.31 -0.38 -0.27 -0.06 -0.43 ... You can see that all variables are formatted as numerical data (num), so R correctly recognizes that the data are numbers. Now create some line charts using monthly, seasonal, and annual data, which help us look for general patterns over time. Choose one month and plot a line chart with average temperature anomaly on the vertical axis and time (from 1880 to the latest year available) on the horizontal axis. Label each axis appropriately and give your chart a suitable title (Refer to Figure 1.1 as an example.) R walk-through 1.2 Drawing a line chart of temperature and time The data is formatted as numerical (num) data, so R recognizes each variable as a series of numbers (instead of text), but does not recognize that these numbers correspond to the same variable for different time periods (known as ‘time series data’ in economics). Letting R know that we have time series data will make coding easier later (especially with making graphs). You can use the ts function to specify that a variable is a time series. Make sure to amend the code below so that the end year (end = c()) corresponds to the latest year in your dataset (our example uses 2017). tempdata$Jan <- ts(tempdata$Jan, start = c(1880), end = c(2017), frequency = 1) tempdata$DJF <- ts(tempdata$DJF, start = c(1880), end = c(2017), frequency = 1) tempdata$MAM <- ts(tempdata$MAM, start = c(1880), end = c(2017), frequency = 1) tempdata$JJA <- ts(tempdata$JJA, start = c(1880), end = c(2017), frequency = 1) tempdata$SON <- ts(tempdata$SON, start = c(1880), end = c(2017), frequency = 1) tempdata$J.D <- ts(tempdata$J.D, start = c(1880), end = c(2017), frequency = 1) Note that we placed each of these quarterly series in the relevant middle month. You could do the same for the remaining series, but we will only use the series above in this R walk-through. We can now use these variables to draw line charts using the plot function. As an example, we will draw a line chart using data for January (tempdata$Jan) for the years 1880–2016. The title option on the next line adds a chart title, and the abline option draws a horizontal line according to our specifications. Make sure to amend the code below so that your chart title corresponds to the latest year in your dataset (our example uses 2016). # Set line width and colour plot(tempdata$Jan, type = ''l'', col = ''blue'', lwd = 2, ylab = ''Annual temperature anomalies'', xlab = ''Year'') # Add a title title(''Average temperature anomaly in January in the northern hemisphere (1880-2016)'') # Add a horizontal line (at y = 0) abline(h = 0, col = ''darkorange2'', lwd = 2) # Add a label to the horizontal line text(2000, -0.1, ''1951-1980 average'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-02.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-02-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-02-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-02-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-02.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Northern hemisphere January temperatures (1880–2016). '' /> Northern hemisphere January temperatures (1880–2016). Figure 1.2 Northern hemisphere January temperatures (1880–2016). Try different values for type and col in the plot function to figure out what these options do (some online research could help). xlab and ylab define the respective axis titles. It is important to remember that all axis and chart titles should be enclosed in quotation marks (''''), as well as any words that are not options (for example, colour names or filenames). Extra practice: The columns labelled DJF, MAM, JJA, and SON contain seasonal averages (means). For example, the MAM column contains the average of the March, April, and May columns for each year. Plot a separate line chart for each season, using average temperature anomaly for that season on the vertical axis and time (from 1880 to the latest year available) on the horizontal axis. The column labelled J–D contains the average temperature anomaly for each year. Plot a line chart with annual average temperature anomaly on the vertical axis and time (from 1880 to the latest year available) on the horizontal axis. Your chart should look like Figure 1.1. Extension: Add a horizontal line that intersects the vertical axis at 0, and label it ‘1951–1980 average’. What do your charts from Questions 2 to 4(a) suggest about the relationship between temperature and time? R walk-through 1.3 Producing a line chart for the annual temperature anomalies This is where the power of programming languages becomes evident: to produce the same line chart for a different variable, we simply take the code used in R walk-through 1.2 and replace the variable name Jan with the name for the annual variable (J.D). Again, make sure to amend the code so that your chart title corresponds to the latest year in your data (our example uses 2016). # Set line width and colour plot(tempdata$J.D, type = ''l'', col = ''blue'', lwd = 2, ylab = ''Annual temperature anomalies'', xlab = ''Year'') # n creates a line break title(''Average annual temperature anomaly n in the northern hemisphere (1880-2016)'') # Add a horizontal line (at y = 0) abline(h = 0, col = ''darkorange2'', lwd = 2) # Add a label to the horizontal line text(2000, -0.1, ''1951-1980 average'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Northern hemisphere annual temperatures (1880–2016). '' /> Northern hemisphere annual temperatures (1880–2016). Figure 1.3 Northern hemisphere annual temperatures (1880–2016). You now have charts for three different time intervals: month (Question 2), season (Question 3), and year (Question 4). For each time interval, discuss what we can learn about patterns in temperature over time that we might not be able to learn from the charts of other time intervals. Compare your chart from Question 4 to Figure 1.4, which also shows the behaviour of temperature over time using data taken from the National Academy of Sciences. Discuss the similarities and differences between the charts. (For example, are the horizontal and vertical axes variables the same, or do the lines have the same shape?) Looking at the behaviour of temperature over time from 1000 to 1900 in Figure 1.4, are the observed patterns in your chart unusual? Based on your answers to Questions 4 and 5, do you think the government should be concerned about climate change? <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Northern hemisphere temperatures over the long run (1000–2006). '' /> Northern hemisphere temperatures over the long run (1000–2006). Figure 1.4 Northern hemisphere temperatures over the long run (1000–2006). Part 1.2 Variation in temperature over time Learning objectives for this part summarize data in a frequency table, and visualize distributions with column charts describe a distribution using mean and variance. Aside from changes in the mean temperature, the government is also worried that climate change will result in more frequent extreme weather events. The island has experienced a few major storms and severe heat waves in the past, both of which caused serious damage and disruption to economic activity. Will weather become more extreme and vary more as a result of climate change? A New York Times article uses the same temperature dataset you have been using to investigate the distribution of temperatures and temperature variability over time. Read through the article, paying close attention to the descriptions of the temperature distributions. We can use the mean and median to describe distributions, and we can use deciles to describe parts of distributions. To visualize distributions, we can use column charts (sometimes referred to as frequency histograms). We are now going to create similar charts of temperature distributions to the ones in the New York Times article, and look at different ways of summarizing distributions. frequency tableA record of how many observations in a dataset have a particular value, range of values, or belong to a particular category. In order to create a column chart using the temperature data we have, we first need to summarize the data using a frequency table. Instead of using deciles to group the data, we use intervals of 0.05, so that temperature anomalies with a value from −0.3 to −0.025 will be in one group, a value greater than −0.025 and up to 0.02 in another group, and so on. The frequency table shows us how many values belong to a particular group. Using the monthly data for June, July, and August, create two frequency tables similar to Figure 1.5 for the years 1951–1980 and 1981–2010 respectively. The values in the first column should range from −0.3 to 1.05, in intervals of 0.05. See R walk-through 1.3 for how to do this. Range of temperature anomaly (T) Frequency −0.30 −0.25 … 1.00 1.05 Figure 1.5 A frequency table. Using the frequency tables from Question 1: Plot two separate column charts (frequency histograms) for 1951–1980 and 1981–2010 to show the distribution of temperatures, with frequency on the vertical axis and the range of temperature anomaly on the horizontal axis. Your charts should look similar to those in the New York Times article. Using your charts, describe the similarities and differences (if any) between the distributions of temperature anomalies in 1951–1980 and 1981–2010. R walk-through 1.4 Creating frequency tables and histograms Since we will be looking at data from different subperiods (year intervals) separately, we will create a categorical variable (a variable that has two or more categories) that indicates the subperiod for each observation (row). In R this type of variable is called a ‘factor variable’. When we create a factor variable, we need to define the categories that this variable can take. tempdata$Period <- factor(NA, levels = c(''1921-1950'', ''1951-1980'', ''1981-2010''), ordered = TRUE) We created a new variable called Period and defined the possible categories (which R refers to as ‘levels’). Since we will not be using data for some years (before 1921 and after 2010), we want Period to take the value ‘NA’ (not available) for these observations (rows), and the appropriate category for all the other observations (between 1921–2010). One way to do this is by defining Period as ‘NA’ for all observations, then change the values of Period for the observations in 1921–2010. tempdata$Period[(tempdata$Year > 1920) & (tempdata$Year < 1951)] <- ''1921-1950'' tempdata$Period[(tempdata$Year > 1950) & (tempdata$Year < 1981)] <- ''1951-1980'' tempdata$Period[(tempdata$Year > 1980) & (tempdata$Year < 2011)] <- ''1981-2010'' We need to use all monthly anomalies from June, July, and August, but they are currently in three separate columns. We will use the c (combine) function to create one new variable (called temp_summer) that contains all these values. # Combine the temperature data for June, July, and August temp_summer <- c(tempdata$Jun, tempdata$Jul, tempdata$Aug) There are many ways to achieve the same result. One alternative is to use the unlist function and apply it to Columns 7 to 9 (containing the data for June to August) of tempdata. temp_summer <- unlist(tempdata[,7:9],use.names = FALSE) Now we have one long variable (temp_summer), with the monthly temperature anomalies for the three months (from 1880 to the latest year) attached to each other. But remember that we want to make separate calculations for each category in Period (1921–1950, 1951–1980, 1981–2010). To make a variable showing the categories for the temp_summer variable, we use the c function again. # Mirror the Period information for temp_sum temp_Period <- c(tempdata$Period, tempdata$Period, tempdata$Period) # Repopulate the factor information temp_Period <- factor(temp_Period, levels = 1:nlevels(tempdata$Period), labels = levels(tempdata$Period)) After using the c function, we had to use the factor function again to tell R that our new variable temp_Period is a factor variable. We have now created the variables needed to make frequency tables and histograms (temp_summer and temp_Period). To obtain the frequency table for 1951–1980, we use the hist function on the monthly temperature anomalies from the period ‘1951–1980’: temp_summer[(temp_Period == ''1951-1980'')]. The option plot = FALSE tells R not to make a plot of this information. (See what happens if you set it to TRUE.) hist(temp_summer[(temp_Period == ''1951-1980'')], plot = FALSE) ## $breaks ## [1] -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 ## [12] 0.25 ## ## $counts ## [1] 2 7 6 7 14 6 14 12 10 9 3 ## ## $density ## [1] 0.4444444 1.5555556 1.3333333 1.5555556 3.1111111 1.3333333 3.1111111 ## [8] 2.6666667 2.2222222 2.0000000 0.6666667 ## ## $mids ## [1] -0.275 -0.225 -0.175 -0.125 -0.075 -0.025 0.025 0.075 0.125 0.175 ## [11] 0.225 ## ## $xname ## [1] ''temp_summer[(temp_Period == ''1951-1980'')]'' ## ## $equidist ## [1] TRUE ## ## attr(,''class'') ## [1] ''histogram'' From the output you can see that we can get the temperature ranges (the values in $breaks correspond to Column 1 of Figure 1.5) and the frequencies ($counts), which is all we need to create a frequency table. However, in our case the frequency table is merely a temporary input required to produce a histogram. We can make the three histograms we need all at once, using the histogram function from the mosaic package. The function below includes multiple commands: | temp_Period splits the data according to its category, given by temp_Period. type = ''count'' indicates that we want to display the counts (frequencies) in each category. breaks = seq(-0.5, 1.3, 0.1) gives a sequence of numbers −0.5, −0.4, …, 1.3, which are boundaries for the categories. main = ''Histogram of temperature anomalies'' gives Figure 1.6 its title. # Load the library we use for the following command. library(mosaic) histogram(~ temp_summer | temp_Period, type = ''count'', breaks = seq(-0.5, 1.3, 0.10), main = ''Histogram of Temperature anomalies'', xlab = ''Summer temperature distribution'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Summer temperature distributions for different time periods. '' /> Summer temperature distributions for different time periods. Figure 1.6 Summer temperature distributions for different time periods. To explain what a histogram displays, we refer to the histogram for the period from 1921–1950. Notice the vertical bars that are centred at values such as –0.35, –0.25, –0.15, –0.05, and so forth. We will look first at the highest of the bars, which is centred at –0.15. This bar represents values of the temperature anomalies that fall in the interval from –0.2 to –0.1. The height of this bar is a representation of how many values fall into this interval, (23 observations, in this case). As it is the highest bar, this indicates that this is the interval in which the largest proportion of temperature anomalies fell for the period from 1921 to 1950. As you can see, there are virtually no temperature anomalies larger than 0.3. The height of these bars gives a useful overview of the distribution of the temperature anomalies. Now consider how this distribution changes as we move through the three distinct time periods. The distribution is clearly moving to the right for the period 1981–2010, which is an indication that the temperature is increasing; in other words, an indication of global warming. varianceA measure of dispersion in a frequency distribution, equal to the mean of the squares of the deviations from the arithmetic mean of the distribution. The variance is used to indicate how ‘spread out’ the data is. A higher variance means that the data is more spread out. Example: The set of numbers 1, 1, 1 has zero variance (no variation), while the set of numbers 1, 1, 999 has a high variance of 221,334 (large spread). Now we will use our data to look at different aspects of distributions. Firstly, we will learn how to use deciles to determine which observations are ‘normal’ and ‘abnormal’, and then learn how to use variance to describe the shape of a distribution. The New York Times article considers the bottom third (the lowest or coldest one-third) of temperature anomalies in 1951–1980 as ‘cold’ and the top third (the highest or hottest one-third) of anomalies as ‘hot’. In decile terms, temperatures in the 1st to 3rd decile are ‘cold’ and temperatures in the 7th to 10th decile or above are ‘hot’ (rounded to the nearest decile). Use R’s quantile function to determine what values correspond to the 3rd and 7th decile across all months in 1951–1980. R walk-through 1.5 Using the quantile function First, we need to create a variable that contains all monthly anomalies in the years 1951–1980. Then, we use R’s quantile function to find the required percentiles (0.3 and 0.7 refer to the 3rd and 7th deciles, respectively). Note: You may get slightly different values to those shown here if you are using the latest data. # Select years 1951 to 1980 temp_all_months <- subset(tempdata, (Year >= 1951 & Year <= 1980)) # Columns 2 to 13 contain months Jan to Dec. temp_51to80 <- unlist(temp_all_months[, 2:13]) # c(0.3, 0.7) indicates the chosen percentiles. perc <- quantile(temp_51to80, c(0.3, 0.7)) # The cold threshold p30 <- perc[1] p30 ## 30% ## -0.1 # The hot threshold p70 <- perc[2] p70 ## 70% ## 0.11 Based on the values you found in Question 3, count the number of anomalies that are considered ‘hot’ in 1981–2010, and express this as a percentage of all the temperature observations in that period. Does your answer suggest that we are experiencing hotter weather more frequently in 1981–2010? (Remember that each decile represents 10% of observations, so 30% of temperatures were considered ‘hot’ in 1951–1980.) R walk-through 1.6 Using the mean function Note: You may get slightly different values to those shown here if you are using the latest data. We repeat the steps used in R walk-through 1.5, now looking at monthly anomalies in the years 1981–2010. We can simply change the year values in the code from R walk-through 1.5. # Select years 1951 to 1980 temp_all_months <- subset(tempdata, (Year >= 1981 & Year <= 2010)) # Columns 2 to 13 contain months Jan to Dec. temp_81to10 <- unlist(temp_all_months[, 2:13]) Now that we have all the monthly data for 1981–2010, we want to count the proportion of observations that are smaller than –0.1. This is easily achieved with the following lines of code: paste(''Proportion smaller than p30'') ## [1] ''Proportion smaller than p30'' temp <- temp_81to10 < p30 mean(temp) ## [1] 0.01944444 What we did was first create a variable called temp, which equals 1 (TRUE) for all the monthly temperature anomalies in temp_81to10 that are smaller than the 30th percentile value (temp_81to10 < p30), and 0 (FALSE) otherwise. The mean of this variable is the proportion of 1s. Here we find that 0.019 (= 1.9%) of observations are smaller than p30. That means that between 1951 and 1980, 30% of observations for the temperature anomaly were smaller than –0.10, but between 1981 and 2010 only about two per cent of months are considered cold. That is a large change. Let’s check whether we get a similar result for the number of observations that are larger than 0.11. paste(''Proportion larger than p70'') ## [1] ''Proportion larger than p70'' mean(temp_81to10 > p70) ## [1] 0.8444444 The New York Times article discusses whether temperatures have become more variable over time. One way to measure temperature variability is by calculating the variance of the temperature distribution. For each season (DJF, MAM, JJA, and SON): Calculate the mean (average) and variance separately for the following time periods: 1921–1950, 1951–1980, and 1981–2010. For each season, compare the variances in different periods, and explain whether or not temperature appears to be more variable in later periods. R walk-through 1.7 Calculating and understanding mean and variance One way to calculate mean and variance is to use the mosaic package introduced in R walk-through 1.4. (Remember to first install the mosaic package and load it into R with library(mosaic).) # Only run if you haven't loaded mosaic yet library(mosaic) paste(''Mean of DJF temperature anomalies across periods'') ## [1] ''Mean of DJF temperature anomalies across periods'' mean(~DJF|Period,data = tempdata) ## 1921-1950 1951-1980 1981-2010 ## -0.0573333333 -0.0006666667 0.5206666667 paste(''Variance of DJF anomalies across periods'') ## [1] ''Variance of DJF anomalies across periods'' var(~DJF|Period,data = tempdata) ## 1921-1950 1951-1980 1981-2010 ## 0.05907540 0.05361333 0.07149609 Using the data in tempdata (data = tempdata), we calculated the mean (mean) and variance (var) of variable ~DJF separately for (|) each value of Period. The mosaic package allows us to calculate the means/variances for each period all at once. If mosaic is not loaded, you will get the error message: Error in mean(~DJF | Period, data = tempdata) : unused argument (data = tempdata). Looking at the results, it appears that it is not only the mean (December, January, and February) temperature anomaly that increases through 1981–2010, but also the variance. Let’s calculate the variances through the periods for the other seasons. paste(''Variance of MAM anomalies across periods'') ## [1] ''Variance of MAM anomalies across periods'' var(~MAM|Period,data = tempdata) ## 1921-1950 1951-1980 1981-2010 ## 0.03029069 0.02661333 0.07535126 paste(''Variance of JJA anomalies across periods'') ## [1] ''Variance of JJA anomalies across periods'' var(~JJA|Period,data = tempdata) ## 1921-1950 1951-1980 1981-2010 ## 0.01726713 0.01459264 0.06588690 paste(''Variance of SON anomalies across periods'') ## [1] ''Variance of SON anomalies across periods'' var(~SON|Period,data = tempdata) ## 1921-1950 1951-1980 1981-2010 ## 0.02421437 0.02587920 0.10431506 We recognize that the variances seem to remain fairly constant across the first two periods, but they do increase markedly for the 1981–2010 period. We can plot a line chart to see these changes graphically. (This type of chart is formally known as a ‘time-series plot’). Make sure to change the chart title according to the latest year in your data (here we used 2016). plot(tempdata$DJF, type = ''l'', col = ''blue'', lwd = 2, ylab = ''Annual temperature anomalies'', xlab = ''Year'') # n creates a line break title(''Average temperature anomaly in DJF and JJA n in the northern hemisphere (1880-2016)'') # Add a horizontal line (at y = 0) abline(h = 0, col = ''darkorange2'', lwd = 2) lines(tempdata$JJA, col = ''darkgreen'', lwd = 2) # Add a label to the horizontal line text(1895, 0.1, ''1951-1980 average'') legend(1880, 1.5, legend = c(''DJF'', ''JJA''), col = c(''blue'', ''darkgreen''), lty = 1, cex = 0.8, lwd = 2) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Northern hemisphere winter and summer quarter temperatures (1958–2016). '' /> Northern hemisphere winter and summer quarter temperatures (1958–2016). Figure 1.7 Northern hemisphere winter and summer quarter temperatures (1958–2016). Using the findings of the New York Times article and your answers to Questions 1 to 5, discuss whether temperature appears to be more variable over time. Would you advise the government to spend more money on mitigating the effects of extreme weather events? Part 1.3 Carbon emissions and the environment Learning objectives for this part use scatterplots and the correlation coefficient to assess the degree of association between two variables explain what correlation measures and the limitations of correlation. correlationA measure of how closely related two variables are. Two variables are correlated if knowing the value of one variable provides information on the likely value of the other, for example high values of one variable being commonly observed along with high values of the other variable. Correlation can be positive or negative. It is negative when high values of one variable are observed with low values of the other. Correlation does not mean that there is a causal relationship between the variables. Example: When the weather is hotter, purchases of ice cream are higher. Temperature and ice cream sales are positively correlated. On the other hand, if purchases of hot beverages decrease when the weather is hotter, we say that temperature and hot beverage sales are negatively correlated.correlation coefficientA numerical measure, ranging between 1 and −1, of how closely associated two variables are—whether they tend to rise and fall together, or move in opposite directions. A positive coefficient indicates that when one variable takes a high (low) value, the other tends to be high (low) too, and a negative coefficient indicates that when one variable is high the other is likely to be low. A value of 1 or −1 indicates that knowing the value of one of the variables would allow you to perfectly predict the value of the other. A value of 0 indicates that knowing one of the variables provides no information about the value of the other. The government has heard that carbon emissions could be responsible for climate change, and has asked you to investigate whether this is the case. To do so, we are now going to look at carbon emissions over time, and use another type of chart, the scatterplot, to show their relationship to temperature anomalies. One way to measure the relationship between two variables is correlation. R walk-through 1.8 explains what correlation is and how to calculate it in R. In the questions below, we will make charts using the CO2 data from the US National Oceanic and Atmospheric Administration. Download the Excel spreadsheet containing this data. Save the data as a csv file and import it into R. The CO2 data was recorded from one observatory in Mauna Loa. Using an Earth System Research Laboratory article as a reference, explain whether or not you think this data is a reliable representation of the global atmosphere. The variables trend and interpolated are similar, but not identical. In your own words, explain the difference between these two measures of CO2 levels. Why might there be seasonal variation in CO2 levels? Now we will use a line chart to look for general patterns over time. Plot a line chart with interpolated and trend CO2 levels on the vertical axis and time (starting from January 1960) on the horizontal axis. Label the axes and the chart legend, and give your chart an appropriate title. What does this chart suggest about the relationship between CO2 and time? We will now combine the CO2 data with the temperature data from Part 1.1, and then examine the relationship between these two variables visually using scatterplots, and statistically using the correlation coefficient. If you have not yet downloaded the temperature data, follow the instructions in Part 1.1. Choose one month and add the CO2 trend data to the temperature dataset from Part 1.1, making sure that the data corresponds to the correct year. Make a scatterplot of CO2 level on the vertical axis and temperature anomaly on the horizontal axis. Calculate and interpret the (Pearson) correlation coefficient between these two variables. Discuss the shortcomings of using this coefficient to summarize the relationship between variables. R walk-through 1.8 Scatterplots and the correlation coefficient First we will use the read.csv function to import the CO2 datafile into R, and call it CO2data. CO2data <- read.csv(''1_CO2 data.csv'') This file has monthly data, but in contrast to the data in tempdata, the data is all in one column (this is more conventional than the column per month format). To make this task easier, we will pick the June data from the CO2 emissions and add them as an additional variable to the tempdata dataset. R has a convenient function called merge to do this. First we create a new dataset that contains only the June emissions data (‘CO2data_june’). CO2data_june <- CO2data[CO2data$Month == 6,] Then we use this data in the merge function. The merge function takes the original ‘tempdata’ and the ‘CO2data’ and merges (combines) them together. As the two dataframes have a common variable, Year, R automatically matches the data by year. (Extension: Look up ?merge or Google ‘How to use the R merge function’ to figure out what all.x does, and to see other options that this function allows.) names(CO2data)[1] <- ''Year'' tempCO2data <- merge(tempdata, CO2data_june) Let us have a look at the data and check that it was combined correctly: head(tempCO2data[, c(''Year'', ''Jun'', ''Trend'')]) ## Year Jun Trend ## 1 1958 0.04 314.85 ## 2 1959 0.14 315.92 ## 3 1960 0.18 317.36 ## 4 1961 0.19 317.48 ## 5 1962 -0.10 318.27 ## 6 1963 -0.02 319.16 To make a scatterplot, we use the plot function. R’s default chart for plot is a scatterplot, so we do not need to specify the chart type. One new option that applies to scatterplots is pch = , which determines the appearance of the data points. The number 16 corresponds to filled-in circles, but you can experiment with other numbers (from 0 to 25) to see what the data points look like. plot(tempCO2data$Jun, tempCO2data$Trend, xlab = ''Temperature anomaly (degrees Celsius)'', ylab = ''CO2 levels (trend, mole fraction)'', pch = 16, col = ''blue'') title(''Scatterplot for CO2 emissions and temperature anomalies'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-08.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-08-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-08-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-08-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-08.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''CO2 emissions and northern hemisphere June temperatures (1958–2016). '' /> CO2 emissions and northern hemisphere June temperatures (1958–2016). Figure 1.8 CO2 emissions and northern hemisphere June temperatures (1958–2016). The cor function calculates the correlation coefficient. Note: You may get slightly different results if you are using the latest data. cor(tempCO2data$Jun, tempCO2data$Trend) ## [1] 0.9157744 In this case, the correlation coefficient tells us that the data is quite close to resembling an upward-sloping straight line (as seen on the scatterplot). There is a strong positive association between the two variables (higher temperature anomalies are associated with higher CO2 levels). One limitation of this correlation measure is that it only tells us about the strength of the upward- or downward-sloping linear relationship between two variables, in other words how closely the scatterplot aligns along an upward- or downward-sloping straight line. The correlation coefficient cannot tell us if the two variables have a different kind of relationship (such as that represented by a wavy line). Note: The word ‘strong’ is used for coefficients that are close to 1 or −1, and ‘weak’ is used for coefficients that are close to 0, though there is no precise range of values that are considered ‘strong’ or ‘weak’. If you need more insight into correlation coefficients, you may find it helpful to watch online tutorials such as ‘Correlation coefficient intuition’ from the Khan Academy. As we are dealing with time-series data, it is often more instructive to look at a line plot, as a scatterplot cannot convey how the observations relate to each other in the time dimension. If you were to check the variable types (using str(tempCO2data)), you would see that the data is not yet in time-series format. We could continue with the format as it is, but for plotting purposes it is useful to let R know that we are dealing with time-series data. We therefore apply the ts function as we did in Part 1.1. tempCO2data$Jun <- ts(tempCO2data$Jun, start = c(1958), end = c(2017), frequency = 1) tempCO2data$Trend <- ts(tempCO2data$Trend, start = c(1958), end = c(2017), frequency = 1) Let’s start by plotting the June temperature anomalies. plot(tempCO2data$Jun, type = ''l'', col = ''blue'', lwd = 2, ylab = ''June temperature anomalies'', xlab = ''Year'') title(''June temperature anomalies and CO2 emissions'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-09.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-09-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-09-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-09-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-09.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Northern hemisphere June temperatures (1958–2016). '' /> Northern hemisphere June temperatures (1958–2016). Figure 1.9 Northern hemisphere June temperatures (1958–2016). Typically, when using the plot function we would now only need to add the line for the second variable using the lines command. The issue, however, is that the CO2 emissions variable (Trend) is on a different scale, and the automatic vertical axis scale (from –0.2 to about 1.2) would not allow for the display of Trend. To resolve this issue you can introduce a second vertical axis using the commands below. (Tip: You are unlikely to remember the exact commands required, however you can Google ‘R plot 2 vertical axes’ or a similar search term, and then adjust the code you find so it will work on your dataset.) # Create extra margins used for the second axis par(mar = c(5, 5, 2, 5)) plot(tempCO2data$Jun, type = ''l'', col = ''blue'', lwd = 2, ylab = ''June temperature anomalies'', xlab = ''Year'') title(''June temperature anomalies and CO2 emissions'') # This puts the next plot into the same picture. par(new = T) # No axis, no labels plot(tempCO2data$Trend, pch = 16, lwd = 2, axes = FALSE, xlab = NA, ylab = NA, cex = 1.2) axis(side = 4) mtext(side = 4, line = 3, 'CO2 emissions') legend(''topleft'', legend = c(''June temp anom'', ''CO2 emis''), lty = c(1, 1), col = c(''blue'', ''black''), lwd = 2) causationA direction from cause to effect, establishing that a change in one variable produces a change in another. While a correlation gives an indication of whether two variables move together (either in the same or opposite directions), causation means that there is a mechanism that explains this association. Example: We know that higher levels of CO2 in the atmosphere lead to a greenhouse effect, which warms the Earth’s surface. Therefore we can say that higher CO2 levels are the cause of higher surface temperatures.spurious correlationA strong linear association between two variables that does not result from any direct relationship, but instead may be due to coincidence or to another unseen factor. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-10.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-10-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-10-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-10-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-01-10.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''CO2 emissions and northern hemisphere June temperatures (1958–2016). '' /> CO2 emissions and northern hemisphere June temperatures (1958–2016). Figure 1.10 CO2 emissions and northern hemisphere June temperatures (1958–2016). This line graph not only shows how the two variables move together in general, but also clearly demonstrates that both variables display a clear upward trend over the sample period. This is an important feature of many (not all) time series variables, and is important for the interpretation (see the ‘Find out more’ box on spurious correlations that follows). Extra practice: Choose two months and add the CO2 trend data to the temperature dataset from Part 1.1, making sure that the data corresponds to the correct year. Create a separate chart for each month. What do your charts and the correlation coefficients suggest about the relationship between CO2 levels and temperature anomalies? Even though two variables are strongly correlated with each other, this is not necessarily because one variable’s behaviour is the result of the other (a characteristic known as causation). The two variables could be spuriously correlated. The following example illustrates spurious correlation: A child’s academic performance may be positively correlated with the size of the child’s house or the number of rooms in the house, but could we conclude that building an extra room would make a child smarter, or doing well at school would make someone’s house bigger? It is more plausible that income or wealth, which determines the size of home that a family can afford and the resources available for studying, is the ‘unseen factor’ in this relationship. We could also determine whether income is the reason for this spurious correlation by comparing exam scores for children whose parents have similar incomes but different house sizes. If there is no correlation between exam scores and house size, then we can deduce that house size was not ‘causing’ exam scores (or vice versa). See this TEDx Talk for more examples of the dangers of confusing correlation with causation. Consider the example of spurious correlation described above. In your own words, explain spurious correlation and the difference between correlation and causation. Give an example of spurious correlation, similar to the one above, for either CO2 levels or temperature anomalies. Choose an example of spurious correlation from Tyler Vigen’s website. Explain whether you think it is a coincidence, or whether this correlation could be due to one or more other variables. Find out more What makes some correlations spurious? In the spurious correlations website given in Question 6(c), most of the examples you will see involve data series (variables) that are trending (meaning that they tend to increase or decrease over time). If you calculate a correlation between two variables that are trending, you are bound to find a large positive or negative correlation coefficient, even if there is no plausible explanation for a relationship between the two variables. For example, ‘per capita cheese consumption’ (which increases over time due to increased disposable incomes or greater availability of cheese) has a correlation coefficient of 0.95 with the ‘number of people who die from becoming tangled in their bedsheets’ (which also increases over time due to a growing population and a growing availability of bedsheets). The case for our example (the relationship between temperature and CO2 emissions) is slightly different. There is a well-known chemical link between the two. So we understand how CO2 emissions could potentially cause changes in temperature. But in general, do not be tempted to conclude that there is a causal link just because a high correlation coefficient can be seen. Be very cautious when attaching too much meaning to high correlation coefficients when the data displays trending behaviour. This part shows that summary statistics, such as the correlation coefficient, can help identify possible patterns or relationships between variables, but we cannot draw conclusions about causation from them alone. It is also important to think about other explanations for what we see in the data, and whether we would expect there to be a relationship between the two variables. natural experimentAn empirical study exploiting naturally occurring statistical controls in which researchers do not have the ability to assign participants to treatment and control groups, as is the case in conventional experiments. Instead, differences in law, policy, weather, or other events can offer the opportunity to analyse populations as if they had been part of an experiment. The validity of such studies depends on the premise that the assignment of subjects to the naturally occurring treatment and control groups can be plausibly argued to be random. However, there are ways to determine whether there is a causal relationship between two variables, for example, by looking at the scientific processes that connect the variables (as with CO2 and temperature anomalies), or by using a natural experiment. To read more about how natural experiments help evaluate whether one variable causes another, see Section 1.8 of Economy, Society, and Public Policy. In Empirical Project 3, we will take a closer look at natural experiments and how we can use them to identify causal links between variables."
});
index.addDoc({
    id: 10,
    title: "Doing Economics: Empirical Project 1: Working in Google Sheets",
    content: "Empirical Project 1 Working in Google Sheets Part 1.1 The behaviour of average surface temperature over time Learning objectives for this part use line charts to describe the behaviour of real-world variables over time. In the questions below, we look at data from NASA about land-ocean temperature anomalies in the northern hemisphere. Figure 1.1 is constructed using this data, and shows temperatures in the northern hemisphere over the period 1880–2016, expressed as differences from the average temperature from 1951 to 1980. We start by creating charts similar to Figure 1.1, in order to visualize the data and spot patterns more easily. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-01.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Northern hemisphere temperatures (1880–2016). '' /> Figure 1.1 Northern hemisphere temperatures (1880–2016). Before plotting any charts, download the data and make sure you understand how temperature is measured: Go to NASA’s Goddard Institute for Space Studies website. Under the subheading ‘Combined Land-Surface Air and Sea-Surface Water Temperature Anomalies’, select the CSV version of ‘Northern Hemisphere-mean monthly, seasonal, and annual means’ (right-click and select ‘Save Link As…’). The default name of this file is NH.Ts+dSST.csv. Give it a suitable name and upload it to Google Sheets. In this dataset, temperature is measured as ‘anomalies’ rather than absolute temperature. Using NASA’s Frequently Asked Questions section as a reference, explain in your own words what temperature ‘anomalies’ means. Why have researchers chosen this particular measure over other measures (such as absolute temperature)? Now create some line charts using monthly, seasonal, and annual data, which help us look for general patterns over time. Choose one month and plot a line chart with average temperature anomaly on the vertical axis and time (from 1880 to the latest year available) on the horizontal axis. Label each axis appropriately and give your chart a suitable title (Refer to Figure 1.1 as an example.) Google Sheets walk-through 1.1 Drawing a line chart of temperature and time <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to draw a line chart of temperature and time. '' /> Figure 1.2 How to draw a line chart of temperature and time. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the temperature data looks like. Column A has time (in years), Column B has temperature deviations, and Column C contains the average northern hemisphere temperature. We will be using Columns A and B to make the line chart. '' /> The data This is what the temperature data looks like. Column A has time (in years), Column B has temperature deviations, and Column C contains the average northern hemisphere temperature. We will be using Columns A and B to make the line chart. Figure 1.2a This is what the temperature data looks like. Column A has time (in years), Column B has temperature deviations, and Column C contains the average northern hemisphere temperature. We will be using Columns A and B to make the line chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a line chart : Your column chart will look similar to this, with temperature deviation on the vertical axis and time on the horizontal axis. Notice that the horizontal axis currently does not have units (it should be in years). '' /> Draw a line chart Your column chart will look similar to this, with temperature deviation on the vertical axis and time on the horizontal axis. Notice that the horizontal axis currently does not have units (it should be in years). Figure 1.2b Your column chart will look similar to this, with temperature deviation on the vertical axis and time on the horizontal axis. Notice that the horizontal axis currently does not have units (it should be in years). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis variable to years : To display years on the horizontal axis, we need to edit the ‘X-axis’ option. '' /> Change the horizontal axis variable to years To display years on the horizontal axis, we need to edit the ‘X-axis’ option. Figure 1.2c To display years on the horizontal axis, we need to edit the ‘X-axis’ option. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis variable to years : After completing step 8, the horizontal axis will be in time (years). '' /> Change the horizontal axis variable to years After completing step 8, the horizontal axis will be in time (years). Figure 1.2d After completing step 8, the horizontal axis will be in time (years). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-02-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add axis titles and a chart title : Give the axes and the chart appropriate titles. '' /> Add axis titles and a chart title Give the axes and the chart appropriate titles. Figure 1.2e Give the axes and the chart appropriate titles. Extra practice: The columns labelled ‘DJF’, ‘MAM’, ‘JJA’, and ‘SON’ contain seasonal averages (means). For example, the ‘MAM’ column contains the average of the March, April, and May columns for each year. Plot a separate line chart for each season, using average temperature anomaly for that season on the vertical axis and time (from 1880 to the latest year available) on the horizontal axis. The column labelled ‘J–D’ contains the average temperature anomaly for each year. Plot a line chart with annual average temperature anomaly on the vertical axis and time (from 1880 to the latest year available) on the horizontal axis. Your chart should look like Figure 1.1. Extension: Add a horizontal line that intersects the vertical axis at 0, and label it ‘1951–1980 average’. What do your charts from Questions 2 to 4(a) suggest about the relationship between temperature and time? Google Sheets walk-through 1.2 Plotting a line chart and adding a horizontal line <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to plot a line chart and add a horizontal line. '' /> Figure 1.3 How to plot a line chart and add a horizontal line. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a line chart of mean temperature anomaly over time : We are going to draw a line chart for the January to December mean (Column N). '' /> Draw a line chart of mean temperature anomaly over time We are going to draw a line chart for the January to December mean (Column N). Figure 1.3a We are going to draw a line chart for the January to December mean (Column N). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a line chart of mean temperature anomaly over time : After completing step 5, your line chart will look similar to this. Temperature deviation is on the vertical axis and time (in years) is on the horizontal axis. '' /> Draw a line chart of mean temperature anomaly over time After completing step 5, your line chart will look similar to this. Temperature deviation is on the vertical axis and time (in years) is on the horizontal axis. Figure 1.3b After completing step 5, your line chart will look similar to this. Temperature deviation is on the vertical axis and time (in years) is on the horizontal axis. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add a horizontal line showing the 1951–1980 average : To add a horizontal line, we will create a new variable (called ‘1951-1980 average’, shown in Column T) and add it to our line chart. This variable will have the value 0 so it will show up as a horizontal line on the chart. '' /> Add a horizontal line showing the 1951–1980 average To add a horizontal line, we will create a new variable (called ‘1951-1980 average’, shown in Column T) and add it to our line chart. This variable will have the value 0 so it will show up as a horizontal line on the chart. Figure 1.3c To add a horizontal line, we will create a new variable (called ‘1951-1980 average’, shown in Column T) and add it to our line chart. This variable will have the value 0 so it will show up as a horizontal line on the chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add a horizontal line showing the 1951–1980 average : The cells in Column T contain the vertical axis values we need to add to the chart, so we need to select them all. '' /> Add a horizontal line showing the 1951–1980 average The cells in Column T contain the vertical axis values we need to add to the chart, so we need to select them all. Figure 1.3d The cells in Column T contain the vertical axis values we need to add to the chart, so we need to select them all. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-03-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add horizontal and vertical axis titles : After step 12, you chart should look similar to the one shown here. '' /> Add horizontal and vertical axis titles After step 12, you chart should look similar to the one shown here. Figure 1.3e After step 12, you chart should look similar to the one shown here. You now have charts for three different time intervals: month (Question 2), season (Question 3), and year (Question 4). For each time interval, discuss what we can learn about patterns in temperature over time that we might not be able to learn from the charts of other time intervals. Compare your chart from Question 4 to Figure 1.4 which also shows the behaviour of temperature over time using data taken from the National Academy of Sciences. Discuss the similarities and differences between the charts. (For example, are the horizontal and vertical axes variables the same, or do the lines have the same shape?) Looking at the behaviour of temperature over time from 1000 to 1900 in Figure 1.4, are the observed patterns in your chart unusual? Based on your answers to Questions 4 and 5, do you think the government should be concerned about climate change? <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Northern hemisphere temperatures over the long run (1000–2006). '' /> Figure 1.4 Northern hemisphere temperatures over the long run (1000–2006). Part 1.2 Variation in temperature over time Learning objectives for this part summarize data in a frequency table, and visualize distributions with column charts describe a distribution using mean and variance. Aside from changes in the mean temperature, the government is also worried that climate change will result in more frequent extreme weather events. The island has experienced a few major storms and severe heat waves in the past, both of which caused serious damage and disruption to economic activity. Will weather become more extreme and vary more as a result of climate change? A New York Times article uses the same temperature dataset you have been using to investigate the distribution of temperatures and temperature variability over time. Read through the article, paying close attention to the descriptions of the temperature distributions. We can use the mean and median to describe distributions, and we can use deciles to describe parts of distributions. To visualize distributions, we can use column charts in Google Sheets. (For some practice on using these concepts and creating column charts in Google Sheets, see Section 1.3 of Economy, Society, and Public Policy). We are now going to create similar charts of temperature distributions to the ones in the New York Times article, and look at different ways of summarizing distributions. frequency tableA record of how many observations in a dataset have a particular value, range of values, or belong to a particular category. In order to create a column chart using the temperature data we have, we first need to summarize the data using a frequency table. Instead of using deciles to group the data, we use intervals of 0.05, so that temperature anomalies with a value from −0.3 to −0.025 will be in one group, a value greater than −0.025 up until 0.02 in another group, and so on. The frequency table shows us how many values belong to a particular group. Using the monthly data for June, July, and August (columns G to I in your spreadsheet), create two frequency tables similar to Figure 1.5 for the years 1951–1980 and 1981–2010, respectively. The values in the first column should range from −0.3 to 1.05, in intervals of 0.05. Range of temperature anomaly (T) Frequency −0.30 −0.25 … 1.00 1.05 Figure 1.5 A frequency table. Google Sheets walk-through 1.3 Creating a frequency table <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to create a frequency table in Google Sheets. '' /> How to create a frequency table in Google Sheets. Figure 1.6 How to create a frequency table in Google Sheets. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create a table : In this example, we will make a frequency table for the years 1951–1980 in Columns A and B. It’s a good idea to put all the tables in a separate place from the data. '' /> Create a table In this example, we will make a frequency table for the years 1951–1980 in Columns A and B. It’s a good idea to put all the tables in a separate place from the data. Figure 1.6a In this example, we will make a frequency table for the years 1951–1980 in Columns A and B. It’s a good idea to put all the tables in a separate place from the data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create a table : After step 2, your table will look like Figure 1.5. '' /> Create a table After step 2, your table will look like Figure 1.5. Figure 1.6b After step 2, your table will look like Figure 1.5. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the data : It is easier to make a frequency table if you have filtered the data to show only the values you need for the table (the years 1951–1980 in this case). '' /> Filter the data It is easier to make a frequency table if you have filtered the data to show only the values you need for the table (the years 1951–1980 in this case). Figure 1.6c It is easier to make a frequency table if you have filtered the data to show only the values you need for the table (the years 1951–1980 in this case). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use the FREQUENCY function to fill in the rest of the table : Now that the data is filtered, we will use Google Sheets’ FREQUENCY function to fill in Column B. First, select the cells that need to be filled in. '' /> Use the FREQUENCY function to fill in the rest of the table Now that the data is filtered, we will use Google Sheets’ FREQUENCY function to fill in Column B. First, select the cells that need to be filled in. Figure 1.6d Now that the data is filtered, we will use Google Sheets’ FREQUENCY function to fill in Column B. First, select the cells that need to be filled in. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use the FREQUENCY function to fill in the rest of the table : The values in the cells you selected will be used to fill in the frequency table. '' /> Use the FREQUENCY function to fill in the rest of the table The values in the cells you selected will be used to fill in the frequency table. Figure 1.6e The values in the cells you selected will be used to fill in the frequency table. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use the FREQUENCY function to fill in the rest of the table : After step 11, you will have calculated the first entry in Column B. '' /> Use the FREQUENCY function to fill in the rest of the table After step 11, you will have calculated the first entry in Column B. Figure 1.6f After step 11, you will have calculated the first entry in Column B. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-06-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use the FREQUENCY function to fill in the rest of the table : The full formula will be: =FREQUENCY(‘1.3’!G74:103,A3:A30). Note: The values you get may be slightly different to those shown here if you are using the latest data. '' /> Use the FREQUENCY function to fill in the rest of the table The full formula will be: =FREQUENCY(‘1.3’!G74:103,A3:A30). Note: The values you get may be slightly different to those shown here if you are using the latest data. Figure 1.6g The full formula will be: =FREQUENCY(‘1.3’!G74:103,A3:A30). Note: The values you get may be slightly different to those shown here if you are using the latest data. Using the frequency tables from Question 1: Plot two separate column charts for 1951–1980 and 1981–2010 to show the distribution of temperatures, with frequency on the vertical axis and the range of temperature anomaly on the horizontal axis. Your charts should look similar to those in the New York Times article. Using your charts, describe the similarities and differences (if any) between the distributions of temperature anomalies in 1951–1980 and 1981–2010. varianceA measure of dispersion in a frequency distribution, equal to the mean of the squares of the deviations from the arithmetic mean of the distribution. The variance is used to indicate how ‘spread out’ the data is. A higher variance means that the data is more spread out. Example: The set of numbers 1, 1, 1 has zero variance (no variation), while the set of numbers 1, 1, 999 has a high variance of 221,334 (large spread). Now we will use our data to look at different aspects of distributions. First, we will learn how to use deciles to determine which observations are ‘normal’ and ‘abnormal’, and then learn how to use variance to describe the shape of a distribution. The New York Times article considers the bottom third (the lowest or coldest one-third) of temperature anomalies in 1951–1980 as ‘cold’ and the top third (the highest or hottest one-third) of anomalies as ‘hot’. In decile terms, temperatures in the 1st to 3rd decile are ‘cold’ and temperatures in the 7th to 10th decile or above are ‘hot’ (rounded to the nearest decile). Use the PERCENTILE function to determine what values correspond to the 3rd and 7th decile, across all months in 1951–1980. Google Sheets walk-through 1.4 Calculating percentiles <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to use Google Sheets’ PERCENTILE function. '' /> Figure 1.7 How to use Google Sheets’ PERCENTILE function. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : We will be using the same data as in walk-through 1.3. '' /> The data We will be using the same data as in walk-through 1.3. Figure 1.7a We will be using the same data as in walk-through 1.3. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use PERCENTILE to get the value for the 3rd decile : The PERCENTILE function will find the value corresponding to the chosen percentile in the cells you selected. The value 0.3 refers to the 30th percentile, also known as the 3rd decile. '' /> Use PERCENTILE to get the value for the 3rd decile The PERCENTILE function will find the value corresponding to the chosen percentile in the cells you selected. The value 0.3 refers to the 30th percentile, also known as the 3rd decile. Figure 1.7b The PERCENTILE function will find the value corresponding to the chosen percentile in the cells you selected. The value 0.3 refers to the 30th percentile, also known as the 3rd decile. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-07-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use PERCENTILE to get the value for the 7th decile : Repeat step 3 to calculate the value corresponding to the 7th decile. Note: The values you get may be slightly different to those shown here if you are using the latest data. '' /> Use PERCENTILE to get the value for the 7th decile Repeat step 3 to calculate the value corresponding to the 7th decile. Note: The values you get may be slightly different to those shown here if you are using the latest data. Figure 1.7c Repeat step 3 to calculate the value corresponding to the 7th decile. Note: The values you get may be slightly different to those shown here if you are using the latest data. Based on the values you found in Question 3, count the number of anomalies that are considered ‘hot’ in 1981–2010, and express this as a percentage of all the temperature observations in that period. Does your answer suggest that we are experiencing hotter weather more frequently in 1981–2010? (Remember that each decile represents 10% of observations, so 30% of temperatures were considered ‘hot’ in 1951–1980.) Google Sheets walk-through 1.5 Using Google Sheets’ COUNTIF function <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to use Google Sheets’ COUNTIF function. '' /> Figure 1.8 How to use Google Sheets’ COUNTIF function. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the data, keeping the years 1981–2010 : We will be using the years 1981–2010 only. To make the data easier to view, we will filter the data so that only the years 1981–2010 are visible. '' /> Filter the data, keeping the years 1981–2010 We will be using the years 1981–2010 only. To make the data easier to view, we will filter the data so that only the years 1981–2010 are visible. Figure 1.8a We will be using the years 1981–2010 only. To make the data easier to view, we will filter the data so that only the years 1981–2010 are visible. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use COUNTIF to get the number of cells with a value less than the 3rd decile of 1951–1980 : The COUNTIF function counts the number of cells you selected that satisfy a given condition (in this case, having a value less than or equal to the value of the 3rd decile in 1951–1980). '' /> Use COUNTIF to get the number of cells with a value less than the 3rd decile of 1951–1980 The COUNTIF function counts the number of cells you selected that satisfy a given condition (in this case, having a value less than or equal to the value of the 3rd decile in 1951–1980). Figure 1.8b The COUNTIF function counts the number of cells you selected that satisfy a given condition (in this case, having a value less than or equal to the value of the 3rd decile in 1951–1980). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use COUNTIF to get the number of cells with a value greater than the 7th decile of 1951–1980 : Now, the condition is that values should be greater than or equal to the value of the 7th decile in 1951–1980. '' /> Use COUNTIF to get the number of cells with a value greater than the 7th decile of 1951–1980 Now, the condition is that values should be greater than or equal to the value of the 7th decile in 1951–1980. Figure 1.8c Now, the condition is that values should be greater than or equal to the value of the 7th decile in 1951–1980. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-08-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use the numbers obtained to calculate percentages : COUNTIF gives us numbers, but to convert these into percentages we need to divide the numbers from COUNTIF by the total number of observations. Note: The values you get may be slightly different to those shown here if you are using the latest data. '' /> Use the numbers obtained to calculate percentages COUNTIF gives us numbers, but to convert these into percentages we need to divide the numbers from COUNTIF by the total number of observations. Note: The values you get may be slightly different to those shown here if you are using the latest data. Figure 1.8d COUNTIF gives us numbers, but to convert these into percentages we need to divide the numbers from COUNTIF by the total number of observations. Note: The values you get may be slightly different to those shown here if you are using the latest data. The New York Times article discusses whether temperatures have become more variable over time. One way to measure temperature variability is by calculating the variance of the temperature distribution. For each season (‘DJF’, ‘MAM’, ‘JJA’, and ‘SON’): Calculate the mean (average) and variance separately for the following time periods: 1921–1950, 1951–1980, and 1981–2010. For each season, compare the variances in different periods, and explain whether or not temperature appears to be more variable in later periods. Google Sheets walk-through 1.6 Calculating and understanding the variance <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to calculate variance. '' /> Figure 1.9 How to calculate variance. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The temperature data : This data is temperature anomalies for 1981–2010, showing the months March to May only. The column chart shows what the data looks like. Each column shows how many values fall within the ranges shown on the horizontal axis. For example, the leftmost column tells us that 25 values are less than 0.3. Note: The values in your dataset may be slightly different to those shown here, if you are using the latest data. '' /> The temperature data This data is temperature anomalies for 1981–2010, showing the months March to May only. The column chart shows what the data looks like. Each column shows how many values fall within the ranges shown on the horizontal axis. For example, the leftmost column tells us that 25 values are less than 0.3. Note: The values in your dataset may be slightly different to those shown here, if you are using the latest data. Figure 1.9a This data is temperature anomalies for 1981–2010, showing the months March to May only. The column chart shows what the data looks like. Each column shows how many values fall within the ranges shown on the horizontal axis. For example, the leftmost column tells us that 25 values are less than 0.3. Note: The values in your dataset may be slightly different to those shown here, if you are using the latest data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Made up data that is less spread out : This data on the right is made up. From this chart, you can see that the values are all quite close together, with the smallest value being 0.6 and the largest being 0.8. Comparing the charts, the real temperature data looks more spread out than the made up data. '' /> Made up data that is less spread out This data on the right is made up. From this chart, you can see that the values are all quite close together, with the smallest value being 0.6 and the largest being 0.8. Comparing the charts, the real temperature data looks more spread out than the made up data. Figure 1.9b This data on the right is made up. From this chart, you can see that the values are all quite close together, with the smallest value being 0.6 and the largest being 0.8. Comparing the charts, the real temperature data looks more spread out than the made up data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-09-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculating and interpreting the variance : The formula shown calculates the variance of the real temperature data. As expected, it is much larger than the variance of the made up data. '' /> Calculating and interpreting the variance The formula shown calculates the variance of the real temperature data. As expected, it is much larger than the variance of the made up data. Figure 1.9c The formula shown calculates the variance of the real temperature data. As expected, it is much larger than the variance of the made up data. Using the findings of the New York Times article and your answers to Questions 1 to 5, discuss whether temperature appears to be more variable over time. Would you advise the government to spend more money on mitigating the effects of extreme weather events? correlationA measure of how closely related two variables are. Two variables are correlated if knowing the value of one variable provides information on the likely value of the other, for example high values of one variable being commonly observed along with high values of the other variable. Correlation can be positive or negative. It is negative when high values of one variable are observed with low values of the other. Correlation does not mean that there is a causal relationship between the variables. Example: When the weather is hotter, purchases of ice cream are higher. Temperature and ice cream sales are positively correlated. On the other hand, if purchases of hot beverages decrease when the weather is hotter, we say that temperature and hot beverage sales are negatively correlated. Part 1.3 Carbon emissions and the environment Learning objectives for this part use scatterplots and the correlation coefficient to assess the degree of association between two variables explain what correlation measures and the limitations of correlation. The government has heard that carbon emissions could be responsible for climate change, and has asked you to investigate whether this is the case. To do so, we are now going to look at carbon emissions over time, and use another type of chart (scatter charts) to show their relationship with temperature anomalies. One way to measure the relationship between two variables is correlation. Google Sheets walk-through 1.7 explains what correlation is and how to calculate it in Google Sheets. In the questions below, we will make charts using the CO2 data from the US National Oceanic and Atmospheric Administration. Download the Excel spreadsheet containing this data and upload it to Google Sheets. The CO2 data was recorded from one Observatory in Mauna Loa. Using an Earth System Research Laboratory article as a reference, explain whether or not you think this data is a reliable representation of the global atmosphere. The variables ‘trend’ and ‘interpolated’ are similar, but not identical. In your own words, explain the difference between these two measures of CO2 levels. Why might there be seasonal variation in CO2 levels? Now we will use a line chart to look for general patterns over time. Plot a line chart with interpolated and trend CO2 levels on the vertical axis and time (starting from January 1960) on the horizontal axis. Label the axes and the chart legend, and give your chart an appropriate title. What does this chart suggest about the relationship between CO2 and time? correlation coefficientA numerical measure, ranging between 1 and −1, of how closely associated two variables are—whether they tend to rise and fall together, or move in opposite directions. A positive coefficient indicates that when one variable takes a high (low) value, the other tends to be high (low) too, and a negative coefficient indicates that when one variable is high the other is likely to be low. A value of 1 or −1 indicates that knowing the value of one of the variables would allow you to perfectly predict the value of the other. A value of 0 indicates that knowing one of the variables provides no information about the value of the other. We will now combine the CO2 data with the temperature data from Part 1.1, and then examine the relationship between these two variables visually, using scatterplots, and statistically, using the correlation coefficient. If you have not yet downloaded the temperature data, follow the instructions in Part 1.1. Choose one month and add the CO2 trend data to the temperature dataset from Part 1.1, making sure that the data corresponds to the correct year. Make a scatterplot of CO2 level on the vertical axis and temperature anomaly on the horizontal axis. Calculate and interpret the (Pearson) correlation coefficient between these two variables. Discuss the shortcomings of using this coefficient to summarize the relationship between variables. Google Sheets walk-through 1.7 Calculating correlation and drawing a scatterplot <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to create scatterplots and calculate the correlation coefficient. '' /> Figure 1.10 How to create scatterplots and calculate the correlation coefficient. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The CO2 data : This is what the monthly CO2 data looks like. Column A has the years (1958–present), Column B has the months (1–12), and Columns C to E have different measures of CO2 emissions. We will use the Trend data (Column E) to make a scatterplot of CO2 emissions and temperature anomalies for January. You will need to have the temperature anomaly spreadsheet open because we will be using it later. '' /> The CO2 data This is what the monthly CO2 data looks like. Column A has the years (1958–present), Column B has the months (1–12), and Columns C to E have different measures of CO2 emissions. We will use the Trend data (Column E) to make a scatterplot of CO2 emissions and temperature anomalies for January. You will need to have the temperature anomaly spreadsheet open because we will be using it later. Figure 1.10a This is what the monthly CO2 data looks like. Column A has the years (1958–present), Column B has the months (1–12), and Columns C to E have different measures of CO2 emissions. We will use the Trend data (Column E) to make a scatterplot of CO2 emissions and temperature anomalies for January. You will need to have the temperature anomaly spreadsheet open because we will be using it later. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the CO2 data according to your chosen month : After completing step 3, only the data for the selected months is shown in the spreadsheet. Data for the other months is still there, but it’s hidden. '' /> Filter the CO2 data according to your chosen month After completing step 3, only the data for the selected months is shown in the spreadsheet. Data for the other months is still there, but it’s hidden. Figure 1.10b After completing step 3, only the data for the selected months is shown in the spreadsheet. Data for the other months is still there, but it’s hidden. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the CO2 data according to your chosen month : We need to copy and paste the data in Column E into the temperature anomalies spreadsheet. Filtering the data first means that only these values will be copied (rather than all values in Column E). '' /> Filter the CO2 data according to your chosen month We need to copy and paste the data in Column E into the temperature anomalies spreadsheet. Filtering the data first means that only these values will be copied (rather than all values in Column E). Figure 1.10c We need to copy and paste the data in Column E into the temperature anomalies spreadsheet. Filtering the data first means that only these values will be copied (rather than all values in Column E). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the temperature data to correspond to the years present in the CO2 data : The values in Column A (years) need to match with the years available in the CO2 data. We will filter the data so that temperature data is only visible for the years that CO2 data is available. '' /> Filter the temperature data to correspond to the years present in the CO2 data The values in Column A (years) need to match with the years available in the CO2 data. We will filter the data so that temperature data is only visible for the years that CO2 data is available. Figure 1.10d The values in Column A (years) need to match with the years available in the CO2 data. We will filter the data so that temperature data is only visible for the years that CO2 data is available. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Paste the CO2 data into the temperature data spreadsheet : When pasting the CO2 data, pay particular attention to the year in the first space. For March to December, the first year in the CO2 data is 1958. For January or February, the first year in the CO2 data is 1959. If the data has been filtered correctly, after step 9 the CO2 data will be matched with the corresponding year. '' /> Paste the CO2 data into the temperature data spreadsheet When pasting the CO2 data, pay particular attention to the year in the first space. For March to December, the first year in the CO2 data is 1958. For January or February, the first year in the CO2 data is 1959. If the data has been filtered correctly, after step 9 the CO2 data will be matched with the corresponding year. Figure 1.10e When pasting the CO2 data, pay particular attention to the year in the first space. For March to December, the first year in the CO2 data is 1958. For January or February, the first year in the CO2 data is 1959. If the data has been filtered correctly, after step 9 the CO2 data will be matched with the corresponding year. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a scatterplot for temperature anomaly and CO2 level : Select the data that will be used, and open the chart editor. '' /> Draw a scatterplot for temperature anomaly and CO2 level Select the data that will be used, and open the chart editor. Figure 1.10f Select the data that will be used, and open the chart editor. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a scatterplot for temperature anomaly and CO2 level : After completing step 12, your scatterplot will look similar to this chart, with CO2 level on the vertical axis and temperature anomaly on the horizontal axis. '' /> Draw a scatterplot for temperature anomaly and CO2 level After completing step 12, your scatterplot will look similar to this chart, with CO2 level on the vertical axis and temperature anomaly on the horizontal axis. Figure 1.10g After completing step 12, your scatterplot will look similar to this chart, with CO2 level on the vertical axis and temperature anomaly on the horizontal axis. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-h.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-h-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-h-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-h-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-h.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the range of the vertical axis : Currently the points are clustered in the top half of the graph. To see them more clearly, we need to change the range of the vertical axis. After completing step 14, you will see the individual data points more clearly. '' /> Change the range of the vertical axis Currently the points are clustered in the top half of the graph. To see them more clearly, we need to change the range of the vertical axis. After completing step 14, you will see the individual data points more clearly. Figure 1.10h Currently the points are clustered in the top half of the graph. To see them more clearly, we need to change the range of the vertical axis. After completing step 14, you will see the individual data points more clearly. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-i.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-i-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-i-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-i-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-i.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add titles : Give the axes and the chart appropriate titles. '' /> Add titles Give the axes and the chart appropriate titles. Figure 1.10i Give the axes and the chart appropriate titles. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-j.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-j-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-j-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-j-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-j.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate and interpret the correlation coefficient : The correlation coefficient we use here tells us how close the data is to resembling an upward- or downward-sloping straight line on a scatterplot. The correlation coefficient ranges from −1 to 1. A coefficient of 1 or −1 means that there is a perfect upward- or downward-sloping linear relationship between the two variables, while a coefficient of 0 means that there is no systematic upward- or downward-sloping linear relationship between the two variables. Note: The coefficient you get may be slightly different if you are using the latest data. '' /> Calculate and interpret the correlation coefficient The correlation coefficient we use here tells us how close the data is to resembling an upward- or downward-sloping straight line on a scatterplot. The correlation coefficient ranges from −1 to 1. A coefficient of 1 or −1 means that there is a perfect upward- or downward-sloping linear relationship between the two variables, while a coefficient of 0 means that there is no systematic upward- or downward-sloping linear relationship between the two variables. Note: The coefficient you get may be slightly different if you are using the latest data. Figure 1.10j The correlation coefficient we use here tells us how close the data is to resembling an upward- or downward-sloping straight line on a scatterplot. The correlation coefficient ranges from −1 to 1. A coefficient of 1 or −1 means that there is a perfect upward- or downward-sloping linear relationship between the two variables, while a coefficient of 0 means that there is no systematic upward- or downward-sloping linear relationship between the two variables. Note: The coefficient you get may be slightly different if you are using the latest data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-k.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-k-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-k-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-k-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-k.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate and interpret the correlation coefficient : Here are some more examples of correlation coefficients and how to interpret them. Note: The word ‘strong’ is used for coefficients that are close to 1 or −1, and ‘weak’ is used for coefficients that are close to 0, though there is no precise range of values that are considered ‘strong’ or ‘weak’. '' /> Calculate and interpret the correlation coefficient Here are some more examples of correlation coefficients and how to interpret them. Note: The word ‘strong’ is used for coefficients that are close to 1 or −1, and ‘weak’ is used for coefficients that are close to 0, though there is no precise range of values that are considered ‘strong’ or ‘weak’. Figure 1.10k Here are some more examples of correlation coefficients and how to interpret them. Note: The word ‘strong’ is used for coefficients that are close to 1 or −1, and ‘weak’ is used for coefficients that are close to 0, though there is no precise range of values that are considered ‘strong’ or ‘weak’. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-l.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-l-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-l-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-l-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-l.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Limitations of the correlation coefficient : One limitation of this correlation measure is that it only tells us about the strength of the linear relationship between two variables. The correlation coefficient cannot tell us if the two variables have a different kind of relationship (e.g. a wavy line). For example, suppose your electricity bill is high in winter but low in summer (the U-shaped pattern shown in the chart). This correlation measure cannot detect the U-shaped pattern, understating the actual association between the two variables. Note: There are other measures of correlation that can deal with this issue, but we will only look at this correlation measure (called Pearson correlation) for now. '' /> Limitations of the correlation coefficient One limitation of this correlation measure is that it only tells us about the strength of the linear relationship between two variables. The correlation coefficient cannot tell us if the two variables have a different kind of relationship (e.g. a wavy line). For example, suppose your electricity bill is high in winter but low in summer (the U-shaped pattern shown in the chart). This correlation measure cannot detect the U-shaped pattern, understating the actual association between the two variables. Note: There are other measures of correlation that can deal with this issue, but we will only look at this correlation measure (called Pearson correlation) for now. Figure 1.10l One limitation of this correlation measure is that it only tells us about the strength of the linear relationship between two variables. The correlation coefficient cannot tell us if the two variables have a different kind of relationship (e.g. a wavy line). For example, suppose your electricity bill is high in winter but low in summer (the U-shaped pattern shown in the chart). This correlation measure cannot detect the U-shaped pattern, understating the actual association between the two variables. Note: There are other measures of correlation that can deal with this issue, but we will only look at this correlation measure (called Pearson correlation) for now. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-m.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-m-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-m-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-m-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-01-10-m.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Limitations of the correlation coefficient : Remember that correlation is a measure of association and does not imply that one variable causes the other. Spurious correlation is when two variables are strongly correlated but there is no direct relationship between them. In this example, the number of insurance scams in your city is strongly correlated with your monthly electricity bill (almost a perfectly straight line), but it is difficult to argue that your electricity consumption caused the scams, or vice versa. Similarly, we cannot use correlation or scatterplots to predict the value of either variable outside the range shown. For example, from this chart, we cannot conclude that when there are 45 insurance scams, your electricity bill will be higher than $37. We can only use correlation and scatterplots to summarize and look at the data we have. '' /> Limitations of the correlation coefficient Remember that correlation is a measure of association and does not imply that one variable causes the other. Spurious correlation is when two variables are strongly correlated but there is no direct relationship between them. In this example, the number of insurance scams in your city is strongly correlated with your monthly electricity bill (almost a perfectly straight line), but it is difficult to argue that your electricity consumption caused the scams, or vice versa. Similarly, we cannot use correlation or scatterplots to predict the value of either variable outside the range shown. For example, from this chart, we cannot conclude that when there are 45 insurance scams, your electricity bill will be higher than $37. We can only use correlation and scatterplots to summarize and look at the data we have. Figure 1.10m Remember that correlation is a measure of association and does not imply that one variable causes the other. Spurious correlation is when two variables are strongly correlated but there is no direct relationship between them. In this example, the number of insurance scams in your city is strongly correlated with your monthly electricity bill (almost a perfectly straight line), but it is difficult to argue that your electricity consumption caused the scams, or vice versa. Similarly, we cannot use correlation or scatterplots to predict the value of either variable outside the range shown. For example, from this chart, we cannot conclude that when there are 45 insurance scams, your electricity bill will be higher than $37. We can only use correlation and scatterplots to summarize and look at the data we have. Extra practice: Choose two months and add the CO2 trend data to the temperature dataset from Part 1.1, making sure that the data corresponds to the correct year. Create a separate chart for each month. What do your charts and the correlation coefficients suggest about the relationship between CO2 levels and temperature anomalies? causationA direction from cause to effect, establishing that a change in one variable produces a change in another. While a correlation gives an indication of whether two variables move together (either in the same or opposite directions), causation means that there is a mechanism that explains this association. Example: We know that higher levels of CO2 in the atmosphere lead to a greenhouse effect, which warms the Earth’s surface. Therefore we can say that higher CO2 levels are the cause of higher surface temperatures.spurious correlationA strong linear association between two variables that does not result from any direct relationship, but instead may be due to coincidence or to another unseen factor. Even though two variables are strongly correlated with each other, it is not necessarily the case that one variable’s behaviour is the result of the other (a characteristic known as causation). The two variables could be spuriously correlated. The following example illustrates spurious correlation: A child’s academic performance may be positively correlated with the number of rooms in their house or house size, but could we conclude that building an extra room would make a child smarter, or doing well at school would make your house bigger? It is more plausible that income or wealth, which determines the size of home that a family can afford and the resources available for studying, is the ‘unseen factor’ in this relationship. We could also determine whether income is the reason for this spurious correlation by comparing exam scores for children whose parents have similar income but different house sizes. If there is no correlation between exam scores and house size, then we can deduce that house size was not ‘causing’ exam scores (or vice versa). See this TEDx Talk for more examples of the dangers of confusing correlation with causation. Consider the example of spurious correlation described above. In your own words, explain spurious correlation and the difference between correlation and causation. Give an example of spurious correlation, similar to the one above, for either CO2 levels or temperature anomalies. Choose an example of spurious correlation from Tyler Vigen’s website. Explain whether you think it is a coincidence, or whether this correlation could be due to one or more other variables. Find out more What makes some correlations spurious? In the spurious correlations website given in Question 6(c), most of the examples you will see involve data series that are trending (meaning that they tend to increase or decrease over time). If you calculate a correlation between two series that are trending, you are bound to find a large positive or negative correlation coefficient, even if there is no plausible explanation for a relationship between the two series. For example, ‘per capita cheese consumption’ (which increases over time due to increased disposable incomes or greater availability of cheese) has a correlation coefficient of 0.95 with the ‘number of people who die from becoming tangled in their bedsheets’ (which also increases over time due to a growing population and a growing availability of bedsheets). natural experimentAn empirical study exploiting naturally occurring statistical controls in which researchers do not have the ability to assign participants to treatment and control groups, as is the case in conventional experiments. Instead, differences in law, policy, weather, or other events can offer the opportunity to analyse populations as if they had been part of an experiment. The validity of such studies depends on the premise that the assignment of subjects to the naturally occurring treatment and control groups can be plausibly argued to be random. The case for our example (the relationship between temperature and CO2 emissions) is slightly different. There is a well-known chemical link between the two. So we understand how CO2 emissions could potentially cause changes in temperature. But in general, do not be tempted to conclude that there is a causal link just because a high correlation coefficient can be seen. Be very cautious when attaching too much meaning to high correlation coefficients when data displays trending behaviour. This part shows that summary statistics, such as the correlation coefficient, can help identify possible patterns or relationships between variables, but we cannot make conclusions about causation from them alone. It is also important to think about other explanations for what we see in the data, and whether we would expect there to be a relationship between the two variables. However, there are ways to determine whether there is a causal relationship between two variables, for example, by looking at the scientific processes that connect the variables (as with CO2 and temperature anomalies), or by using a natural experiment. To read more about how natural experiments help evaluate whether one variable causes another, see Section 1.8 of Economy, Society, and Public Policy. In Empirical Project 3, we will take a closer look at natural experiments and how we can use them to identify causal links between variables."
});
index.addDoc({
    id: 11,
    title: "Doing Economics: Empirical Project 1 Solutions",
    content: "Empirical Project 1 Solutions These are not model answers. They are provided to help students, including those doing the project outside a formal class, to check their progress while working through the questions using the Excel, R, or Google Sheets walk-throughs. There are also brief notes for the more interpretive questions. Students taking courses using Doing Economics should follow the guidance of their instructors. Note These solutions are based on data downloaded in January 2018. Your solutions may differ slightly if using more updated data. Part 1.1 The behaviour of average surface temperature over time According to the source mentioned in the question: ‘Temperature anomalies indicate how much warmer or colder it is than normal for a particular place and time. For the GISS analysis, normal always means the average over the 30-year period 1951–1980 for that place and time of year. This base period is specific to GISS, not universal. But note that trends do not depend on the choice of the base period: If the absolute temperature at a specific location is two degrees higher than a year ago, so is the corresponding temperature anomaly, no matter what base period is selected, since the normal temperature used as base point is the same for both years. Note that regional mean anomalies (in particular global anomalies) are not computed from the current absolute mean and the 1951–1980 mean for that region, but from station temperature anomalies. Finding absolute regional means encounters significant difficulties that create large uncertainties. This is why the GISS analysis deals with anomalies rather than absolute temperatures. For a more detailed discussion of that topic, see The elusive absolute surface air temperature.’ There are many valid ways to summarize this in your own words, for example: Temperature anomaly measures, at any given place and time, the difference between observed temperature and the reference long-term average, or ‘normal’ temperature value. The long-term average is typically computed by averaging 30 or more years of data. The GISS analysis, for example, uses the average temperature from the period 1951–1980. A positive anomaly indicates that the observed temperature is warmer than the baseline long-term average temperature. The use of anomalies, compared to other measures such as absolute temperature, allows for a more accurate representation of temperatures over larger areas. It provides a frame of reference that allows for more meaningful comparisons between locations. The compilation of other indicators, such as absolute average temperatures, are difficult and controversial. The absolute average temperature data is more susceptible to uncertainties and inaccuracies. Temperature stations are unevenly distributed, in regions with very few stations, interpolation must be made over large areas. The temperature at a mountain top is lower than at the bottom. If a mountainous area is in general cooler than the baseline in a given month, the anomaly will show that temperatures for both locations (the top and bottom areas of a mountain) are below the reference value. If we use absolute temperature, however, the disparity between the measures at a mountain top and bottom would be quite large. Using anomalies also diminishes problems when stations are added, removed or missing. Solution figure 1.1 shows an example chart for January. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-01.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-01-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-01-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-01-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-01.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''An example of a line chart with average temperature anomaly for January on the vertical axis and time (1880–2016) on the horizontal axis. '' /> An example of a line chart with average temperature anomaly for January on the vertical axis and time (1880–2016) on the horizontal axis. Solution figure 1.1 An example of a line chart with average temperature anomaly for January on the vertical axis and time (1880–2016) on the horizontal axis. Line charts for each season, using average temperature anomaly for that season on the vertical axis and time (1880–2016) on the horizontal axis, are shown in Solution figures 1.2–1.5. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-02.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-02-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-02-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-02-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-02.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''A line chart showing average temperature anomaly for spring. '' /> A line chart showing average temperature anomaly for spring. Solution figure 1.2 A line chart showing average temperature anomaly for spring. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''A line chart showing average temperature anomaly for summer. '' /> A line chart showing average temperature anomaly for summer. Solution figure 1.3 A line chart showing average temperature anomaly for summer. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''A line chart showing average temperature anomaly for autumn. '' /> A line chart showing average temperature anomaly for autumn. Solution figure 1.4 A line chart showing average temperature anomaly for autumn. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''A line chart showing average temperature anomaly for winter. '' /> A line chart showing average temperature anomaly for winter. Solution figure 1.5 A line chart showing average temperature anomaly for winter. An example of a line chart showing the annual average temperature anomaly is shown in Solution figure 1.6. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-01-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''A line chart with annual average temperature anomaly on the vertical axis and time (1880–2016) on the horizontal axis. '' /> A line chart with annual average temperature anomaly on the vertical axis and time (1880–2016) on the horizontal axis. Solution figure 1.6 A line chart with annual average temperature anomaly on the vertical axis and time (1880–2016) on the horizontal axis. Solution figures 1.1 and 1.6 show that temperature has been increasing over time. The average temperature anomaly initially fluctuated around a relatively low mean between 1880 and 1920, and then fluctuated around a higher mean value between 1920 and 1975. From about 1975, temperature anomalies were positive and displayed an increasing trend, indicating that absolute temperatures are also increasing. The overall positive relationship between temperature and time shown by the charts provides evidence to support the presence of global warming. We are concerned about global temperature changes over time, so it is vital to look at the behaviour of the same variable over time. While averages of temperature taken over longer periods (one year or one decade) are more useful at revealing the overall trend of global warming, the averages taken over shorter periods (such as seasons) can give us more detailed information about the underlying mechanisms of global warming. Seasonal and monthly data can help us see the difference in patterns compared to what we observe in the annual data. For example, we could see if the rising annual average temperatures are due to temperatures rising only in a few months, or due to temperatures rising in all months. Figure 1.4 is reproduced below. Both charts show temperature anomalies over time, but Solution figure 1.6 uses a shorter time frame (the years 1880–present, compared to 1000–2006). The vertical axis variables are slightly different: Solution figure 1.6 uses the deviation from the mean temperature in 1951–1980, while Figure 1.4 uses deviation from the mean temperature in 1961–1990. Both charts show a similar pattern in temperature anomalies from 1880 onwards. However, Figure 1.4 shows that temperatures have undergone large fluctuations in the past (for example from 1300–1450), and there were certain points in time before the Industrial Revolution where temperatures were similar to the 1951–1980 levels (for example, 1000 and the late 1300s–early 1400s). Taken together, the charts suggest that there are probably many reasons for temperature fluctuations (not just human activity), but that temperatures have increased since the 1980s, to levels never seen in the past millennium. The government should therefore be concerned about climate change. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-01-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Northern hemisphere temperatures over the long run (1000–2006). '' /> Northern hemisphere temperatures over the long run (1000–2006). Figure 1.4 Northern hemisphere temperatures over the long run (1000–2006). Part 1.2 Variation in temperature over time Solution figures 1.7 and 1.8 show the variation in temperature over time for the periods 1951–1980 and 1981–2010 respectively. Range of temperature anomaly (T) Frequency −0.30 0 −0.25 2 −0.20 7 −0.15 6 −0.10 8 −0.05 12 0.00 7 0.05 14 0.10 12 0.15 11 0.20 8 0.25 3 0.30 0 0.35 0 0.40 0 0.45 0 0.50 0 0.55 0 0.60 0 0.65 0 0.70 0 0.75 0 0.80 0 0.85 0 0.90 0 0.95 0 1.00 0 1.05 0 A frequency table for 1951–1980. Solution figure 1.7 A frequency table for 1951–1980. Range of temperature anomaly (T) Frequency −0.30 0 −0.25 0 −0.20 0 −0.15 0 −0.10 1 −0.05 3 0.00 2 0.05 7 0.10 1 0.15 5 0.20 3 0.25 4 0.30 7 0.35 7 0.40 4 0.45 6 0.50 8 0.55 4 0.60 4 0.65 5 0.70 7 0.75 4 0.80 6 0.85 2 0.90 0 0.95 0 1.00 0 1.05 0 A frequency table for 1981–2010. Solution figure 1.8 A frequency table for 1981–2010. Solution figures 1.9 and 1.10 are column charts showing the distribution of temperatures for 1951–1980 and 1981–2010. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-02-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-02-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-02-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-02-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-02-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''A column chart for 1951–1980. '' /> A column chart for 1951–1980. Solution figure 1.9 A column chart for 1951–1980. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-02-02-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-02-02-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-02-02-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-02-02-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-02-02-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''A column chart for 1981–2010. '' /> A column chart for 1981–2010. Solution figure 1.10 A column chart for 1981–2010. From Solution figures 1.9 and 1.10, you can see that the distribution of summer temperature anomalies has been shifting to the right. This means more and more months have been hotter than they were before. The distribution in 1981–2010 is also flatter than that of 1951–1980. This, however, does not necessarily mean that temperature variability around the world has been increasing. Many climate scientists have pointed out that this is merely a reflection of the fact that different parts of the world are warming up at different speeds. In the period 1951–1980, the value corresponding to the 3rd decile is −0.1, and the value corresponding to the 7th decile is 0.11. Temperature anomalies below −0.1 are therefore considered ‘cold’, and temperature anomalies above 0.11 are considered ‘hot’. In the period 1981–2010, 2.2% of temperatures would be considered ‘cold’ (compared to 30% in 1951–1980), and 84.4% of temperatures would be considered ‘hot’ (compared to 30% in 1951–1980). The increase in the percentage of ‘hot’ weather and decrease in the percentage of ‘cold’ weather suggests that we are experiencing hotter weather more frequently in 1981–2010. Solution figure 1.11 shows the mean and variance separately for the following time periods: 1921–1950, 1951–1980, and 1981–2010. 1921–1950 1951–1980 1981–2010 Mean DJF −0.06 −0.0007 0.52 MAM −0.06 0.0007 0.50 JJA −0.05 0.0007 0.41 SON 0.07 0.0003 0.43 Variance DJF 0.06 0.05 0.07 MAM 0.03 0.03 0.07 JJA 0.02 0.01 0.06 SON 0.02 0.03 0.10 Mean and variance per season for periods 1921–1950, 1951–1980, and 1981–2010. Solution figure 1.11 Mean and variance per season for periods 1921–1950, 1951–1980, and 1981–2010. Temperature in most seasons appears to be more variable in 1981–2010 compared to 1951–1980 or 1921–1950 (and the mean anomaly in each season has increased in each period for most seasons). The temperature anomalies in DJF have a larger variance than those in JJA. The variance in DJF is about three times larger than that in JJA, particularly until 1980. For the period 1981–2010, the JJA temperature anomalies start becoming more variable. The column charts in Question 2(a) (Solution figures 1.9 and 1.10) and the table in Question 5(a) (Solution figure 1.11) suggest that temperatures are becoming more variable over time. However, (as stated in the article), many scientists argue that the increasing spread is merely a reflection of the fact that some regions of the world are warming faster than others. The government should look at temperature variability in their particular region to see if there is a similar pattern to that of the northern hemisphere in general, before deciding how much to spend on mitigating the effects of extreme weather events. Part 1.3 Carbon emissions and the environment The location of the observatory at the summit of Mauna Loa means the data is representative of the globe. This is because the observatory is surrounded by miles of bare lava, eliminating the influence of CO2 absorbed or emitted locally by plants, soils, and human activities. Data collected at the observatory is selected to minimize the effects of anomalies and other shocks. The measurements are also calibrated rigorously and frequently, and compared to others taken at laboratories using different methods. Systematic and persistent biases are less than 0.2 ppm (parts per million), indicating that the measurements are highly accurate. Both measures are constructed based on monthly mean CO2 mole fraction. The mole fraction of CO2, expressed as parts per million (ppm), is the number of molecules of CO2 in every one million molecules of dried air (water vapor removed). The trend mean mole fraction for each month is determined by removing the seasonal cycles. Trend values are linearly interpolated for missing values. The interpolated value is the sum of the average seasonal cycle value and the trend value. The data shows that the levels of CO2 are typically higher when recorded in spring and summer as plants absorb and consume more CO2 in these months than in autumn and winter. During autumn and winter, plants decrease photosynthesis and become net producers of CO2. Many scientists argue that climate change has led to higher rates of photosynthesis during growth seasons and higher rates of exhalation in autumn and winter. Climate change can therefore increase the seasonality of CO2 levels. The graph in Solution figure 1.12 suggests a positive relationship between CO2 and time. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Trend and interpolated monthly mean CO2 (mole fraction). '' /> Trend and interpolated monthly mean CO2 (mole fraction). Solution figure 1.12 Trend and interpolated monthly mean CO2 (mole fraction). The scatterplot in Solution figure 1.13 plots CO2 levels against temperature anomaly for June. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Scatterplot CO2 vs temperature (June). '' /> Scatterplot CO2 vs temperature (June). Solution figure 1.13 Scatterplot CO2 vs temperature (June). Solution figure 1.13 shows data for the month of June. The (Pearson) correlation coefficient is 0.92, indicating a strong positive linear association between the two variables. When CO2 levels increase, temperatures increase. Limitations of the Pearson correlation coefficient include: an inability to detect non-linear relationships between variables (for example, if data followed a U-shaped pattern). The correlation coefficient simply measures the strength of the linear relationship between variables. an inability to determine whether there is a causal relationship between the variables. The months chosen to produce Solution figures 1.14 and 1.15 are January and December. The correlation coefficient for January is 0.82, and the correlation coefficient for December is 0.81, both indicating a strong positive linear correlation. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''A scatterplot showing CO2 levels and temperature anomaly for January. '' /> A scatterplot showing CO2 levels and temperature anomaly for January. Solution figure 1.14 A scatterplot showing CO2 levels and temperature anomaly for January. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-05-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-05-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-05-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-05-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-01-03-05-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''A scatterplot showing CO2 levels and temperature anomaly for December. '' /> A scatterplot showing CO2 levels and temperature anomaly for December. Solution figure 1.15 A scatterplot showing CO2 levels and temperature anomaly for December. Spurious correlation occurs when two variables appear to be correlated while there is not actually a cause-and-effect relationship between them. The correlation observed can be due to coincidence or a third variable driving both variables. Two events or variables are correlated when they happen or change together. Causation occurs when one variable changes as a result of changes in another variable. An example related to CO2 levels is that the number of people attaining primary, secondary, or university education has increased over time, and so have CO2 levels. However, it’s difficult to argue that CO2 levels cause more people to go to school, or that more people going to school would directly cause CO2 levels to rise. Rather, it’s more likely that greater educational attainment is an indirect effect of economic development, which is the cause of increased CO2 levels. The number of people who have drowned by falling into a pool is closely correlated to the number of films that Nicolas Cage has appeared in. The correlation is a result of coincidence as there are no reasonable and testable links between the variables. Another example is US spending on science, space, and technology is correlated with suicides by hanging, strangulation, and suffocation. The two variables could both be driven by a common third variable. For example, rising levels of competition in modern societies can raise the need for technological progress and excellence, affecting both variables positively. This causes the two variables to be correlated."
});
index.addDoc({
    id: 12,
    title: "Doing Economics: Empirical Project 2: Collecting and analysing data from experiments",
    content: "Empirical Project 2 Collecting and analysing data from experiments Learning objectives In this project you will: collect data from an experiment and enter it into a spreadsheet (Part 2.1) use summary measures, for example, mean and standard deviation, and line and column charts to describe and compare data (Parts 2.1 and 2.2) calculate and interpret the p-value (Part 2.3) evaluate the usefulness of experiments for determining causality, and the limitations of these experiments (Part 2.3). Key concepts Concepts needed for this project: mean, variance, correlation, and causation. Concepts introduced in this project: standard deviation, range, and p-value. Introduction CORE projects This empirical project is related to material in: Unit 2 of Economy, Society, and Public Policy Unit 4 of The Economy. Just as scientists use experiments to investigate how physical processes work under certain conditions, social scientists use experiments to investigate how people might behave in particular situations. Although we cannot perfectly predict how people will actually behave, the controlled environment of experiments allows us to isolate the effects of a given change and identify specific reasons for the observed behaviour. If we keep all conditions the same and only changed one thing, then we can be more certain that any differences we observe are due to that one change. Economists use experiments to look at social interactions where one person’s decision affects the outcome for that individual and the outcomes for others. Some goods and services are called public goods because when one person bears the cost of providing the good, everyone can enjoy it. Irrigation projects and the production of new knowledge are examples of public goods. The problem with public goods provisioning is that completely self-interested people prefer to benefit from the good without paying anything for it—this is known as ‘free riding’. However, there are real-world examples of successful public goods provisioning, such as common irrigation projects in India and Nepal. What could explain such sustained contributions to a public good? One explanation is that people contribute because they care about the wellbeing of others, or because they respect norms that ‘free riding is bad’. Or they might contribute for the shame they would feel (or worse) if they were publicly punished. If others in your community know that you haven’t contributed and could punish you (for example, by gossiping about you, withholding help in the future, or ostracizing you), then you may contribute either out of self-interest or because you want to be able to think of yourself as a good person. To see whether punishment could result in sustained contributions to a public good, researchers Herrmann, Thöni, and Gächter (2008) did a study where different groups of people, in various countries, participated in two public goods experiments. The first experiment had ten rounds. In each round: Each person in the experiment (we call them subjects) is given $20. The subjects are randomly sorted into small groups, typically of four people who don’t know each other. They are asked to decide on a contribution from their $20 to a common pool of money. The pool of money is a public good. For every dollar contributed, each person in the group receives $0.40, including the contributor. After each round, the participants are told how much other members of their group contributed. For example, if three players each contribute $1, and one contributes nothing, each of the three will have $20.20 altogether, and the fourth will have $21.20. The second experiment was the same as the first, except with an additional punishment option. After observing the contributions of their group, individual players could pay to punish other players by making them pay a $3 fine. The punisher remained anonymous but had to pay $1 per player punished. You can read the Herrmann et al. (2008) study, and the economic concepts behind their experiment in Section 2.7 of Economy, Society, and Public Policy. In this project, we will first learn more about how experimental data can be collected by playing a public goods game to collect our own data. Then we will look at ways to describe and analyse the experimental data from the two experiments described above, in order to answer the following research questions: Were there any differences in behaviour (average contributions) between the experiments? Can we attribute the observed differences in behaviour to the change in conditions, rather than to chance or coincidence? Working in Excel Working in R Working in Google Sheets"
});
index.addDoc({
    id: 13,
    title: "Doing Economics: Empirical Project 2: Working in Excel",
    content: "Empirical Project 2 Working in Excel Part 2.1 Collecting data by playing a public goods game Learning objectives for this part collect data from an experiment and enter it into a spreadsheet use summary measures, for example, mean and standard deviation, and line charts to describe and compare data. Note You can still do Parts 2.2 and 2.3 without completing this part of the project. Before taking a closer look at the experimental data, you will play a public goods game like the one in the introduction with your classmates to learn how experimental data can be collected. If your instructor has not set up a game, follow the instructions below to set up your own game. Instructions How to set up the public goods game Form a group of at least four people. (You may also want to set a maximum of 8 or 10 players to make the game easier to play). Choose one person to be the game administrator. The administrator will monitor the game, while the other people play the game. Administrator Create the game: Go to the ‘Economics Games’ website, scroll down to the bottom of the page, and click ‘Create a Multiplayer Game and Get Logins’. Then click ‘Externalities and public goods’. Under the heading ‘Voluntary contribution to a public good’, click ‘Choose this Game’. Enter in the number of people playing the game, and select ‘1’ for the number of universes. Then click ‘Get Logins’. A pop-up will appear, showing the login IDs and passwords for the players and for the administrator. Start the game: Give each player a different login ID. The game should be played anonymously, so make sure that players do not know the login IDs of other players. You are now ready to start the first round of the game. There are ten rounds in total. Confirm that all the rounds are complete: On the top right corner of the webpage, click ‘Login’, enter your login ID and password, and then click the green ‘Login’ button. You will be taken to the game administration page, which will show the average contribution in each round, and the results of the round just played. Wait until all the players have finished playing ten rounds before refreshing this page. Collect the game results: Once the players have finished playing ten rounds, refresh this page. The table at the top of the page will now show the average contribution (in euros) for each of the ten rounds played. Select the whole table, then copy and paste it into a new worksheet in Excel. Players Login: Once the administrator has created the game, go to the ‘Economics Games’ website. On the top right corner, click ‘Login’, enter the login ID and password that your administrator has given you, then click the green ‘Login’ button. You will be taken to the public goods game that your administrator has set up. Play the first round of the game: Read the instructions at top of the page carefully before starting the game. In each round, you must decide how much to contribute to the public good. Enter your choice for each universe (group of players) that you are a part of (if the same players are in two universes, then make the same contribution in both), then click ‘Validate’. View the results of the first round: You will then be shown the results of the first round, including how much each player (including yourself) contributed, the payoffs, and the profits. Click ‘Next’ to start the next round. Complete all the rounds of the game: Repeat steps 2 and 3 until you have played ten rounds in total, then collect the results of the game from your administrator. The results from your game will look like Figure 2.1. In the questions below you will compare your results with those in Figure 3 of Herrmann et al. (2008), but first you need to reformat your table to look like Figure 2.2. Follow the steps in Excel walk-through 2.1 to reformat your table. Round 10 9 8 7 6 5 4 3 2 1 Average contribution A table formatted with ‘Round’ and ‘Average contribution’ as the row variables. Figure 2.1 A table formatted with ‘Round’ and ‘Average contribution’ as the row variables. Round Average contribution 1 2 3 4 5 6 7 8 9 10 A table formatted with ‘Round’ and ‘Average contribution’ as the column variables. Figure 2.2 A table formatted with ‘Round’ and ‘Average contribution’ as the column variables. Excel walk-through 2.1 Reformatting a table <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to reformat a table. '' /> Figure 2.3 How to reformat a table. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : The table generated from playing the public goods game will look like the one shown in Rows 1–2. We need to reformat it so that the columns show the different variables, rather than the rows. '' /> The data The table generated from playing the public goods game will look like the one shown in Rows 1–2. We need to reformat it so that the columns show the different variables, rather than the rows. Figure 2.3a The table generated from playing the public goods game will look like the one shown in Rows 1–2. We need to reformat it so that the columns show the different variables, rather than the rows. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Copy and paste table headings : First, the table headings need to be copied and pasted so that they occupy one row, rather than one column. '' /> Copy and paste table headings First, the table headings need to be copied and pasted so that they occupy one row, rather than one column. Figure 2.3b First, the table headings need to be copied and pasted so that they occupy one row, rather than one column. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Copy and paste the data values : After completing step 4, your table will look like this. '' /> Copy and paste the data values After completing step 4, your table will look like this. Figure 2.3c After completing step 4, your table will look like this. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Rearrange rows in the correct order : The rows in Column A are arranged in descending order (Round 10, Round 9, and so on), so we will use Excel’s ‘Sort and Filter’ option to reverse this order. '' /> Rearrange rows in the correct order The rows in Column A are arranged in descending order (Round 10, Round 9, and so on), so we will use Excel’s ‘Sort and Filter’ option to reverse this order. Figure 2.3d The rows in Column A are arranged in descending order (Round 10, Round 9, and so on), so we will use Excel’s ‘Sort and Filter’ option to reverse this order.</span> <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Rearrange rows in the correct order : The ‘Expand the selection’ option means that the values in Column B will move along with the values in Column A (for example, the average contribution of 9 will still show up next to Round 1). This option prevents the information from being mismatched during the sorting process. '' /> Rearrange rows in the correct order The ‘Expand the selection’ option means that the values in Column B will move along with the values in Column A (for example, the average contribution of 9 will still show up next to Round 1). This option prevents the information from being mismatched during the sorting process. Figure 2.3e The ‘Expand the selection’ option means that the values in Column B will move along with the values in Column A (for example, the average contribution of 9 will still show up next to Round 1). This option prevents the information from being mismatched during the sorting process. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-03-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The reformatted table : The table is now reformatted as required. '' /> The reformatted table The table is now reformatted as required. Figure 2.3f The table is now reformatted as required. Use the results of the game you have played to answer the following questions. Make a line chart with average contribution as the vertical axis variable, and period (from 1 to 10) on the horizontal axis. Describe how average contributions have changed over the course of the game. Excel walk-through 2.2 Drawing a line chart with multiple variables Follow the walk-through in the CORE video, or in Figure 2.4, to find out how to draw a line chart with multiple variables in Excel. How to draw a line chart with multiple variables <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to plot a line chart with multiple variables. '' /> Figure 2.4 How to plot a line chart with multiple variables. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the data looks like. Each column has data for a particular country, and each row has data for a given time period (1 to 10). We will draw Figure 3 from Herrmann et al. (2008) as an example; the steps to do Figure 2A are identical. '' /> The data This is what the data looks like. Each column has data for a particular country, and each row has data for a given time period (1 to 10). We will draw Figure 3 from Herrmann et al. (2008) as an example; the steps to do Figure 2A are identical. Figure 2.4a This is what the data looks like. Each column has data for a particular country, and each row has data for a given time period (1 to 10). We will draw Figure 3 from Herrmann et al. (2008) as an example; the steps to do Figure 2A are identical. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a line graph : After completing step 3, the graph will look like this. Notice that the horizontal axis variable and vertical axis variables are not the same as Figure 3 (due to Excel’s default setting). '' /> Draw a line graph After completing step 3, the graph will look like this. Notice that the horizontal axis variable and vertical axis variables are not the same as Figure 3 (due to Excel’s default setting). Figure 2.4b After completing step 3, the graph will look like this. Notice that the horizontal axis variable and vertical axis variables are not the same as Figure 3 (due to Excel’s default setting). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Switch the horizontal and vertical axis variables : We can switch the horizontal and vertical axis variables in the ‘Select Data’ options. '' /> Switch the horizontal and vertical axis variables We can switch the horizontal and vertical axis variables in the ‘Select Data’ options. Figure 2.4c We can switch the horizontal and vertical axis variables in the ‘Select Data’ options. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Switch the horizontal and vertical axis variables : After step 7, the lines on your chart will look like those in Figure 2A or Figure 3 from Herrmann et al. '' /> Switch the horizontal and vertical axis variables After step 7, the lines on your chart will look like those in Figure 2A or Figure 3 from Herrmann et al. Figure 2.4d After step 7, the lines on your chart will look like those in Figure 2A or Figure 3 from Herrmann et al. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Move the legend to the right : After step 9, the legend will now be on the right-hand side of your chart. You can also experiment with the other positions to see which looks better. '' /> Move the legend to the right After step 9, the legend will now be on the right-hand side of your chart. You can also experiment with the other positions to see which looks better. Figure 2.4e After step 9, the legend will now be on the right-hand side of your chart. You can also experiment with the other positions to see which looks better. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-04-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add axis titles and a chart title : After step 13, your chart will look like Figure 2A or Figure 3 from Herrmann et al. '' /> Add axis titles and a chart title After step 13, your chart will look like Figure 2A or Figure 3 from Herrmann et al. Figure 2.4f Add axis titles and a chart title Compare your line chart with Figure 3 of Herrmann et al. (2008).1 Comment on any similarities or differences between the results (for example, the amount contributed at the start and end, or the change in average contributions over the course of the game). Can you think of any reasons why your results are similar to (or different from) those in Figure 3? You may find it helpful to read the ‘Experiments’ section of the Herrmann et al. (2008) study for a more detailed description of how the experiments were conducted. Part 2.2 Describing the data Learning objectives for this part use summary measures, for example, mean and standard deviation, and column charts to describe and compare data. Note You can still do Parts 2.2 and 2.3 without completing Part 2.1. We will now use the data for Figures 2A and 3 of Herrmann et al. (2008), and evaluate the effect of the punishment option on average contributions. Rather than compare two charts showing all of the data from each experiment, as the authors of the study did, we will use summary measures to compare the data, and show the data from both experiments (with and without punishment) on the same chart. First, download and save the data. The spreadsheet contains two tables: The first table shows average contributions in a public goods game without punishment (Figure 3). The second shows average contributions in a public goods game with punishment (Figure 2A). meanA summary statistic for a set of observations, calculated by adding all values in the set and dividing by the number of observations.varianceA measure of dispersion in a frequency distribution, equal to the mean of the squares of the deviations from the arithmetic mean of the distribution. The variance is used to indicate how ‘spread out’ the data is. A higher variance means that the data is more spread out. Example: The set of numbers 1, 1, 1 has zero variance (no variation), while the set of numbers 1, 1, 999 has a high variance of 221,334 (large spread). You can see that in each period (row), the average contribution varies across countries, in other words, there is a distribution of average contributions in each period. The mean and variance are two ways to summarize distributions. We will now use these measures, along with other measures (range and standard deviation) to summarize and compare the distribution of contributions in both experiments. Before answering these questions, make sure you understand mean and variance, and how to calculate these measures in Excel. See Figure 1.5 in Exercise 1.3 of Economy, Society, and Public Policy for more information about the mean. See Excel walk-through 1.6 for more information about the variance. Using the data for Figures 2A and 3 of Herrmann et al. (2008): Calculate the mean contribution in each period (row) separately for both experiments, using Excel’s AVERAGE function. Plot a line chart of mean contribution on the vertical axis and time period (from 1 to 10) on the horizontal axis (with a separate line for each experiment). Make sure the lines in the legend are clearly labelled according to the experiment (with punishment or without punishment). Describe any differences and similarities you see in the mean contribution over time in both experiments. Instead of looking at all periods, we can focus on contributions in the first and last period. Plot a column chart showing the mean contribution in the first and last period for both experiments. Your chart should look like Figure 2.6. Excel walk-through 2.3 Drawing a column chart to compare two groups Follow the walk-through in the CORE video, or in Figure 2.5, to find out how to draw a column or bar chart in Excel. How to draw a column or bar chart <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to draw a column chart to compare two groups. '' /> Figure 2.5 How to draw a column chart to compare two groups. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the data looks like. Column R has the means for Figure 3 (without punishment). Column S has the means for Figure 2A (with punishment). We will use the cells in bold font (Cells R3, R12, S3, and S12) to make the chart. '' /> The data This is what the data looks like. Column R has the means for Figure 3 (without punishment). Column S has the means for Figure 2A (with punishment). We will use the cells in bold font (Cells R3, R12, S3, and S12) to make the chart. Figure 2.5a This is what the data looks like. Column R has the means for Figure 3 (without punishment). Column S has the means for Figure 2A (with punishment). We will use the cells in bold font (Cells R3, R12, S3, and S12) to make the chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw the column chart : After completing step 3, the column chart will look like this. '' /> Draw the column chart After completing step 3, the column chart will look like this. Figure 2.5b After completing step 3, the column chart will look like this. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change horizontal and vertical axis variables, and change legend labels : After completing step 6, the chart will now look like this, with the data for Period 1 grouped together, and the data for Period 10 grouped together. '' /> Change horizontal and vertical axis variables, and change legend labels After completing step 6, the chart will now look like this, with the data for Period 1 grouped together, and the data for Period 10 grouped together. Figure 2.5c After completing step 6, the chart will now look like this, with the data for Period 1 grouped together, and the data for Period 10 grouped together. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change horizontal and vertical axis variables, and change legend labels : After step 7, ‘Series 1’ will be renamed as ‘Without punishment’. '' /> Change horizontal and vertical axis variables, and change legend labels After step 7, ‘Series 1’ will be renamed as ‘Without punishment’. Figure 2.5d After step 7, ‘Series 1’ will be renamed as ‘Without punishment’. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis labels : Once you change the series title, the changes will show up in the legend. '' /> Change the horizontal axis labels Once you change the series title, the changes will show up in the legend. Figure 2.5e Once you change the series title, the changes will show up in the legend. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis labels : Instead of ‘1’ and ‘2’ on the horizontal axis, the labels will change to ‘1’ and ‘10’ (referring to the period number in the experiment) once you exit the box. '' /> Change the horizontal axis labels Instead of ‘1’ and ‘2’ on the horizontal axis, the labels will change to ‘1’ and ‘10’ (referring to the period number in the experiment) once you exit the box. Figure 2.5f Instead of ‘1’ and ‘2’ on the horizontal axis, the labels will change to ‘1’ and ‘10’ (referring to the period number in the experiment) once you exit the box. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add data labels on top of the columns : After completing step 14, numbers showing the height of the column will appear on top of the columns selected. '' /> Add data labels on top of the columns After completing step 14, numbers showing the height of the column will appear on top of the columns selected. Figure 2.5g After completing step 14, numbers showing the height of the column will appear on top of the columns selected. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-h.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-h-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-h-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-h-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-05-h.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add axis titles and a chart title : After step 18, your chart will look like Figure 2.6. '' /> Add axis titles and a chart title After step 18, your chart will look like Figure 2.6. Figure 2.5h After step 18, your chart will look like Figure 2.6. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Average contributions in Periods 1 and 10, with and without punishment. '' /> Average contributions in Periods 1 and 10, with and without punishment. Figure 2.6 Average contributions in Periods 1 and 10, with and without punishment. varianceA measure of dispersion in a frequency distribution, equal to the mean of the squares of the deviations from the arithmetic mean of the distribution. The variance is used to indicate how ‘spread out’ the data is. A higher variance means that the data is more spread out. Example: The set of numbers 1, 1, 1 has zero variance (no variation), while the set of numbers 1, 1, 999 has a high variance of 221,334 (large spread). The mean is one useful measure of the ‘middle’ of a distribution, but is not a complete description of what our data looks like. We also need to know how ‘spread out’ the data is in order to get a clearer picture and make comparisons between the distributions. The variance is one way to measure spread—the higher the variance, the more spread out the data is. standard deviationA measure of dispersion in a frequency distribution, equal to the square root of the variance. The standard deviation has a similar interpretation to the variance. A larger standard deviation means that the data is more spread out. Example: The set of numbers 1, 1, 1 has a standard deviation of zero (no variation or spread), while the set of numbers 1, 1, 999 has a standard deviation of 46.7 (large spread). A similar measure is standard deviation, which is the square root of the variance. Standard deviation is commonly used because it provides a handy rule of thumb for large datasets—most of the data (95% if there are many observations) will be less than two standard deviations away from the mean. Using the data for Figures 2A and 3 of Herrmann et al. (2008): Calculate the standard deviation for Periods 1 and 10 separately, for both experiments. Does the rule of thumb apply? (In other words, are most values within two standard deviations of the mean?) As shown in Figure 2.6, the mean contribution for both experiments was 10.6 in Period 1. With reference to your standard deviation calculations, explain whether this means that the two sets of data are the same. Excel walk-through 2.4 Calculating the standard deviation Follow the walk-through in the CORE video, or in Figure 2.7, to find out how to calculate standard deviation in Excel. How to calculate standard deviation <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to calculate and understand the standard deviation. '' /> Figure 2.7 How to calculate and understand the standard deviation. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : We will compare the example data from Excel walk-through 2.1 with some new data that is less spread out. You can see from Column H that all the values are between 10 to 12 (inclusive). '' /> The data We will compare the example data from Excel walk-through 2.1 with some new data that is less spread out. You can see from Column H that all the values are between 10 to 12 (inclusive). Figure 2.7a We will compare the example data from Excel walk-through 2.1 with some new data that is less spread out. You can see from Column H that all the values are between 10 to 12 (inclusive). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Standard deviation calculation and interpretation : Excel’s STDEV.P function will calculate the standard deviation over the selected cells. To enter in the formula, click on an empty cell. '' /> Standard deviation calculation and interpretation Excel’s STDEV.P function will calculate the standard deviation over the selected cells. To enter in the formula, click on an empty cell. Figure 2.7b Excel’s STDEV.P function will calculate the standard deviation over the selected cells. To enter in the formula, click on an empty cell. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-07-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The relationship between the standard deviation and the variance : Both the variance and standard deviation measure spread. We need the variance to calculate the standard deviation, but we usually use the standard deviation to describe distributions because of the handy rule of thumb. '' /> The relationship between the standard deviation and the variance Both the variance and standard deviation measure spread. We need the variance to calculate the standard deviation, but we usually use the standard deviation to describe distributions because of the handy rule of thumb. Figure 2.7c Both the variance and standard deviation measure spread. We need the variance to calculate the standard deviation, but we usually use the standard deviation to describe distributions because of the handy rule of thumb. rangeThe interval formed by the smallest (minimum) and the largest (maximum) value of a particular variable. The range shows the two most extreme values in the distribution, and can be used to check whether there are any outliers in the data. (Outliers are a few observations in the data that are very different from the rest of the observations.) Another measure of spread is the range, the interval formed by the smallest (minimum) and the largest (maximum) values of a particular variable. For example, we might say that the number of periods in the public goods experiment ranges from 1 to 10. Once we know the most extreme values in our dataset, we have a better picture of what our data looks like. Calculate the maximum and minimum value for Periods 1 and 10 separately, for both experiments. Excel walk-through 2.5 Finding the minimum, maximum, and range of a variable <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to find the minimum, maximum, and range of a variable. '' /> Figure 2.8 How to find the minimum, maximum, and range of a variable. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : Here we are going to calculate the minimum and maximum value of the example data in Excel walk-through 2.1: Reformatting a table. '' /> The data Here we are going to calculate the minimum and maximum value of the example data in Excel walk-through 2.1: Reformatting a table. Figure 2.8a Here we are going to calculate the minimum and maximum value of the example data in Excel walk-through 2.1: Reformatting a table. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the minimum value : The MIN function calculates the minimum value of the selected cells. Here, the minimum value is 4. '' /> Calculate the minimum value The MIN function calculates the minimum value of the selected cells. Here, the minimum value is 4. Figure 2.8b The MIN function calculates the minimum value of the selected cells. Here, the minimum value is 4. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-08-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the maximum value and the range : The MAX function calculates the maximum value. The maximum value here is 18. The range is the interval formed by the minimum and the maximum value. In words, we say ‘the range is 4 to 18’ or ‘the average contribution ranges from 4 to 18’. In numbers, we say ‘the range is [4, 18]’. '' /> Calculate the maximum value and the range The MAX function calculates the maximum value. The maximum value here is 18. The range is the interval formed by the minimum and the maximum value. In words, we say ‘the range is 4 to 18’ or ‘the average contribution ranges from 4 to 18’. In numbers, we say ‘the range is [4, 18]’. Figure 2.8c The MAX function calculates the maximum value. The maximum value here is 18. The range is the interval formed by the minimum and the maximum value. In words, we say ‘the range is 4 to 18’ or ‘the average contribution ranges from 4 to 18’. In numbers, we say ‘the range is [4, 18]’. A concise way to describe the data is in a summary table. With just four numbers (mean, standard deviation, minimum value, maximum value), we can get a general idea of what the data looks like. In Excel, create a summary table as shown in Figure 2.9 below. Make three more summary tables, for Period 10 (without punishment), Period 1 (with punishment), and Period 10 (with punishment). Use your answers to Questions 2 to 4 to complete the summary tables. Comment on any similarities and differences in the distributions, both across time and across experiments. Mean Standard deviation Minimum Maximum Contribution (Period 1, without punishment) A summary table for contributions in a given period. Figure 2.9 A summary table for contributions in a given period. Part 2.3 Did changing the rules of the game affect behaviour? Learning objectives for this part calculate and interpret the p-value evaluate the usefulness of experiments for determining causality, and the limitations of these experiments. The punishment option was introduced into the public goods game in order to see whether it could help sustain contributions, compared to the game without a punishment option. We will now use a calculation called a p-value to compare the results from both experiments more formally. By comparing the results in Period 10 of both experiments, we can see that the mean contribution in the experiment with punishment is 8.5 units higher than in the experiment without punishment (see Figure 2.6). Is it more likely that this behaviour is due to chance, or is it more likely to be due to the difference in experimental conditions? You can conduct another experiment to understand why we might see differences in behaviour that are due to chance. First, flip a coin six times, using one hand only, and record the results (for example, Heads, Heads, Tails, etc.). Then, using the same hand, flip a coin six times and record the results again. Compare the outcomes from Question 1(a). Did you get the same number of heads in both cases? Even if you did, was the sequence of the outcomes (for example, Heads, Tails, Tails …) the same in both cases? The important point to note is that even when we conduct experiments under the same controlled conditions, due to an element of randomness, we may not observe the exact same behaviour each time we do the experiment. Randomness arises because the statistical analysis is conducted on a sample of data, and the sample we observe is only one of many possible samples. Whatever differences we calculate between two samples would almost certainly change if we had observed another pair of samples. Importantly, economists aren’t really interested in whether two samples are actually different, but rather whether the underlying populations, from which the samples were drawn, are different. And this is the challenge faced by the empirical economist. The p-value gives us a measure of how likely it is that we could observe the differences in our sample groups, if there were no difference between the populations. The smaller the p-value, the less likely that we would observe such differences. And the smaller this p-value, the smaller will be our confidence in the hypothesis that there are no differences in the populations. When we are interested in whether a treatment works — in this case, whether having the punishment option makes a difference — we want a way to check whether any observed differences could just be due to sample variation. The size of the difference alone cannot tell us whether it might just be due to chance. Even if the observed difference seems large, it could be small relative to how much the data vary. Figures 2.10 and 2.11 show the mean exam score of two groups of high school students and the size of house in which they live (represented by the height of the columns, and reported in the boxes above the columns), with the dots representing the underlying data. Figure 2.10 shows a relatively large difference in means that could have arisen by chance because the data is widely spread out (the standard deviation is large), while Figure 2.11 shows a relatively small difference that looks unlikely to be due to chance because the data is tightly clustered together (the standard deviation is very small). Note that we are looking at two distinct questions here: first, is there a large or small difference in exam score associated with the size of house of the student and second, is that difference likely to have arisen by chance. A social scientist is interested in the answer to both questions. If the difference is large but could easily have occurred by chance or if the difference is very small and unlikely to have occurred by chance, then the results are not suggestive of an important relationship between size of house and exam grade. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''An example of a large difference in means that is likely to have happened by chance. '' /> An example of a large difference in means that is likely to have happened by chance. Figure 2.10 An example of a large difference in means that is likely to have happened by chance. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''An example of a small difference in means that is unlikely to have happened by chance. '' /> An example of a small difference in means that is unlikely to have happened by chance. Figure 2.11 An example of a small difference in means that is unlikely to have happened by chance. p-valueThe probability of observing the data collected, assuming that any differences observed between the two groups of interest have happened by chance. The p-value ranges from 0 to 1, where lower values indicate a higher probability that the underlying assumption (differences observed have happened by chance) is false. The lower the probability (the lower the p-value), the less likely it is to observe the given data, and therefore the more likely it is that the assumption is false (the observed differences are unlikely to have happened by chance). To help us decide whether the observed differences have arisen by chance we can calculate a p-value. Without going into any detail, to calculate the p-value we require the sample means and the sample standard deviations. Combined with the assumption that there are no differences in the population we can then calculate the p-value which is a measure (but not the probability) of how likely it is that the observed differences are due to chance. It is important to notice that the p-value does not give us a definite answer. When we look at the data in Figure 2.10, we cannot be absolutely certain that there really is a link between house size and exam scores. But if the p-value for the difference is very small (for example, 0.02) then we know that there would only be a 2% probability of seeing the differences we did observe in the sample if in truth (i.e. in the population) there was no relationship between house size and exam scores. hypothesis testA test in which a null (default) and an alternative hypothesis are posed about some characteristic of the population. Sample data is then used to test how likely it is that these sample data would be seen if the null hypothesis was true. Find out more Hypothesis testing and p-values The process of formulating a hypothesis about the data, calculating the p-value, and using it to assess whether what we observe is consistent with the hypothesis, is known as a hypothesis test. When we conduct a hypothesis test, we consider two hypotheses: either there is no difference between the populations, in which case the differences we observe must have happened by chance (known as the ’null hypothesis’); or the populations really are different (known as the ‘alternative hypothesis’). The smaller the p-value, the lower is the probability that the differences we observe could have happened simply by chance, i.e. if the null hypothesis was true. The smaller the p-value, the stronger is the evidence in favour of the alternative hypothesis. It is a common, but highly debatable practice, to pick a cutoff level for the p-value, and reject the null hypothesis if the p-value is below this cutoff. This approach has been criticized recently by statisticians and social scientists because the cutoff level is quite arbitrary. Instead of using a cutoff, we prefer to calculate p-values and use them to assess the strength of the evidence. Whether the statistical evidence is strong enough for us to draw a firm conclusion about the data will always be a matter of judgement. In particular, you want to make sure that you understand the consequences of concluding that the null hypothesis is not true, and hence that the alternative is true. You may be quite easily prepared to conclude that house sizes and exam scores are related, but much more cautious about deciding that a new medication is more effective than an existing one if you know that this new medication has severe side effects. In the case of the medication, you might want to see stronger evidence against the null hypothesis before deciding that doctors should be advised to prescribe the new medication. To calculate the p-value, we use a function in Excel called T.TEST. Using the data for Figures 2A and 3: Use Excel’s T.TEST function to calculate the p-value for the difference in means in Period 1 (with and without punishment). What is the p-value? What does this p-value tell us about the difference in means in Period 1? Excel walk-through 2.6 Using Excel’s T.TEST function <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to use Excel’s T.TEST function. '' /> Figure 2.12 How to use Excel’s T.TEST function. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : Here we are going to calculate the p-values we need. '' /> The data Here we are going to calculate the p-values we need. Figure 2.12a Here we are going to calculate the p-values we need. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the p-value for Period 1 data : Excel’s T.TEST function will calculate the p-value for the two groups of cells selected. In the example shown, the formula to type is =T.TEST(B3:Q3,B17:Q17,2,1). '' /> Calculate the p-value for Period 1 data Excel’s T.TEST function will calculate the p-value for the two groups of cells selected. In the example shown, the formula to type is =T.TEST(B3:Q3,B17:Q17,2,1). Figure 2.12b Excel’s T.TEST function will calculate the p-value for the two groups of cells selected. In the example shown, the formula to type is =T.TEST(B3:Q3,B17:Q17,2,1). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-12-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the p-value for Period 10 data : Follow these steps to calculate the p-value for Period 10 (the cells in the dotted boxes). The only difference from the previous formula (for Period 1) is the data selected. '' /> Calculate the p-value for Period 10 data Follow these steps to calculate the p-value for Period 10 (the cells in the dotted boxes). The only difference from the previous formula (for Period 1) is the data selected. Figure 2.12c Follow these steps to calculate the p-value for Period 10 (the cells in the dotted boxes). The only difference from the previous formula (for Period 1) is the data selected. Using the data for Period 10: Use Excel’s T.TEST function to calculate the p-value for the difference in means in Period 10 (with and without punishment). What is the p-value? What does this p-value tell us about the relationship between punishment, and behaviour in the public goods game? With reference to Figures 2.10 and 2.11, explain why we cannot use the size of the difference to directly conclude whether the difference could be due to chance. spurious correlationA strong linear association between two variables that does not result from any direct relationship, but instead may be due to coincidence or to another unseen factor. An important point to note is that calculating p-values may not tell us anything about causation. The example of house size and exam scores shown in Figure 2.11, gives us evidence that some kind of relationship between house size and exam scores is very likely. However, we would not conclude that building an extra room automatically makes someone smarter. P-values cannot help us detect these spurious correlations. However, calculating p-values for experimental evidence can help us determine whether there is a causal link between two variables. If we conduct an experiment and find a difference in outcomes with a low p-value, then we may conclude that the change in experimental conditions is likely to have caused the difference. Refer to the results from the public goods games. Which characteristics of the experimental setting make it likely that the punishment option was the cause of the change in behaviour? With reference to Figure 2.6, explain why we need to compare the two groups in Period 1 in order to conclude that there is a causal link between the punishment option and behaviour in the game. Experiments can be useful for identifying causal links. However, if people’s behaviour in experimental conditions were different from their behaviour in the real world, our results would not be applicable anywhere outside the experiment. Discuss some limitations of lab experiments, and suggest some ways to address (or partially address) them. (You may find pages 158–171 of the paper ‘What do laboratory experiments measuring social preferences reveal about the real world?’ helpful, as well as the discussion on free riding and altruism in Section 2.6 of Economy, Society, and Public Policy.) Benedikt Herrmann, Christian Thöni, and Simon Gächter. 2008. Figure 3 in ‘Antisocial punishment across societies’. Science Magazine 319 (5868): p. 1365. ↩"
});
index.addDoc({
    id: 14,
    title: "Doing Economics: Empirical Project 2: Working in R",
    content: "Empirical Project 2 Working in R Download the code To download the code chunks used in this project, right-click on the download link and select ‘Save Link As…’. You’ll need to save the code download to your working directory, and open it in RStudio. Don’t forget to also download the data into your working directory by following the steps in this project. Getting started in R For this project you will need the following packages: tidyverse, to help with data manipulation readxl, to import an Excel spreadsheet. If you need to install either of these packages, run the following code: install.packages(c(''readxl'', ''tidyverse'')) You can import the libraries now, or when they are used in the R walk-throughs below. library(readxl) library(tidyverse) Part 2.1 Collecting data by playing a public goods game Learning objectives for this part collect data from an experiment and enter it into a spreadsheet use summary measures, for example, mean and standard deviation, and line charts to describe and compare data. Note You can still do Parts 2.2 and 2.3 without completing this part of the project. Before taking a closer look at the experimental data, you will play a public goods game like the one in the introduction with your classmates to learn how experimental data can be collected. If your instructor has not set up a game, follow the instructions below to set up your own game. Instructions How to set up the public goods game Form a group of at least four people. (You may also want to set a maximum of 8 or 10 players to make the game easier to play). Choose one person to be the game administrator. The administrator will monitor the game, while the other people play the game. Administrator Create the game: Go to the ‘Economics Games’ website, scroll down to the bottom of the page, and click ‘Create a Multiplayer Game and Get Logins’. Then click ‘Externalities and public goods’. Under the heading ‘Voluntary contribution to a public good’, click ‘Choose this Game’. Enter in the number of people playing the game, and select ‘1’ for the number of universes. Then click ‘Get Logins’. A pop-up will appear, showing the login IDs and passwords for the players and for the administrator. Start the game: Give each player a different login ID. The game should be played anonymously, so make sure that players do not know the login IDs of other players. You are now ready to start the first round of the game. There are ten rounds in total. Confirm that all the rounds are complete: On the top right corner of the webpage, click ‘Login’, enter your login ID and password, and then click the green ‘Login’ button. You will be taken to the game administration page, which will show the average contribution in each round, and the results of the round just played. Wait until all the players have finished playing ten rounds before refreshing this page. Collect the game results: Once the players have finished playing ten rounds, refresh this page. The table at the top of the page will now show the average contribution (in euros) for each of the ten rounds played. Select the whole table, then copy and paste it into a new worksheet in Excel. Players Login: Once the administrator has created the game, go to the ‘Economics Games’ website. On the top right corner, click ‘Login’, enter the login ID and password that your administrator has given you, then click the green ‘Login’ button. You will be taken to the public goods game that your administrator has set up. Play the first round of the game: Read the instructions at top of the page carefully before starting the game. In each round, you must decide how much to contribute to the public good. Enter your choice for each universe (group of players) that you are a part of (if the same players are in two universes, then make the same contribution in both), then click ‘Validate’. View the results of the first round: You will then be shown the results of the first round, including how much each player (including yourself) contributed, the payoffs, and the profits. Click ‘Next’ to start the next round. Complete all the rounds of the game: Repeat steps 2 and 3 until you have played ten rounds in total, then collect the results of the game from your administrator. Use the results of the game you have played to answer the following questions. Make a line chart with average contribution as the vertical axis variable, and period (from 1 to 10) on the horizontal axis. Describe how average contributions have changed over the course of the game. R walk-through 2.1 Plotting a line chart with multiple variables Use the data from your own experiment to answer Question 1. As an example, we will use the data for the first three cities of the dataset that will be introduced in Part 2.2. Here you can see commands to R which are spread across two lines. You can spread a command across multiple lines, but you must adhere to the following two rules for this to work. First, the line break should come inside a set of parenthesis (i.e. between ( and ) or straight after the assignment operator (<-). Second, the line break must not be inside a string (whatever is inside quotes) or in the middle of a word or number. Period <- seq(1, 10) Copenhagen <- c(14.1, 14.1, 13.7, 12.9, 12.3, 11.7, 10.8, 10.6, 9.8, 5.3) Dniprop <- c(11.0, 12.6, 12.1, 11.2, 11.3, 10.5, 9.5, 10.3, 9.0, 8.7) Minsk <- c(12.8, 12.3, 12.6, 12.3, 11.8, 9.9, 9.9, 8.4, 8.3, 6.9) # Put the data into a data frame data_ex <- data.frame(Period, Copenhagen, Dniprop, Minsk) plot(data_ex$Period, data_ex$Copenhagen, ylim = c(4, 16), ylab = ''Average contribution'', type = ''l'', col = ''blue'', lwd = 2) # Select colour and line width lines(data_ex$Dniprop, col = ''red'', lwd = 2) lines(data_ex$Minsk, col = ''green'', lwd = 2) title(''Average contribution to public goods game: without punishment'') legend(''bottomleft'', lwd = 2, lty = 1, cex = 1.2, legend = c(''Copenhagen'', ''Dniprop'', ''Minsk''), col = c(''blue'', ''red'', ''green'')) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-01.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-01-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-01-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-01-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-01.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Average contributions in different locations. '' /> Average contributions in different locations. Figure 2.1 Average contributions in different locations. Compare your line chart with Figure 3 of Herrmann et al. (2008).1 Comment on any similarities or differences between the results (for example, the amount contributed at the start and end, or the change in average contributions over the course of the game). Can you think of any reasons why your results are similar to (or different from) those in Figure 3? You may find it helpful to read the ‘Experiments’ section of the Herrmann et al. (2008) study for a more detailed description of how the experiments were conducted. Part 2.2 Describing the data Learning objectives for this part use summary measures, for example, mean and standard deviation, and column charts to describe and compare data. Note You can still do Parts 2.2 and 2.3 without completing this part of the project. We will now use the data used in Figures 2A and 3 of Herrmann et al. (2008), and evaluate the effect of the punishment option on average contributions. Rather than compare two charts showing all of the data from each experiment, as the authors of the study did, we will use summary measures to compare the data, and show the data from both experiments (with and without punishment) on the same chart. First, download and save the data. The spreadsheet contains two tables: The first table shows average contributions in a public goods game without punishment (Figure 3). The second table shows average contributions in a public goods game with punishment (Figure 2A). You can see that in each period (row), the average contribution varies across countries, in other words, there is a distribution of average contributions in each period. R walk-through 2.2 Importing the datafile into R Both the tables you need are in a single Excel worksheet. Note down the cell ranges of each table, in this case A2:Q12 for the without punishment data and A16:Q26 for the punishment data. We will now use this range information to import the data into two dataframes (data_N and data_P respectively). # This package provides useful functionality later. library(tidyverse) library(readxl) # Set your working directory to the correct folder. # Insert your file path for 'YOURFILEPATH'. setwd(''YOURFILEPATH'') data_N <- read_excel(''Public-goods-experimental-data.xlsx'', range = ''A2:Q12'') data_P <- read_excel(''Public-goods-experimental-data.xlsx'', range = ''A16:Q26'') Look at the data either by opening the dataframes from the Environment window or by typing data_N or data_P into the Console. meanA summary statistic for a set of observations, calculated by adding all values in the set and dividing by the number of observations.varianceA measure of dispersion in a frequency distribution, equal to the mean of the squares of the deviations from the arithmetic mean of the distribution. The variance is used to indicate how ‘spread out’ the data is. A higher variance means that the data is more spread out. Example: The set of numbers 1, 1, 1 has zero variance (no variation), while the set of numbers 1, 1, 999 has a high variance of 221,334 (large spread). You can see that in each period (row), the average contribution varies across countries; in other words, there is a distribution of average contributions in each period. The mean and variance are two ways to summarize distributions. We will now use these measures, along with other measures (range and standard deviation) to summarize and compare the distribution of contributions in both experiments. Using the data for Figures 2A and 3 of Herrmann et al. (2008): Calculate the mean contribution in each period (row) separately for both experiments. Plot a line chart of mean contribution on the vertical axis and time period (from 1 to 10) on the horizontal axis (with a separate line for each experiment). Make sure the lines in the legend are clearly labelled according to the experiment (with punishment or without punishment). Describe any differences and similarities you see in the mean contribution over time in both experiments. R walk-through 2.3 Calculating the mean using a loop or the apply function Calculate mean contribution We calculate the mean using two different methods, to illustrate that there are usually many ways of achieving the same thing. We apply the first method on data_N, which uses a loop to calculate the average separately over each column except the first (data_P[row,2:17] or data_P[row,-1]). We use the second method (the apply function) on data_P. # Use a loop for data_N data_N$meanC <- 0 for (row in 1:nrow(data_N)) { data_N$meanC[row] <- rowMeans(data_N[row, 2:17]) } # Use the apply function for data_P data_P$meanC <- apply(data_P[, 2:17], 1, mean) As the name suggests, the apply function applies another function (the mean function in this case) to all rows or columns in a dataframe. The second input, 1, applies the specified function to all rows in data_P[,2:17]. Typing 2 would have calculated column means instead (check and see for yourself). Type ?apply in your console for more details, or see R walk-through 2.5 for further practice. Plot the mean contribution Now we will produce a line chart showing the mean contributions. plot(data_N$Period, data_N$meanC, type = ''l'', col = ''blue'', lwd = 2, xlab = ''Round'', ylim = c(4, 14), ylab = ''Average contribution'') lines(data_P$meanC, col = ''red'', lwd = 2) title(''Average contribution to public goods game'') legend(''bottomleft'', lty = 1, cex = 1.2, lwd = 2, legend = c(''Without punishment'', ''With punishment''), col = c(''blue'', ''red'')) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-02.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-02-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-02-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-02-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-02.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Average contribution to public goods game, with and without punishment. '' /> Average contribution to public goods game, with and without punishment. Figure 2.2 Average contribution to public goods game, with and without punishment. The difference between experiments is stark, as the contributions increase and then stabilize at around $13 when there is punishment, but decrease consistently from around $11 to $4 across the rounds when there is no punishment. Instead of looking at all periods, we can focus on contributions in the first and last period. Plot a column chart showing the mean contribution in the first and last period for both experiments. Your chart should look like Figure 2.3. R walk-through 2.4 Drawing a column chart to compare two groups To make a column chart, we will use the barplot function. We first extract the four data points we need (Periods 1 and 10, with and without punishment) and place them into a matrix, which we then input into the barplot function. temp_d <- c(data_N$meanC[1], data_N$meanC[10], data_P$meanC[1], data_P$meanC[10]) temp <- matrix(temp_d, nrow = 2, ncol = 2, byrow = TRUE) temp ## [,1] [,2] ## [1,] 10.57831 4.383769 ## [2,] 10.63876 12.869879 barplot(temp, main = ''Mean contributions in a public goods game'', ylab = ''Contribution'', beside = TRUE, col = c(''Blue'', ''Red''), names.arg = c(''Round 1'', ''Round 10'')) legend(''bottomleft'', pch = 1, col = c(''Blue'', ''Red''), c(''Without punishment'', ''With punishment'')) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Mean contributions in a public goods game. '' /> Mean contributions in a public goods game. Figure 2.3 Mean contributions in a public goods game. Tip Experimenting with these charts will help you to learn how to use R. The details of how to specify the column chart may look complicated, but you can see from Figure 2.3 what the options main, ylab, col and names.arg do. To figure out what the beside option does, try switching the option from TRUE to FALSE and see what happens. varianceA measure of dispersion in a frequency distribution, equal to the mean of the squares of the deviations from the arithmetic mean of the distribution. The variance is used to indicate how ‘spread out’ the data is. A higher variance means that the data is more spread out. Example: The set of numbers 1, 1, 1 has zero variance (no variation), while the set of numbers 1, 1, 999 has a high variance of 221,334 (large spread). The mean is one useful measure of the ‘middle’ of a distribution, but is not a complete description of what our data looks like. We also need to know how ‘spread out’ the data is in order to get a clearer picture and make comparisons between distributions. The variance is one way to measure spread: the higher the variance, the more spread out the data is. standard deviationA measure of dispersion in a frequency distribution, equal to the square root of the variance. The standard deviation has a similar interpretation to the variance. A larger standard deviation means that the data is more spread out. Example: The set of numbers 1, 1, 1 has a standard deviation of zero (no variation or spread), while the set of numbers 1, 1, 999 has a standard deviation of 46.7 (large spread). A similar measure is standard deviation, which is the square root of the variance and is commonly used because there is a handy rule of thumb for large datasets, which is that most of the data (95% if there are many observations) will be less than two standard deviations away from the mean. Using the data for Figures 2A and 3 of Herrmann et al. (2008): Calculate the standard deviation for Periods 1 and 10 separately, for both experiments. Does the rule of thumb apply? (In other words, are most values within two standard deviations of the mean?) As shown in Figure 2.3, the mean contribution for both experiments was 10.6 in Period 1. With reference to your standard deviation calculations, explain whether this means that the two sets of data are the same. R walk-through 2.5 Calculating and understanding the standard deviation In order to calculate these standard deviations and variances, we will use the apply function, which we introduced in R walk-through 2.3. As we saw, apply is a command asking R to apply the same function to all the rows or columns of a dataframe, and the basic structure is as follows: apply(dataframe, dimension (rows or columns), the function to apply). So to calculate the variances, we use the following command: data_N$varC <- apply(data_N[, 2:17], 1, var) Here we take data_N[, 2:17] and apply the var function to each row (recall that the second input 1 does this; 2 would indicate columns). Note that as in R walk-through 2.3, we exclude the first column from the calculation, as that contains the period numbers. The result is saved as a new variable called varC. We then apply the same principle to the standard deviation calculation and the data_P dataframe. data_N$sdC <- apply(data_N[, 2:17], 1, sd) data_P$varC <- apply(data_P[, 2:17], 1, var) data_P$sdC <- apply(data_P[, 2:17], 1, sd) To determine whether 95% of the observations fall within two standard deviations of the mean, we can use a line chart. As we have 16 countries in every period, we would expect about one observation (0.05 × 16 = 0.8) to fall outside this interval. citylist <- names(data_N[2:17]) plot(data_N$Period, data_N$meanC, type = ''l'', col = ''blue'', lwd = 2, xlab = ''Round'', ylim = c(0, 20), ylab = ''Average contribution'') # mean + 2 sd lines(data_N$meanC + 2 * data_N$sdC, col = ''red'', lwd = 2) # mean – 2 sd lines(data_N$meanC - 2 * data_N$sdC, col = ''red'', lwd = 2) for(i in citylist) { points(data_N[[1]], data_N[[i]]) } title(''Contribution to public goods game without punishment'') legend(''bottomleft'', legend = c(''Mean'', ''+/- 2 sd''), col = c(''blue'', ''red''), lwd = 2, lty = 1, cex = 1.2) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Contribution to public goods game without punishment. '' /> Contribution to public goods game without punishment. Figure 2.4 Contribution to public goods game without punishment. None of the observations fall outside the mean ± two standard deviations interval for the public goods game without punishment. Let’s see the equivalent chart for the version with punishment. citylist <- names(data_N[2:17]) plot(data_P$Period, data_P$meanC, type = ''l'', col = ''blue'', xlab = ''Round'', ylim = c(0, 22), ylab = ''Average contribution'') # mean + 2 sd lines(data_P$meanC + 2 * data_P$sdC, col = ''red'') # mean – 2 sd lines(data_P$meanC - 2 * data_P$sdC, col = ''red'') for(i in citylist) { points(data_P[[1]], data_P[[i]]) } title(''Contribution to public goods game with punishment'') legend(''bottomleft'', legend = c(''Mean'', ''+/- 2 sd''), col = c(''blue'', ''red''), lty = 1, cex = 1.2) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Contribution to public goods game with punishment. '' /> Contribution to public goods game with punishment. Figure 2.5 Contribution to public goods game with punishment. Here it looks as if we only have one observation outside the interval (in Period 8). In that aspect the two experiments look similar. However, from comparing these two charts, we see that the game with punishment displays a greater variation of responses than the game without punishment. In other words, there is a larger standard deviation and variance for the observations coming from the game with punishment. rangeThe interval formed by the smallest (minimum) and the largest (maximum) value of a particular variable. The range shows the two most extreme values in the distribution, and can be used to check whether there are any outliers in the data. (Outliers are a few observations in the data that are very different from the rest of the observations.) Another measure of spread is the range, which is the interval formed by the smallest (minimum) and the largest (maximum) values of a particular variable. For example, we might say that the number of periods in the public goods experiment ranges from 1 to 10. Once we know the most extreme values in our dataset, we have a better picture of what our data looks like. Calculate the maximum and minimum value for Periods 1 and 10 separately, for both experiments. R walk-through 2.6 Finding the minimum, maximum, and range of a variable To calculate the range for both experiments and for all periods, we will use the apply function again. You might think it makes sense to use the following command: data_P$rangeC <- apply(data_P[, 2:17], 1, range) Unfortunately, when you execute this command you are likely to get an error message that reads something like this: Error in $<-.data.frame(*tmp*, ''rangeC'', value = c(5.81818199157715, : replacement has 2 rows, data has 10. Let’s investigate why we get this error message, by picking one data row and calculating the range. range(data_N[1, 2:17]) ## [1] 7.958333 14.102941 You can see that we get two values - the maximum and the minimum. However, we need the range as a single value (maximum–minimum). The error arises because we tried to save two variables as a single variable. So we first need to calculate the maximum and minimum for our whole dataset (using the apply function) and save this in a new variable called temp. temp <- apply(data_N[, 2:17], 1, range) temp ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 7.958333 6.272727 6.25000 5.97500 5.42500 4.546875 3.921875 ## [2,] 14.102941 14.132353 13.72059 12.89706 12.33824 11.676471 10.779412 ## [,8] [,9] [,10] ## [1,] 3.15625 2.171875 1.300000 ## [2,] 10.63235 9.764706 8.681818 Now we use the difference between the respective maximums (Column 2) and minimums (Column 1) to define our new range variable rangeC. data_N$rangeC <- temp[2, ] - temp[1, ] And now we do the same for data_P: temp <- apply(data_P[,2:17], 1, range) data_P$rangeC <- temp[2, ] - temp[1, ] Let’s create a chart of the ranges for both experiments for all periods in order to compare them. plot(data_N$Period, data_N$rangeC, type = ''l'', col = ''blue'', lwd = 2, xlab = ''Round'', ylim = c(4, 14), ylab = ''Range of contributions'') lines(data_P$rangeC, col = ''red'', lwd = 2) title(''Range of contributions to public goods game'') legend(''bottomright'', lwd = 2, lty = 1, cex = 1.2, legend = c(''Without punishment'', ''With punishment''), col = c(''blue'', ''red''), ) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-02-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Range of contributions to public goods game. '' /> Range of contributions to public goods game. Figure 2.6 Range of contributions to public goods game. This chart confirms what we found in R walk-through 2.5, which is that there is a greater spread (variation) of contributions in the game with punishment. A concise way to describe the data is in a summary table. With just four numbers (mean, standard deviation, minimum value, maximum value), we can get a general idea of what the data looks like. Create a table of summary statistics that displays mean, variance, standard deviation, minimum, maximum and range for Periods 1 and 10 and for both experiments. Comment on any similarities and differences in the distributions, both across time and across experiments. R walk-through 2.7 Creating a table of summary statistics We have already done most of the work for creating this summary table in R walk-through 2.6. Since we also want to display the minimum and maximum values, we should add these to the dataframes data_N and data_P. data_N$minC <- apply(data_N[, 2:17], 1, min) data_N$maxC <- apply(data_N[, 2:17], 1, max) data_P$minC <- apply(data_P[, 2:17], 1, min) data_P$maxC <- apply(data_P[, 2:17], 1, max) Now we display the summary statistics in a table. We enclose our command in the round function, which reduces the number of digits displayed after the decimal point (2 in our case) and makes the table easier to read. print(''Public goods game without punishment'') ## [1] ''Public goods game without punishment'' # We want to see Rows 1 and 10 and Column 1 (which contains the period number) as well as Columns 18 to 23 (which contain the statistics). round(data_N[c(1, 10), c(1, 18:23)], digits = 2) ## # A tibble: 2 x 7 ## Period meanC varC sdC rangeC minC maxC ## <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> ## 1 1 10.58 4.08 2.02 6.14 7.96 14.10 ## 2 10 4.38 4.78 2.19 7.38 1.30 8.68 We repeat this command for the version with punishment. # Show two digits options(signif = 2) print(''Public goods game with punishment'') ## [1] ''Public goods game with punishment'' round(data_P[c(1, 10), c(1, 18:23)], digits = 2) ## # A tibble: 2 x 7 ## Period meanC varC sdC rangeC minC maxC ## <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> ## 1 1 10.64 10.29 3.21 10.20 5.82 16.02 ## 2 10 12.87 15.19 3.90 11.31 6.20 17.51 Part 2.3 Did changing the rules of the game affect behaviour? Learning objectives for this part calculate and interpret the p-value evaluate the usefulness of experiments for determining causality, and the limitations of these experiments. The punishment option was introduced into the public goods game in order to see whether it could help sustain contributions, compared to the game without a punishment option. We will now use a calculation called a p-value to compare the results from both experiments more formally. By comparing the results in Period 10 of both experiments, we can see that the mean contribution in the experiment with punishment is 8.5 units higher than in the experiment without punishment (see Figure 2.6). Is it more likely that this behaviour is due to chance, or is it more likely to be due to the difference in experimental conditions? You can conduct another experiment to understand why we might see differences in behaviour that are due to chance. First, flip a coin six times, using one hand only, and record the results (for example, Heads, Heads, Tails, etc.). Then, using the same hand, flip a coin six times and record the results again. Compare the outcomes from Question 1(a). Did you get the same number of heads in both cases? Even if you did, was the sequence of the outcomes (for example, Heads, Tails, Tails …) the same in both cases? The important point to note is that even when we conduct experiments under the same controlled conditions, due to an element of randomness, we may not observe the exact same behaviour each time we do the experiment. Randomness arises because the statistical analysis is conducted on a sample of data, and the sample we observe is only one of many possible samples. Whatever differences we calculate between two samples would almost certainly change if we had observed another pair of samples. Importantly, economists aren’t really interested in whether two samples are actually different, but rather whether the underlying populations, from which the samples were drawn, are different. And this is the challenge faced by the empirical economist. The p-value gives us a measure of how likely it is that we could observe the differences in our sample groups, if there were no difference between the populations. The smaller the p-value, the less likely that we would observe such differences. And the smaller this p-value, the smaller will be our confidence in the hypothesis that there are no differences in the populations. When we are interested in whether a treatment works — in this case, whether having the punishment option makes a difference — we want a way to check whether any observed differences could just be due to sample variation. The size of the difference alone cannot tell us whether it might just be due to chance. Even if the observed difference seems large, it could be small relative to how much the data vary. Figures 2.7 and 2.8 show the mean exam score of two groups of high school students and the size of house in which they live (represented by the height of the columns, and reported in the boxes above the columns), with the dots representing the underlying data. Figure 2.7 shows a relatively large difference in means that could have arisen by chance because the data is widely spread out (the standard deviation is large), while Figure 2.8 shows a relatively small difference that looks unlikely to be due to chance because the data is tightly clustered together (the standard deviation is very small). Note that we are looking at two distinct questions here: first, is there a large or small difference in exam score associated with the size of house of the student and second, is that difference likely to have arisen by chance. A social scientist is interested in the answer to both questions. If the difference is large but could easily have occurred by chance or if the difference is very small and unlikely to have occurred by chance, then the results are not suggestive of an important relationship between size of house and exam grade. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''An example of a large difference in means that is likely to have happened by chance. '' /> An example of a large difference in means that is likely to have happened by chance. Figure 2.7 An example of a large difference in means that is likely to have happened by chance. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''An example of a small difference in means that is unlikely to have happened by chance. '' /> An example of a small difference in means that is unlikely to have happened by chance. Figure 2.8 An example of a small difference in means that is unlikely to have happened by chance. p-valueThe probability of observing the data collected, assuming that any differences observed between the two groups of interest have happened by chance. The p-value ranges from 0 to 1, where lower values indicate a higher probability that the underlying assumption (differences observed have happened by chance) is false. The lower the probability (the lower the p-value), the less likely it is to observe the given data, and therefore the more likely it is that the assumption is false (the observed differences are unlikely to have happened by chance). To help us decide whether the observed differences have arisen by chance we can calculate a p-value. Without going into any detail, to calculate the p-value we require the sample means and the sample standard deviations. Combined with the assumption that there are no differences in the population we can then calculate the p-value which is a measure (but not the probability) of how likely it is that the observed differences are due to chance. It is important to notice that the p-value does not give us a definite answer. When we look at the data in Figure 2.7, we cannot be absolutely certain that there really is a link between house size and exam scores. But if the p-value for the difference is very small (for example, 0.02) then we know that there would only be a 2% probability of seeing the differences we did observe in the sample if in truth (i.e. in the population) there was no relationship between house size and exam scores. hypothesis testA test in which a null (default) and an alternative hypothesis are posed about some characteristic of the population. Sample data is then used to test how likely it is that these sample data would be seen if the null hypothesis was true. Find out more Hypothesis testing and p-values The process of formulating a hypothesis about the data, calculating the p-value, and using it to assess whether what we observe is consistent with the hypothesis, is known as a hypothesis test. When we conduct a hypothesis test, we consider two hypotheses: either there is no difference between the populations, in which case the differences we observe must have happened by chance (known as the ‘null hypothesis’); or the populations really are different (known as the ‘alternative hypothesis’). The smaller the p-value, the lower the probability that the differences we observe could have happened simply by chance, i.e. if the null hypothesis were true. The smaller the p-value, the stronger the evidence in favour of the alternative hypothesis. It is a common, but highly debatable practice, to pick a cutoff level for the p-value, and reject the null hypothesis if the p-value is below this cutoff. This approach has been criticized recently by statisticians and social scientists because the cutoff level is quite arbitrary. Instead of using a cutoff, we prefer to calculate p-values and use them to assess the strength of the evidence. Whether the statistical evidence is strong enough for us to draw a firm conclusion about the data will always be a matter of judgement. In particular, you want to make sure that you understand the consequences of concluding that the null hypothesis is not true, and hence that the alternative is true. You may be quite easily prepared to conclude that house sizes and exam scores are related, but much more cautious about deciding that a new medication is more effective than an existing one if you know that this new medication has severe side effects. In the case of the medication, you might want to see stronger evidence against the null hypothesis before deciding that doctors should be advised to prescribe the new medication. We will calculate the p-value and use it to assess how likely it is that the differences we observe are due to chance. Using the data for Figures 2A and 3: Use the t.test function to calculate the p-value for the difference in means in Period 1 (with and without punishment). What does this p-value tell us about the difference in means in Period 1? R walk-through 2.8 Calculating the p-value for the difference in means We need to extract the observations in Period 1 for the data for with and without punishment, and then feed the observations into the t.test function. The t.test function is extremely flexible: if you input two variables (x and y) as shown below, it will automatically test whether the difference in means is due to chance or not (formally speaking, it tests the null hypothesis that the means of both variables are equal). p1_N <- data_N[1, 2:17] p1_P <- data_P[1, 2:17] t.test(x = p1_N, y = p1_P) Unfortunately, if you run this code, you are likely to get an error message like this: Error: Unsupported use of matrix or array for column indexing. The reason for this is that p1_N and p1_P are still ‘tibbles’ (dataframes) with one observation (row) and 16 variables (columns). However, the t.test function requires both x and y to be variables (with one column and many rows), so we need to reformat p1_N and p1_P. One way to do this is to use the t function (which switches the rows and columns; if you are familiar with matrices, this is the same idea as a vector transpose). p1_N <- t(data_N[1, 2:17]) p1_P <- t(data_P[1, 2:17]) t.test(x = p1_N, y = p1_P) ## ## Welch Two Sample t-test ## ## data: p1_N and p1_P ## t = -0.063782, df = 25.288, p-value = 0.9496 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.011123 1.890231 ## sample estimates: ## mean of x mean of y ## 10.57831 10.63876 This result delivers a p-value of 0.9496. This means it is very likely that the assumption that there are no differences in the populations is likely to be true (formally speaking, we cannot reject the null hypothesis). The t.test function automatically assumes that both variables were generated by different groups of people. When calculating the p-value, it assumes that the observed differences are partly due to some variation in characteristics between these two groups, and not just the differences in experimental conditions. However, in this case, the same groups of people did both experiments, so there will not be any variation in characteristics between the groups. When calculating the p-value, we account for this fact with the paired option. p1_N <- t(data_N[1, 2:17]) p1_P <- t(data_P[1, 2:17]) t.test(x = p1_N, y = p1_P, paired = TRUE) ## ## Paired t-test ## ## data: p1_N and p1_P ## t = -0.14996, df = 15, p-value = 0.8828 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.9195942 0.7987027 ## sample estimates: ## mean of the differences ## -0.06044576 As you can see, the p-value becomes smaller as we can attribute more of the differences to the ‘with punishment’ treatment, but the p-value is still very large (0.8828), so we still conclude that the differences in Period 1 are likely to be due to chance. Using the data for Period 10: Use the t.test function to calculate the p-value for the difference in means in Period 10 (with and without punishment). What does this p-value tell us about the relationship between punishment, and behaviour in the public goods game? With reference to Figure 2.7 and Figure 2.8, explain why we cannot use the size of the difference to directly conclude whether the difference could be due to chance. spurious correlationA strong linear association between two variables that does not result from any direct relationship, but instead may be due to coincidence or to another unseen factor. An important point to note is that calculating p-values may not tell us anything about causation. The example of house size and exam scores shown in Figure 2.8, gives us evidence that some kind of relationship between house size and exam scores is very likely. However, we would not conclude that building an extra room automatically makes someone smarter. P-values cannot help us detect these spurious correlations. However, calculating p-values for experimental evidence can help us determine whether there is a causal link between two variables. If we conduct an experiment and find a difference in outcomes with a low p-value, then we may conclude that the change in experimental conditions is likely to have caused the difference. Refer to the results from the public goods games. Which characteristics of the experimental setting make it likely that the with punishment option was the cause of the change in behaviour? Using Figure 2.6, explain why we need to compare the two groups in Period 1 in order to conclude that there is a causal link between the with punishment option and behaviour in the game. Experiments can be useful for identifying causal links. However, if people’s behaviour in experimental conditions were different from their behaviour in the real world, our results would not be applicable anywhere outside the experiment. Discuss some limitations of experiments, and suggest some ways to address (or partially address) them. (You may find pages 158–171 of the paper ‘What do laboratory experiments measuring social preferences reveal about the real world?’ helpful, as well as the discussion on free riding and altruism in Section 2.6 of Economy, Society, and Public Policy.) Benedikt Herrmann, Christian Thöni, and Simon Gächter. 2008. Figure 3 in ‘Antisocial punishment across societies’. Science Magazine 319 (5868): p. 1365. ↩"
});
index.addDoc({
    id: 15,
    title: "Doing Economics: Empirical Project 2: Working in Google Sheets",
    content: "Empirical Project 2 Working in Google Sheets Part 2.1 Collecting data by playing a public goods game Learning objectives for this part collect data from an experiment and enter it into Google Sheets use summary measures, for example, mean and standard deviation, and line charts to describe and compare data. Note You can still do Parts 2.2 and 2.3 without completing this part of the project. Before taking a closer look at the experimental data, you will play a public goods game like the one in the introduction with your classmates to learn how experimental data can be collected. If your instructor has not set up a game, follow the instructions below to set up your own game. Instructions How to set up the public goods game Form a group of at least four people. (You may also want to set a maximum of 8 or 10 players to make the game easier to play). Choose one person to be the game administrator. The administrator will monitor the game, while the other people play the game. Administrator Create the game: Go to the ‘Economics Games’ website, scroll down to the bottom of the page, and click ‘Create a Multiplayer Game and Get Logins’. Then click ‘Externalities and public goods’. Under the heading ‘Voluntary contribution to a public good’, click ‘Choose this Game’. Enter in the number of people playing the game, and select ‘1’ for the number of universes. Then click ‘Get Logins’. A pop-up will appear, showing the login IDs and passwords for the players and for the administrator. Start the game: Give each player a different login ID. The game should be played anonymously, so make sure that players do not know the login IDs of other players. You are now ready to start the first round of the game. There are ten rounds in total. Confirm that all the rounds are complete: On the top right corner of the webpage, click ‘Login’, enter your login ID and password, and then click the green ‘Login’ button. You will be taken to the game administration page, which will show the average contribution in each round, and the results of the round just played. Wait until all the players have finished playing ten rounds before refreshing this page. Collect the game results: Once the players have finished playing ten rounds, refresh this page. The table at the top of the page will now show the average contribution (in euros) for each of the ten rounds played. Select the whole table, then copy and paste it into a new worksheet in Google Sheets. Players Login: Once the administrator has created the game, go to the ‘Economics Games’ website. On the top right corner, click ‘Login’, enter the login ID and password that your administrator has given you, then click the green ‘Login’ button. You will be taken to the public goods game that your administrator has set up. Play the first round of the game: Read the instructions at top of the page carefully before starting the game. In each round, you must decide how much to contribute to the public good. Enter your choice for each universe (group of players) that you are a part of (if the same players are in two universes, then make the same contribution in both), then click ‘Validate’. View the results of the first round: You will then be shown the results of the first round, including how much each player (including yourself) contributed, the payoffs, and the profits. Click ‘Next’ to start the next round. Complete all the rounds of the game: Repeat steps 2 and 3 until you have played ten rounds in total, then collect the results of the game from your administrator. The results from your game will look like Figure 2.1. In the questions below you will compare your results with those in Figure 3 of Herrmann et al. (2008), but first you need to reformat your table to look like Figure 2.2. Follow the steps in Google Sheets walk-through 2.1 to reformat your table. Round 10 9 8 7 6 5 4 3 2 1 Average contribution A table formatted with ‘Round’ and ‘Average contribution’ as the row variables. Figure 2.1 A table formatted with ‘Round’ and ‘Average contribution’ as the row variables. Round Average contribution 1 2 3 4 5 6 7 8 9 10 A table formatted with ‘Round’ and ‘Average contribution’ as the column variables. Figure 2.2 A table formatted with ‘Round’ and ‘Average contribution’ as the column variables. Google Sheets walk-through 2.1 Reformatting a table <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to reformat a table. '' /> Figure 2.3 How to reformat a table. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : The table generated from playing the public goods game will look like the one shown in Rows 1–2. We need to reformat it so that the columns show the different variables, rather than the rows. '' /> The data The table generated from playing the public goods game will look like the one shown in Rows 1–2. We need to reformat it so that the columns show the different variables, rather than the rows. Figure 2.3a The table generated from playing the public goods game will look like the one shown in Rows 1–2. We need to reformat it so that the columns show the different variables, rather than the rows. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Copy and paste the transposed table : The transpose option switches the rows and columns of the table, so the first column becomes the first row and so on. '' /> Copy and paste the transposed table The transpose option switches the rows and columns of the table, so the first column becomes the first row and so on. Figure 2.3b The transpose option switches the rows and columns of the table, so the first column becomes the first row and so on. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Rearrange rows in the correct order : The rows in Column A are arranged in descending order (Round 10, Round 9, and so on), so we will sort the data in reverse order. '' /> Rearrange rows in the correct order The rows in Column A are arranged in descending order (Round 10, Round 9, and so on), so we will sort the data in reverse order. Figure 2.3c The rows in Column A are arranged in descending order (Round 10, Round 9, and so on), so we will sort the data in reverse order. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-03-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The reformatted table : The table is now reformatted as required. '' /> The reformatted table The table is now reformatted as required. Figure 2.3d The table is now reformatted as required. Use the results of the game you have played to answer the following questions. Make a line chart with average contribution as the vertical axis variable, and period (from 1 to 10) on the horizontal axis. Describe how average contributions have changed over the course of the game. Google Sheets walk-through 2.2 Drawing a line chart with multiple variables <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to plot a line chart with multiple variables. '' /> Figure 2.4 How to plot a line chart with multiple variables. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the data looks like. Each column has data for a particular country, and each row has data for a given time period (1 to 10). We will draw Figure 3 from Herrmann et al. as an example; the steps to do Figure 2A are identical. '' /> The data This is what the data looks like. Each column has data for a particular country, and each row has data for a given time period (1 to 10). We will draw Figure 3 from Herrmann et al. as an example; the steps to do Figure 2A are identical. Figure 2.4a This is what the data looks like. Each column has data for a particular country, and each row has data for a given time period (1 to 10). We will draw Figure 3 from Herrmann et al. as an example; the steps to do Figure 2A are identical. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a line graph : After completing step 4, the graph will look like this. '' /> Draw a line graph After completing step 4, the graph will look like this. Figure 2.4b After completing step 4, the graph will look like this. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Move the legend to the right : After step 6, the legend will now be on the right-hand side of your chart. You can also experiment with the other positions to see which looks better. '' /> Move the legend to the right After step 6, the legend will now be on the right-hand side of your chart. You can also experiment with the other positions to see which looks better. Figure 2.4c After step 6, the legend will now be on the right-hand side of your chart. You can also experiment with the other positions to see which looks better. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-04-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add horizontal axis labels for every period : After step 7, the horizontal axis will be labelled with all periods. '' /> Add horizontal axis labels for every period After step 7, the horizontal axis will be labelled with all periods. Figure 2.4d After step 7, the horizontal axis will be labelled with all periods. Compare your line chart with Figure 3 of Herrmann et al. (2008).1 Comment on any similarities or differences between the results (for example, the amount contributed at the start and end, or the change in average contributions over the course of the game). Can you think of any reasons why your results are similar to (or different from) those in Figure 3? You may find it helpful to read the ‘Experiments’ section of the Herrmann et al. (2008) study for a more detailed description of how the experiments were conducted. Part 2.2 Describing the data Learning objectives for this part use summary measures, for example, mean and standard deviation, and column charts to describe and compare data. Note You can still do Parts 2.2 and 2.3 without completing this part of the project. We will now use the data for Figures 2A and 3 of Herrmann et al. (2008), and evaluate the effect of the punishment option on average contributions. Rather than compare two charts showing all of the data from each experiment, as the authors of the study did, we will use summary measures to compare the data, and show the data from both experiments (with and without punishment) on the same chart. meanA summary statistic for a set of observations, calculated by adding all values in the set and dividing by the number of observations.varianceA measure of dispersion in a frequency distribution, equal to the mean of the squares of the deviations from the arithmetic mean of the distribution. The variance is used to indicate how ‘spread out’ the data is. A higher variance means that the data is more spread out. Example: The set of numbers 1, 1, 1 has zero variance (no variation), while the set of numbers 1, 1, 999 has a high variance of 221,334 (large spread). First, download and save the data. The spreadsheet contains two tables: The first table shows average contributions in a public goods game without punishment (Figure 3). The second shows average contributions in a public goods game with punishment (Figure 2A). You can see that in each period (row), the average contribution varies across countries, in other words, there is a distribution of average contributions in each period. The mean and variance are two ways to summarize distributions. We will now use these measures, along with other measures (range and standard deviation) to summarize and compare the distribution of contributions in both experiments. Before answering these questions, make sure you understand mean and variance, and how to calculate these measures in Google Sheets. See Figure 1.5 in Exercise 1.3 of Economy, Society, and Public Policy for more information about the mean. See Google Sheets walk-through 1.6 for more information about the variance. Using the data for Figures 2A and 3 of Herrmann et al. (2008): Calculate the mean contribution in each period (row) separately for both experiments, using the AVERAGE function. Plot a line chart of mean contribution on the vertical axis and time period (from 1 to 10) on the horizontal axis (with a separate line for each experiment). Make sure the lines in the legend are clearly labelled according to the experiment (with punishment or without punishment). Describe any differences and similarities you see in the mean contribution over time in both experiments. Instead of looking at all periods, we can focus on contributions in the first and last period. Plot a column chart showing the mean contribution in the first and last period for both experiments. Your chart should look like Figure 2.6. Google Sheets walk-through 2.3 Drawing a column chart to compare two groups <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to draw a column chart to compare two groups. '' /> Figure 2.5 How to draw a column chart to compare two groups. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the data looks like. Column R has the means for Figure 3 (without punishment). Column S has the means for Figure 2A (with punishment). We will use the cells in bold font (Cells R3, R12, S3, and S12) to make the chart. '' /> The data This is what the data looks like. Column R has the means for Figure 3 (without punishment). Column S has the means for Figure 2A (with punishment). We will use the cells in bold font (Cells R3, R12, S3, and S12) to make the chart. Figure 2.5a This is what the data looks like. Column R has the means for Figure 3 (without punishment). Column S has the means for Figure 2A (with punishment). We will use the cells in bold font (Cells R3, R12, S3, and S12) to make the chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create a table with relevant data only : When creating a new table, make sure that all the adjacent cells are empty as shown. Otherwise, Google Sheets might misinterpret your data and draw the chart incorrectly. '' /> Create a table with relevant data only When creating a new table, make sure that all the adjacent cells are empty as shown. Otherwise, Google Sheets might misinterpret your data and draw the chart incorrectly. Figure 2.5b When creating a new table, make sure that all the adjacent cells are empty as shown. Otherwise, Google Sheets might misinterpret your data and draw the chart incorrectly. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw the column chart : Use the new table you created to make the column chart. '' /> Draw the column chart Use the new table you created to make the column chart. Figure 2.5c Use the new table you created to make the column chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Move the legend to the bottom and add axis titles : After step 4, the legend will now be underneath your chart. You can also experiment with the other positions to see which looks better. '' /> Move the legend to the bottom and add axis titles After step 4, the legend will now be underneath your chart. You can also experiment with the other positions to see which looks better. Figure 2.5d After step 4, the legend will now be underneath your chart. You can also experiment with the other positions to see which looks better. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-05-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add data labels on top of the columns : After completing step 7, numbers showing the height of the column will appear in the columns selected. Your chart should now look like Figure 2.6. '' /> Add data labels on top of the columns After completing step 7, numbers showing the height of the column will appear in the columns selected. Your chart should now look like Figure 2.6. Figure 2.5e After completing step 7, numbers showing the height of the column will appear in the columns selected. Your chart should now look like Figure 2.6. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Average contributions in Periods 1 and 10, with and without punishment. '' /> Average contributions in Periods 1 and 10, with and without punishment. Figure 2.6 Average contributions in Periods 1 and 10, with and without punishment. varianceA measure of dispersion in a frequency distribution, equal to the mean of the squares of the deviations from the arithmetic mean of the distribution. The variance is used to indicate how ‘spread out’ the data is. A higher variance means that the data is more spread out. Example: The set of numbers 1, 1, 1 has zero variance (no variation), while the set of numbers 1, 1, 999 has a high variance of 221,334 (large spread). The mean is one useful measure of the ‘middle’ of a distribution, but is not a complete description of what our data looks like. We also need to know how ‘spread out’ the data is in order to get a clearer picture and make comparisons between the distributions. The variance is one way to measure spread—the higher the variance, the more spread out the data is. standard deviationA measure of dispersion in a frequency distribution, equal to the square root of the variance. The standard deviation has a similar interpretation to the variance. A larger standard deviation means that the data is more spread out. Example: The set of numbers 1, 1, 1 has a standard deviation of zero (no variation or spread), while the set of numbers 1, 1, 999 has a standard deviation of 46.7 (large spread). A similar measure is standard deviation, which is the square root of the variance. Standard deviation is commonly used because it provides a handy rule of thumb for large datasets—most of the data (95% if there are many observations) will be less than two standard deviations away from the mean. Using the data for Figures 2A and 3 of Herrmann et al. (2008): Calculate the standard deviation for Periods 1 and 10 separately, for both experiments. Does the rule of thumb apply? (In other words, are most values within two standard deviations of the mean?) As shown in Figure 2.6, the mean contribution for both experiments was 10.6 in Period 1. With reference to your standard deviation calculations, explain whether this means that the two sets of data are the same. Google Sheets walk-through 2.4 Calculating the standard deviation <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to calculate and understand the standard deviation. '' /> Figure 2.7 How to calculate and understand the standard deviation. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : We will compare the example data from Google Sheets walk-through 2.1 with some new data that is less spread out. You can see from Column H that all the values are between 10 to 12 (inclusive). '' /> The data We will compare the example data from Google Sheets walk-through 2.1 with some new data that is less spread out. You can see from Column H that all the values are between 10 to 12 (inclusive). Figure 2.7a We will compare the example data from Google Sheets walk-through 2.1 with some new data that is less spread out. You can see from Column H that all the values are between 10 to 12 (inclusive). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Standard deviation calculation and interpretation : The STDEVP function will calculate the standard deviation over the selected cells. To enter in the formula, click on an empty cell. '' /> Standard deviation calculation and interpretation The STDEVP function will calculate the standard deviation over the selected cells. To enter in the formula, click on an empty cell. Figure 2.7b The STDEVP function will calculate the standard deviation over the selected cells. To enter in the formula, click on an empty cell. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-07-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The relationship between the standard deviation and the variance : Both the variance and standard deviation measure spread. We need the variance to calculate the standard deviation, but we usually use the standard deviation to describe distributions because of the handy rule of thumb. '' /> The relationship between the standard deviation and the variance Both the variance and standard deviation measure spread. We need the variance to calculate the standard deviation, but we usually use the standard deviation to describe distributions because of the handy rule of thumb. Figure 2.7c Both the variance and standard deviation measure spread. We need the variance to calculate the standard deviation, but we usually use the standard deviation to describe distributions because of the handy rule of thumb. rangeThe interval formed by the smallest (minimum) and the largest (maximum) value of a particular variable. The range shows the two most extreme values in the distribution, and can be used to check whether there are any outliers in the data. (Outliers are a few observations in the data that are very different from the rest of the observations.) Another measure of spread is the range, the interval formed by the smallest (minimum) and the largest (maximum) values of a particular variable. For example, we might say that the number of periods in the public goods experiment ranges from 1 to 10. Once we know the most extreme values in our dataset, we have a better picture of what our data looks like. Calculate the maximum and minimum value for Periods 1 and 10 separately, for both experiments. Google Sheets walk-through 2.5 Finding the minimum, maximum, and range of a variable <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to find the minimum, maximum, and range of a variable. '' /> Figure 2.8 How to find the minimum, maximum, and range of a variable. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : Here we are going to calculate the minimum and maximum value of the example data in Google Sheets walk-through 2.1. '' /> The data Here we are going to calculate the minimum and maximum value of the example data in Google Sheets walk-through 2.1. Figure 2.8a Here we are going to calculate the minimum and maximum value of the example data in Google Sheets walk-through 2.1. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the minimum value : The MIN function calculates the minimum value of the selected cells. Here, the minimum value is 4. '' /> Calculate the minimum value The MIN function calculates the minimum value of the selected cells. Here, the minimum value is 4. Figure 2.8b The MIN function calculates the minimum value of the selected cells. Here, the minimum value is 4. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-08-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the maximum value and the range : The MAX function calculates the maximum value. The maximum value here is 18. The range is the interval formed by the minimum and the maximum value. In words, we say ‘the range is 4 to 18’ or ‘the average contribution ranges from 4 to 18’. In numbers, we say ‘the range is [4,18]’. '' /> Calculate the maximum value and the range The MAX function calculates the maximum value. The maximum value here is 18. The range is the interval formed by the minimum and the maximum value. In words, we say ‘the range is 4 to 18’ or ‘the average contribution ranges from 4 to 18’. In numbers, we say ‘the range is [4,18]’. Figure 2.8c The MAX function calculates the maximum value. The maximum value here is 18. The range is the interval formed by the minimum and the maximum value. In words, we say ‘the range is 4 to 18’ or ‘the average contribution ranges from 4 to 18’. In numbers, we say ‘the range is [4,18]’. A concise way to describe the data is in a summary table. With just four numbers (mean, standard deviation, minimum value, maximum value), we can get a general idea of what the data looks like. In Google Sheets, create a summary table as shown in Figure 2.9 below. Make three more summary tables, for Period 10 (without punishment), Period 1 (with punishment), and Period 10 (with punishment). Use your answers to Questions 2 to 4 to complete the summary tables. Comment on any similarities and differences in the distributions, both across time and across experiments. Mean Standard deviation Minimum Maximum Contribution (Period 1, without punishment) A summary table for contributions in a given period. Figure 2.9 A summary table for contributions in a given period. Part 2.3 Did changing the rules of the game affect behaviour? Learning objectives for this part calculate and interpret the p-value evaluate the usefulness of experiments for determining causality, and the limitations of these experiments. The punishment option was introduced into the public goods game in order to see whether it could help sustain contributions, compared to the game without a punishment option. We will now use a calculation called a p-value to compare the results from both experiments more formally. By comparing the results in Period 10 of both experiments, we can see that the mean contribution in the experiment with punishment is 8.5 units higher than in the experiment without punishment (see Figure 2.6). Is it more likely that this behaviour is due to chance, or is it more likely to be due to the difference in experimental conditions? You can conduct another experiment to understand why we might see differences in behaviour that are due to chance. First, flip a coin six times, using one hand only, and record the results (for example, Heads, Heads, Tails, etc.). Then, using the same hand, flip a coin six times and record the results again. Compare the outcomes from Question 1(a). Did you get the same number of heads in both cases? Even if you did, was the sequence of the outcomes (for example, Heads, Tails, Tails …) the same in both cases? The important point to note is that even when we conduct experiments under the same controlled conditions, due to an element of randomness, we may not observe the exact same behaviour each time we do the experiment. Randomness arises because the statistical analysis is conducted on a sample of data, and the sample we observe is only one of many possible samples. Whatever differences we calculate between two samples would almost certainly change if we had observed another pair of samples. Importantly, economists aren’t really interested in whether two samples are actually different, but rather whether the underlying populations, from which the samples were drawn, are different. And this is the challenge faced by the empirical economist. The p-value gives us a measure of how likely it is that we could observe the differences in our sample groups, if there were no difference between the populations. The smaller the p-value, the less likely that we would observe such differences. And the smaller this p-value, the smaller will be our confidence in the hypothesis that there are no differences in the populations. When we are interested in whether a treatment works — in this case, whether having the punishment option makes a difference — we want a way to check whether any observed differences could just be due to sample variation. The size of the difference alone cannot tell us whether it might just be due to chance. Even if the observed difference seems large, it could be small relative to how much the data vary. Figures 2.10 and 2.11 show the mean exam score of two groups of high school students and the size of house in which they live (represented by the height of the columns, and reported in the boxes above the columns), with the dots representing the underlying data. Figure 2.10 shows a relatively large difference in means that could have arisen by chance because the data is widely spread out (the standard deviation is large), while Figure 2.11 shows a relatively small difference that looks unlikely to be due to chance because the data is tightly clustered together (the standard deviation is very small). Note that we are looking at two distinct questions here: first, is there a large or small difference in exam score associated with the size of house of the student and second, is that difference likely to have arisen by chance. A social scientist is interested in the answer to both questions. If the difference is large but could easily have occurred by chance or if the difference is very small and unlikely to have occurred by chance, then the results are not suggestive of an important relationship between size of house and exam grade. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-10.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''An example of a large difference in means that is likely to have happened by chance. '' /> An example of a large difference in means that is likely to have happened by chance. Figure 2.10 An example of a large difference in means that is likely to have happened by chance. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-02-11.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''An example of a small difference in means that is unlikely to have happened by chance. '' /> An example of a small difference in means that is unlikely to have happened by chance. Figure 2.11 An example of a small difference in means that is unlikely to have happened by chance. p-valueThe probability of observing the data collected, assuming that any differences observed between the two groups of interest have happened by chance. The p-value ranges from 0 to 1, where lower values indicate a higher probability that the underlying assumption (differences observed have happened by chance) is false. The lower the probability (the lower the p-value), the less likely it is to observe the given data, and therefore the more likely it is that the assumption is false (the observed differences are unlikely to have happened by chance). To help us decide whether the observed differences have arisen by chance we can calculate a p-value. Without going into any detail, to calculate the p-value we require the sample means and the sample standard deviations. Combined with the assumption that there are no differences in the population we can then calculate the p-value which is a measure (but not the probability) of how likely it is that the observed differences are due to chance. It is important to notice that the p-value does not give us a definite answer. When we look at the data in Figure 2.10, we cannot be absolutely certain that there really is a link between house size and exam scores. But if the p-value for the difference is very small (for example, 0.02) then we know that there would only be a 2% probability of seeing the differences we did observe in the sample if in truth (i.e. in the population) there was no relationship between house size and exam scores. hypothesis testA test in which a null (default) and an alternative hypothesis are posed about some characteristic of the population. Sample data is then used to test how likely it is that these sample data would be seen if the null hypothesis was true. Find out more Hypothesis testing and p-values The process of formulating a hypothesis about the data, calculating the p-value, and using it to assess whether what we observe is consistent with the hypothesis, is known as a hypothesis test. When we conduct a hypothesis test, we consider two hypotheses: either there is no difference between the populations, in which case the differences we observe must have happened by chance (known as the ’null hypothesis’); or the populations really are different (known as the ‘alternative hypothesis’). The smaller the p-value, the lower is the probability that the differences we observe could have happened simply by chance, i.e. if the null hypothesis was true. The smaller the p-value, the stronger is the evidence in favour of the alternative hypothesis. It is a common, but highly debatable practice, to pick a cutoff level for the p-value, and reject the null hypothesis if the p-value is below this cutoff. This approach has been criticized recently by statisticians and social scientists because the cutoff level is quite arbitrary. Instead of using a cutoff, we prefer to calculate p-values and use them to assess the strength of the evidence. Whether the statistical evidence is strong enough for us to draw a firm conclusion about the data will always be a matter of judgement. In particular, you want to make sure that you understand the consequences of concluding that the null hypothesis is not true, and hence that the alternative is true. You may be quite easily prepared to conclude that house sizes and exam scores are related, but much more cautious about deciding that a new medication is more effective than an existing one if you know that this new medication has severe side effects. In the case of the medication, you might want to see stronger evidence against the null hypothesis before deciding that doctors should be advised to prescribe the new medication. To calculate the p-value in Google Sheets, we use a function called TTEST. Using the data for Figures 2A and 3: Use Google Sheet’s TTEST function to calculate the p-value for the difference in means in Period 1 (with and without punishment). What is the p-value? What does this p-value tell us about the difference in means in Period 1? Google Sheets walk-through 2.6 Calculating and interpreting the p-value <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to calculate and interpret the p-value. '' /> Figure 2.12 How to calculate and interpret the p-value. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : Here we are going to calculate the p-values we need. '' /> The data Here we are going to calculate the p-values we need. Figure 2.12a Here we are going to calculate the p-values we need. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the p-value for Period 1 data : The TTEST function will calculate the p-value for the two groups of cells selected. In the example shown, the formula to type is =TTEST(B3:Q3,B17:Q17,2,1). In this example there is a high probability that the difference we observe could have happened simply by chance. '' /> Calculate the p-value for Period 1 data The TTEST function will calculate the p-value for the two groups of cells selected. In the example shown, the formula to type is =TTEST(B3:Q3,B17:Q17,2,1). In this example there is a high probability that the difference we observe could have happened simply by chance. Figure 2.12b The TTEST function will calculate the p-value for the two groups of cells selected. In the example shown, the formula to type is =TTEST(B3:Q3,B17:Q17,2,1). In this example there is a high probability that the difference we observe could have happened simply by chance. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-02-12-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the p-value for Period 10 data : Follow these steps to calculate the p-value for Period 10 (the cells in the dotted boxes). The only difference from the previous formula (for Period 1) is the data selected. '' /> Calculate the p-value for Period 10 data Follow these steps to calculate the p-value for Period 10 (the cells in the dotted boxes). The only difference from the previous formula (for Period 1) is the data selected. Figure 2.12c Follow these steps to calculate the p-value for Period 10 (the cells in the dotted boxes). The only difference from the previous formula (for Period 1) is the data selected. Using the data for Period 10: Use Google Sheet’s TTEST function to calculate the p-value for the difference in means in Period 10 (with and without punishment). What is the p-value? What does this p-value tell us about the relationship between punishment, and behaviour in the public goods game? With reference to Figures 2.10 and 2.11, explain why we cannot use the size of the difference to directly conclude whether the difference could be due to chance. spurious correlationA strong linear association between two variables that does not result from any direct relationship, but instead may be due to coincidence or to another unseen factor. An important point to note is that calculating p-values may not tell us anything about causation. The example of house size and exam scores shown in Figure 2.11, gives us evidence that some kind of relationship between house size and exam scores is very likely. However, we would not conclude that building an extra room automatically makes someone smarter. P-values cannot help us detect these spurious correlations. However, calculating p-values for experimental evidence can help us determine whether there is a causal link between two variables. If we conduct an experiment and find a difference in outcomes with a low p-value, then we may conclude that the change in experimental conditions is likely to have caused the difference. Refer to the results from the public goods games. Which characteristics of the experimental setting make it likely that the punishment option was the cause of the change in behaviour? With reference to Figure 2.6, explain why we need to compare the two groups in Period 1 in order to conclude that there is a causal link between the punishment option and behaviour in the game. Experiments can be useful for identifying causal links. However, if people’s behaviour in experimental conditions were different from their behaviour in the real world, our results would not be applicable anywhere outside the experiment. Discuss some limitations of lab experiments, and suggest some ways to address (or partially address) them. (You may find pages 158–171 of the paper ‘What do laboratory experiments measuring social preferences reveal about the real world?’ helpful, as well as the discussion on free riding and altruism in Section 2.6 of Economy, Society, and Public Policy.) Benedikt Herrmann, Christian Thöni, and Simon Gächter. 2008. Figure 3 in ‘Antisocial punishment across societies’. Science Magazine 319 (5868): p. 1365. ↩"
});
index.addDoc({
    id: 16,
    title: "Doing Economics: Empirical Project 2 Solutions",
    content: "Empirical Project 2 Solutions These are not model answers. They are provided to help students, including those doing the project outside a formal class, to check their progress while working through the questions using the Excel, R, or Google Sheets walk-throughs. There are also brief notes for the more interpretive questions. Students taking courses using Doing Economics should follow the guidance of their instructors. Part 2.1 Collecting data by playing a public goods game Note Unless otherwise specified, numerical values are shown to two decimal places. The example data used in Questions 1 and 2 is from Excel walk-through 2.1. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-01.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-01-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-01-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-01-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-01.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Average contribution over time. '' /> Average contribution over time. Solution figure 2.1 Average contribution over time. As shown in Solution figure 2.1, the average contributions over the course of the game fluctuate around a mean value of about 11. For period 1, the contribution in the example game (9) is lower than the average contribution in Hermann et al. (2008). Results in Hermann et al. (2008) display a downward pattern over time, unlike those in the example game, which fluctuate without a clear trend. There are many possible reasons why results may be similar or different, including: social norms about how much people should contribute groups are not anonymous in your experiment (you know who your group members are even though the contribution of each member is anonymous); if you are friends with your group members, you may be able to sustain higher contributions. Part 2.2 Describing the data Solution figure 2.2 shows the mean contribution in each period for both experiments. Without punishment With punishment 10.58 10.64 10.63 11.95 10.41 12.66 9.81 12.97 9.31 13.33 8.45 13.50 7.84 13.57 7.38 13.64 6.39 13.57 4.38 12.87 Mean contributions by period, with and without punishment. Solution figure 2.2 Mean contributions by period, with and without punishment. Solution figure 2.3 shows the comparison of mean contributions over time. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Comparison of mean contributions over time. '' /> Comparison of mean contributions over time. Solution figure 2.3 Comparison of mean contributions over time. The mean contributions in the experiments are the same in Period 1. Over time, the mean contribution in the experiment with punishment remains relatively stable, even increasing, while that in the experiment without punishment falls steadily. The divergence over time leads to a large difference (8.5) by the end of Period 10. Solution figure 2.4 shows the mean contribution in the first and last period for both experiments. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-02-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Average contributions in Periods 1 and 10, with and without punishment. '' /> Average contributions in Periods 1 and 10, with and without punishment. Solution figure 2.4 Average contributions in Periods 1 and 10, with and without punishment. Solution figure 2.5 provides the standard deviation for Periods 1 and 10 for both experiments. To check the rule of thumb, check that most values fall within two standard deviations of the mean, which corresponds to the intervals shown by the square brackets (rounded to 1 decimal place): [8.6, 12.6] for Period 1, without punishment [7.5, 13.7] for Period 1, with punishment [2.3, 6.5] for Period 10, without punishment [9.1, 16.7] for Period 10, with punishment. Inspecting the data shows that the rule of thumb applies here. Period Without punishment With punishment 1 2.02 3.21 10 2.19 3.90 Standard deviations in both experiments. Solution figure 2.5 Standard deviations in both experiments. The fact that the mean contributions for both experiments in Period 1 are the same does not mean that the two sets of data are the same. The standard deviation corresponding to the experiment with punish­ment is greater, meaning that the data of the experiment with punishment is spread out over a wider range of values around the mean. This example emphasizes the importance of using more than one summary statistic to get a better picture of what the data looks like. The maximum and minimum for Periods 1 and 10 for both experiments are given in Solution figure 2.6. Without punishment With punishment Period Minimum Maximum Minimum Maximum 1 7.96 14.10 5.82 16.02 10 1.30 8.68 6.20 17.51 Minimum and maximum values for both experiments. Solution figure 2.6 Minimum and maximum values for both experiments. Solution figure 2.7 provides summary tables for Periods 1 and 10 for both experiments. Mean Standard deviation Minimum Maximum Contribution (Period 1, without punishment) 10.58 2.02 7.96 14.10 Contribution (Period 10, without punishment) 4.38 2.19 1.30 8.68 Contribution (Period 1, with punishment) 10.64 3.21 5.82 16.02 Contribution (Period 10, with punishment) 12.87 3.90 6.20 17.51 Summary tables for contributions in both experiments. Solution figure 2.7 Summary tables for contributions in both experiments. The two experiments have the same mean in Period 1. Over time, the mean contribution in the experiment without punishment decreases, while that in the experiment with punishment increases relative to the Period 1 values. The standard deviation in the experiment with punishment is greater in both periods. The standard deviations in both experiments increase over time, although the increase for the experiment with punishment is greater. The difference between maximum and minimum for the experiment with punishment is greater. Part 2.3 Did changing the rules of the game affect behaviour? Solution figure 2.8 provides a possible outcome for the experiment. Outcome sequence 1 Outcome sequence 2 Tails Heads Heads Heads Heads Tails Heads Tails Tails Tails Tails Tails Example data from two coin-toss experiments. Solution figure 2.8 Example data from two coin-toss experiments. In the example above, both the number of heads and the sequence of heads and tails are different, illustrating that we can still get different results under controlled conditions due to chance. The p-value is 0.88. Note that in this case, the p-value had to account for the fact that the data in both experiments was generated by the same groups of people, since each group plays the game with the punishment condition, and without the punishment condition. It is important to account for this fact when calculating the p-value. If the data for each condition came from different groups of people, we might expect that some of the variation in contributions is due to the fact that the people in each group might differ slightly in a number of aspects, such as how altruistic they are. Since the data for both experiments came from the same groups of people, we have controlled for these differences between groups. Thus, we would be more confident that the differences we observe are due to the fact that the experimental conditions are different. Given that the null hypothesis (our assumption when calculating the p-value) is that the means for both groups are the same, the probability of observing a difference in means as large as or even larger than the one observed is 0.88. Thus it is quite likely that the difference in means we observe has happened by chance. The p-value is 0.00001. Given that the null hypothesis is that the means for the two groups are the same, the probability of observing a difference in means as large as or even larger than the one observed is 0.00001. We can reasonably conclude that the difference in means we observe is unlikely to be due to chance (formally speaking, we can reject the null hypothesis based on the statistical evidence). If the two groups of data have larger standard deviations, it is more likely that the observed difference in means is due to chance. Even when the difference in means is large, as in the case shown in Figure 2.10 (Figure 2.7 in the R version), the large spread indicates that the difference is likely to be due to chance. Therefore it is important to consider the size of the difference relative to the standard deviation. The same group of people did both experiments, so they have the same characteristics other than the punishment options. This means that if the groups differ in contributions, it is likely to be due to the difference in punishment options. Controlling for characteristics allows us to isolate the effect of the punishment option. There is one other potential explanation, which is that all groups did the non-punishment game first and the punishment game after that. So the differences could have been the result of some learning process. This explanation, however, seems unlikely, as both experiments start with basically identical contribution means in Period 1. On the whole, we are therefore confident that the differences in period 10 are due to the punishment treatment. In addition to the punishment options available, the contributions in periods after Period 1 can be affected by the strategic behaviour of the subjects in response to observation of past actions taken by other subjects in the group. This effect is not present in Period 1, so the difference in contributions is therefore more likely to reflect the effects of punishment options alone. There can be systematic differences between the lab and the outside world that influence human behaviour. The environment of the lab experiment has unique features that are not present in the real world. The experience of being a subject, the subjects’ awareness that they are being monitored, the power of the experimenter, and the significance of the situation can all cause subjects’ behaviour to be different from behaviour in the real world. As a result, we need to be cautious when extrapolating lab findings to the outside world. There are many limitations to discuss, including the following examples. An individual’s behaviour in a situation can be affected by the degree of anonymity. Differing degrees of anonymity between an experiment scenario and its real-world counterpart reduce the generalizability of lab findings. For example, subjects in the experiment may be unrelated to each other and forbidden from communicating with each other. In the real world, however, people can be related through family, work, and friendship, and are usually able to communicate with each other. For the experimental situation to be representative of its real-world counterpart, it is therefore important to ensure that the degree of anonymity is similar to that of the real world. Lab experiments aim to isolate the effects of changes in one variable by controlling all other variables. Human behaviour is dependent on a large number of variables, many of which are not even measurable. Examples of such variables include past experiences, social norms, and ability. It is difficult, if not impossible, for social scientists to completely control for all these variables. Because of this, social scientists may fail to obtain the ceteris paribus effects of interest. Social scientists should explore new ways to measure these variables and control for them, especially if they affect the relationships being studied. Behaviour can be dependent on the size of the stake involved. Stakes in the real world are typically higher than in experiments. The lab findings may therefore reveal little about real-world behaviour. Individuals with certain characteristics tend to select themselves into experiments, resulting in samples that are not representative of the population. For example, college students interested in the research and seeking additional income are more likely to become experimental subjects. If these characteristics affect the relationships of interest, then the results are biased. The sample selection problem means lab findings cannot be generalized to the outside world. Researchers should try random sampling of the population rather than relying on volunteers. Econometric methodologies that mitigate the effects of sample selection should be adopted. Subjects in the typical experiment face limited and well-specified instructions and choices. In the real world, the choice set can be infinitely large and vaguely defined. Individuals may even be able to influence rules in the real world. Lab experiments usually last no longer than a few hours, whereas in the real world, many decisions are made over long periods of time. Individuals may behave differently as the time horizon expands. In general, in experimental social science studies involving human subjects, it is difficult to ensure perfectly that the sample is representative of the population of interest and that the situation in the experiment is representative of its real-world counterpart. These difficulties limit the generalizability of lab findings to the real world. To alleviate these problems, researchers can use random sampling and design the experiment to be as realistic as possible. Models of laboratory behaviour can be used to anticipate issues, inspiring adjustments to be made to the study. Designs that recognize the weaknesses of the lab can be adopted. Researchers can also use econometric methodologies that mitigate the problems and extract valuable information from imperfect data."
});
index.addDoc({
    id: 17,
    title: "Doing Economics: Empirical Project 3: Measuring the effect of a sugar tax",
    content: "Empirical Project 3 Measuring the effect of a sugar tax Learning objectives In this project you will: use the differences-in-differences method to measure the effects of a policy or program, and explain how this method works use line and column charts to visualize and compare multiple variables create summary tables to describe the data interpret the p-value in the context of a policy or program evaluation. Key concepts Concepts needed for this project: mean, frequency table, and p-value. Concepts introduced in this project: natural experiment, differences-in-differences, and conditional mean. Introduction CORE projects This empirical project is related to material in: Unit 3 of Economy, Society, and Public Policy Unit 22 of The Economy. In Empirical Project 1, we mentioned that natural experiments can help us determine whether one variable causes another variable. A useful application of natural experiments is assessing the effects of a policy. To do so, we compare the outcomes of two groups, both before and after the policy took effect: The treatment group: those who were affected by the policy The control group: those who were not affected by the policy. natural experimentAn empirical study exploiting naturally occurring statistical controls in which researchers do not have the ability to assign participants to treatment and control groups, as is the case in conventional experiments. Instead, differences in law, policy, weather, or other events can offer the opportunity to analyse populations as if they had been part of an experiment. The validity of such studies depends on the premise that the assignment of subjects to the naturally occurring treatment and control groups can be plausibly argued to be random. Specifically, we take the difference in outcomes of the treatment and control group before the policy was implemented, and compare it with the difference in outcomes after the policy was implemented. This method is known in economics as differences-in-differences. We need to compare outcomes before the policy has happened, because in a natural experiment we cannot choose exactly who receives the treatment (whereas in the lab we could randomly assign the treatment). Since the two groups are not randomly chosen, we need to account for any pre-existing differences between the two groups that could affect the outcomes, for example differences in age (for people) or characteristics (for products). If these other factors remain constant over the period considered, then we can reasonably conclude that any observed changes in the outcome differences between the groups are due to the policy. Natural experiments therefore allow us to make causal statements about policies and outcomes. differences-in-differencesA method that applies an experimental research design to outcomes observed in a natural experiment. It involves comparing the difference in the average outcomes of two groups, a treatment and control group, both before and after the treatment took place. We will use the 2014 sugar tax in the US to learn how before-and-after comparisons are done in practice. Sugar-sweetened beverages (SSBs) are considered unhealthy because of their link to conditions such as diabetes and obesity. In November 2014, the city of Berkeley in California became the first US jurisdiction to implement a tax on SSB distributors, with the aim of discouraging SSB consumption. The tax of one cent per fluid ounce meant that if retailers raised their prices to exactly counter the effects of the tax, a $1 can of soda (12 oz) would now cost $1.12. But did sellers actually respond this way? And what effects did the tax have on shoppers’ expenditure on sugary beverages? A group of researchers did a differences-in-differences study of the effects of this SSB tax, which you can read about in a Forbes article. Figure 3.1 summarizes the timeline of the tax, and the data that they collected and published in a 2017 PLoS Medicine journal paper. We will make before-and-after comparisons using the data they collected, in order to learn about the effects of the sugar tax. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-01.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-01-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-01-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-01-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-01.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Berkeley sugar-sweetened beverages tax implementation and evaluation timeline. '' /> Berkeley sugar-sweetened beverages tax implementation and evaluation timeline. Figure 3.1 Berkeley sugar-sweetened beverages tax implementation and evaluation timeline. Lynn D. Silver, Shu Wen Ng, Suzanne Ryan-Ibarra, Lindsey Smith Taillie, Marta Induni, Donna R. Miles, Jennifer M. Poti, and Barry M. Popkin. 2017. Figure 1 in ‘Changes in prices, sales, consumer spending, and beverage consumption one year after a tax on sugar-sweetened beverages in Berkeley, California, US: A before-and-after study’. PLoS Med 14 (4): e1002283. Working in Excel Working in R Working in Google Sheets"
});
index.addDoc({
    id: 18,
    title: "Doing Economics: Empirical Project 3: Working in Excel",
    content: "Empirical Project 3 Working in Excel Excel-specific learning objectives In addition to the learning objectives for this project, in this section you will learn how to create summary tables using Excel’s PivotTable option. Part 3.1 Before-and-after comparisons of retail prices We will first look at price data from the treatment group (stores in Berkeley) to see what happened to the price of sugary and non-sugary beverages after the tax. Download the data from the Global Food Research Program’s website, and select the ‘Berkeley Store Price Survey’ Excel dataset. The first tab of the Excel file contains the data dictionary. Make sure you read the data description column carefully, and check that each variable is in the Data tab. Read ‘S1 Text’, from the journal paper’s supporting information, which explains how the Store Price Survey data was collected. In your own words, explain how the product information was recorded, and the measures that researchers took to ensure that the data was accurate and representative of the treatment group. What were some of the data collection issues that they encountered? Instead of using the name of the store, each store was given a unique ID number (recorded as store_id on the spreadsheet). Using Excel’s filter function, verify that the number of stores in the dataset is the same as that stated in the ‘S1 Text’ (26). Similarly, each product was given a unique ID number (product_id). How many different products are in the dataset? Following the approach described in ‘S1 Text’, we will compare the variable price per ounce in US$ cents (price_per_oz_c). We will look at what happened to prices in the two treatment groups before the tax (time = DEC2014) and after the tax (time = JUN2015): treatment group one: large supermarkets (store_type = 1) treatment group two: pharmacies (store_type = 3). Before doing this analysis, we will use summary measures to see how many observations are in the treatment and control group, and how the two groups differ across some variables of interest. For example, if there are very few observations in a group, we might be concerned about the precision of our estimates and will need to interpret our results in light of this fact. Instead of calculating summary measures one by one (as we did in Empirical Project 2), we will use Excel’s PivotTable option to make frequency tables containing the summary measures that we are interested in. The tables should be in a different tab to the data (either all in the same tab, or in separate tabs). Use Excel’s PivotTable option to create the following tables: A frequency table showing the number (count) of store observations (store type) in December 2014 and June 2015, with ‘store type’ as the row variable and ‘time period’ as the column variable. For each store type, is the number of observations similar in each time period? A frequency table showing the number of taxed and non-taxed beverages in December 2014 and June 2015, with ‘store type’ as the row variable and ‘taxed’ as the column variable. (‘Taxed’ equals 1 if the sugar tax applied to that product, and 0 if the tax did not apply). For each store type, is the number of taxed and non-taxed beverages similar? A frequency table showing the number of each product type (type), with ‘product type’ as the row variable and ‘time period’ as the column variables. Which product types have the highest number of observations and which have the lowest number of observations? Why might some products have more observations than others? Excel walk-through 3.1 Making a frequency table using Excel’s PivotTable option Follow the walk-through in the CORE video, or in Figure 3.2, to find out how to make a pivot table in Excel. How to make a pivot table <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to make a frequency table using Excel’s PivotTable option. '' /> Figure 3.2 How to make a frequency table using Excel’s PivotTable option. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : The data will look like this. We will be making a pivot table using Column C (store type) and Column K (time). It will show how many observations of each store type there are in 2 time periods (DEC2014 and JUN2015). '' /> The data The data will look like this. We will be making a pivot table using Column C (store type) and Column K (time). It will show how many observations of each store type there are in 2 time periods (DEC2014 and JUN2015). Figure 3.2a The data will look like this. We will be making a pivot table using Column C (store type) and Column K (time). It will show how many observations of each store type there are in 2 time periods (DEC2014 and JUN2015). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Insert a blank pivot table : We will make and store the frequency table in a new tab in the spreadsheet. '' /> Insert a blank pivot table We will make and store the frequency table in a new tab in the spreadsheet. Figure 3.2b We will make and store the frequency table in a new tab in the spreadsheet. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Insert a blank pivot table : Now we have to tell Excel which data to use to make the table. '' /> Insert a blank pivot table Now we have to tell Excel which data to use to make the table. Figure 3.2c Now we have to tell Excel which data to use to make the table. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Insert a blank pivot table : The pivot table is currently blank. In order to create the table, we will use the variables listed in the menu to fill in the empty boxes shown at the bottom. '' /> Insert a blank pivot table The pivot table is currently blank. In order to create the table, we will use the variables listed in the menu to fill in the empty boxes shown at the bottom. Figure 3.2d The pivot table is currently blank. In order to create the table, we will use the variables listed in the menu to fill in the empty boxes shown at the bottom. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Choose the variables to put in the pivot table : Excel will create a table that looks like the one above. By default it will use all the values of the variables selected (e.g. all time periods). We can remove unnecessary time periods and blank cells by filtering the data. '' /> Choose the variables to put in the pivot table Excel will create a table that looks like the one above. By default it will use all the values of the variables selected (e.g. all time periods). We can remove unnecessary time periods and blank cells by filtering the data. Figure 3.2e Excel will create a table that looks like the one above. By default it will use all the values of the variables selected (e.g. all time periods). We can remove unnecessary time periods and blank cells by filtering the data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-02-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the values of each variable : After completing step 9, you will have the required table. '' /> Filter the values of each variable After completing step 9, you will have the required table. Figure 3.2f After completing step 9, you will have the required table. conditional meanAn average of a variable, taken over a subgroup of observations that satisfy certain conditions, rather than all observations. Besides counting the number of observations in a particular group, we can also use the PivotTable option to calculate the mean by only using observations that satisfy certain conditions (known as the conditional mean). In this case, we are interested in comparing the mean price of taxed and non-taxed beverages, before and after the tax. Calculate and compare conditional means: Create a table similar to Figure 3.3, showing the average price per ounce (in cents) for taxed and non-taxed beverages separately, with ‘store type’ as the row variable, and ‘taxed’ and ‘time’ as the column variables. To follow the methodology used in the journal paper, make sure to only include products that are present in all time periods, and non-supplementary products (supp = 0). Without doing any calculations, summarize any differences or general patterns between December 2014 and June 2015 that you find in the table. Would we be able to assess the effect of sugar taxes on product prices by comparing the average price of non-taxed goods with that of taxed goods in any given period? Why or why not? Non-taxed Taxed Store type Dec 2014 Jun 2015 Dec 2014 Jun 2015 1 3 The average price of taxed and non-taxed beverages, according to time period and store type. Figure 3.3 The average price of taxed and non-taxed beverages, according to time period and store type. Excel walk-through 3.2 Making a pivot table with more than two variables <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to make a pivot table with more than two variables. '' /> Figure 3.4 How to make a pivot table with more than two variables. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : The data will look like this. We will be making a pivot table using Columns C (store type), Column K (time), and Column I (taxed). It will show the average price of taxed and non-taxed beverages in DEC2014 and JUN2015, according to store type. '' /> The data The data will look like this. We will be making a pivot table using Columns C (store type), Column K (time), and Column I (taxed). It will show the average price of taxed and non-taxed beverages in DEC2014 and JUN2015, according to store type. Figure 3.4a The data will look like this. We will be making a pivot table using Columns C (store type), Column K (time), and Column I (taxed). It will show the average price of taxed and non-taxed beverages in DEC2014 and JUN2015, according to store type. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Count the number of times each product appears in the dataset : We only want to look at products that were present in all time periods, to ensure we are comparing the same group of products over time. We will create a new variable (called ‘Number’, shown in Column M) that shows how many periods of data are available for each product. Excel’s COUNTIFS function will help us count the number of observations that satisfy certain conditions. '' /> Count the number of times each product appears in the dataset We only want to look at products that were present in all time periods, to ensure we are comparing the same group of products over time. We will create a new variable (called ‘Number’, shown in Column M) that shows how many periods of data are available for each product. Excel’s COUNTIFS function will help us count the number of observations that satisfy certain conditions. Figure 3.4b We only want to look at products that were present in all time periods, to ensure we are comparing the same group of products over time. We will create a new variable (called ‘Number’, shown in Column M) that shows how many periods of data are available for each product. Excel’s COUNTIFS function will help us count the number of observations that satisfy certain conditions. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Insert a blank pivot table : We will make and store the frequency table in a new tab in the spreadsheet. '' /> Insert a blank pivot table We will make and store the frequency table in a new tab in the spreadsheet. Figure 3.4c We will make and store the frequency table in a new tab in the spreadsheet. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Insert a blank pivot table : Now we have to tell Excel which data to use to make the table. '' /> Insert a blank pivot table Now we have to tell Excel which data to use to make the table. Figure 3.4d Now we have to tell Excel which data to use to make the table. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Fill in the pivot table : After step 9, Excel will create a table that looks like the one above. By default, it uses all the available data and shows frequencies instead of means. '' /> Fill in the pivot table After step 9, Excel will create a table that looks like the one above. By default, it uses all the available data and shows frequencies instead of means. Figure 3.4e After step 9, Excel will create a table that looks like the one above. By default, it uses all the available data and shows frequencies instead of means. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the values of each variable : We will filter the data so that only the store types we want (1 and 3) are visible. '' /> Filter the values of each variable We will filter the data so that only the store types we want (1 and 3) are visible. Figure 3.4f We will filter the data so that only the store types we want (1 and 3) are visible. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the values of each variable : We will filter the data so that only the time periods we want (DEC2014 and JUN2015) are visible. '' /> Filter the values of each variable We will filter the data so that only the time periods we want (DEC2014 and JUN2015) are visible. Figure 3.4g We will filter the data so that only the time periods we want (DEC2014 and JUN2015) are visible. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-h.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-h-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-h-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-h-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-h.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the values inside the table : Now we tell Excel to only include products that are available in all time periods (Number = 3), and non-supplementary products (supp = 0). '' /> Filter the values inside the table Now we tell Excel to only include products that are available in all time periods (Number = 3), and non-supplementary products (supp = 0). Figure 3.4h Now we tell Excel to only include products that are available in all time periods (Number = 3), and non-supplementary products (supp = 0). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-i.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-i-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-i-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-i-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-i.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the values inside the table : Your table should now look like the one shown above. Notice that there are fewer observations (216 compared to 575) because now we are looking at a subgroup of the data. '' /> Filter the values inside the table Your table should now look like the one shown above. Notice that there are fewer observations (216 compared to 575) because now we are looking at a subgroup of the data. Figure 3.4i Your table should now look like the one shown above. Notice that there are fewer observations (216 compared to 575) because now we are looking at a subgroup of the data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-j.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-j-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-j-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-j-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-j.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the values inside the table to means : Now we change the values shown in the table from frequencies to means. '' /> Change the values inside the table to means Now we change the values shown in the table from frequencies to means. Figure 3.4j Now we change the values shown in the table from frequencies to means. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-k.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-k-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-k-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-k-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-k.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the values inside the table to means : In the box that pops up, you can see the different measures that the pivot table can show (for example, the sum, max, or min). We would like the cells to show averages. '' /> Change the values inside the table to means In the box that pops up, you can see the different measures that the pivot table can show (for example, the sum, max, or min). We would like the cells to show averages. Figure 3.4k In the box that pops up, you can see the different measures that the pivot table can show (for example, the sum, max, or min). We would like the cells to show averages. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-l.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-l-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-l-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-l-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-l.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Remove the ‘Grand total’ rows and columns : By default, the pivot table will have a column showing the mean for each row (for example, the mean for store type 1 in both DEC2014 and JUN2015). We will remove these columns to make our table easier to read. '' /> Remove the ‘Grand total’ rows and columns By default, the pivot table will have a column showing the mean for each row (for example, the mean for store type 1 in both DEC2014 and JUN2015). We will remove these columns to make our table easier to read. Figure 3.4l By default, the pivot table will have a column showing the mean for each row (for example, the mean for store type 1 in both DEC2014 and JUN2015). We will remove these columns to make our table easier to read. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-m.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-m-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-m-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-m-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-m.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Remove the ‘Grand total’ rows and columns : After step 22, you can see that some columns and rows are removed from your table. '' /> Remove the ‘Grand total’ rows and columns After step 22, you can see that some columns and rows are removed from your table. Figure 3.4m After step 22, you can see that some columns and rows are removed from your table. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-n.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-n-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-n-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-n-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-04-n.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Round cell values to two decimal places : To make the table even easier to read, we will round the cell values to two decimal places. '' /> Round cell values to two decimal places To make the table even easier to read, we will round the cell values to two decimal places. Figure 3.4n To make the table even easier to read, we will round the cell values to two decimal places. In order to make a before-and-after comparison, we will make a chart similar to Figure 2 in the journal paper, to show the change in prices for each store type. Using your table from Question 3: Calculate the change in the mean price after the tax (price in June 2015 minus price in December 2014) for taxed and non-taxed beverages, by store type. Using the values you calculated in Question 4(a), plot a column chart to show this information (as done in Figure 2 of the journal paper) with store type on the horizontal axis and price change on the vertical axis. Label each axis and data series appropriately. You should get the same values as shown in Figure 2. Excel walk-through 3.3 Making a column chart to compare two groups <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to make a column chart to compare two groups. '' /> Figure 3.5 How to make a column chart to compare two groups. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create a table showing differences in means : We will be using the pivot table above to create a column chart. (See Excel walk-through 3.2 for help on how to create the pivot table.) First, we will calculate the difference in mean price for each type of good and store type. '' /> Create a table showing differences in means We will be using the pivot table above to create a column chart. (See Excel walk-through 3.2 for help on how to create the pivot table.) First, we will calculate the difference in mean price for each type of good and store type. Figure 3.5a We will be using the pivot table above to create a column chart. (See Excel walk-through 3.2 for help on how to create the pivot table.) First, we will calculate the difference in mean price for each type of good and store type. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create a table showing differences in means : Fill in the table by using the cell formula to calculate the differences required. After step 2, your table will look like the one shown above. '' /> Create a table showing differences in means Fill in the table by using the cell formula to calculate the differences required. After step 2, your table will look like the one shown above. Figure 3.5b Fill in the table by using the cell formula to calculate the differences required. After step 2, your table will look like the one shown above. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a column chart : After step 6, the column chart will look like the one shown above. Notice that Excel has put the columns for taxed and non-taxed products in separate groups, but we want the columns to be grouped according to store type. '' /> Draw a column chart After step 6, the column chart will look like the one shown above. Notice that Excel has put the columns for taxed and non-taxed products in separate groups, but we want the columns to be grouped according to store type. Figure 3.5c After step 6, the column chart will look like the one shown above. Notice that Excel has put the columns for taxed and non-taxed products in separate groups, but we want the columns to be grouped according to store type. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Switch the row and column variable; change the legend and horizontal axis labels : After step 8, the columns will now be grouped according to store type. The next step is to change the labels in the legend, so that ‘Series1’ is renamed as ‘Taxed’ and ‘Series2’ is renamed as ‘Non-taxed’. '' /> Switch the row and column variable; change the legend and horizontal axis labels After step 8, the columns will now be grouped according to store type. The next step is to change the labels in the legend, so that ‘Series1’ is renamed as ‘Taxed’ and ‘Series2’ is renamed as ‘Non-taxed’. Figure 3.5d After step 8, the columns will now be grouped according to store type. The next step is to change the labels in the legend, so that ‘Series1’ is renamed as ‘Taxed’ and ‘Series2’ is renamed as ‘Non-taxed’. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Switch the row and column variable; change the legend and horizontal axis labels : After step 10, ‘Series1’ will be renamed as ‘Taxed’, and this will also show up in the chart legend. '' /> Switch the row and column variable; change the legend and horizontal axis labels After step 10, ‘Series1’ will be renamed as ‘Taxed’, and this will also show up in the chart legend. Figure 3.5e After step 10, ‘Series1’ will be renamed as ‘Taxed’, and this will also show up in the chart legend. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Switch the row and column variable; change the legend and horizontal axis labels : The next step is to change the horizontal axis labels from numbers to store names (‘1’ is ‘Large supermarkets’, and ‘2’ is ‘Pharmacies’). '' /> Switch the row and column variable; change the legend and horizontal axis labels The next step is to change the horizontal axis labels from numbers to store names (‘1’ is ‘Large supermarkets’, and ‘2’ is ‘Pharmacies’). Figure 3.5f The next step is to change the horizontal axis labels from numbers to store names (‘1’ is ‘Large supermarkets’, and ‘2’ is ‘Pharmacies’). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Switch the row and column variable; change the legend and horizontal axis labels : After step 13, you can see that the horizontal axis labels are store types. '' /> Switch the row and column variable; change the legend and horizontal axis labels After step 13, you can see that the horizontal axis labels are store types. Figure 3.5g After step 13, you can see that the horizontal axis labels are store types. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-h.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-h-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-h-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-h-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-03-05-h.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add axis titles and a chart title : After step 17, your chart will look like the bottom chart of Figure 2 in the journal paper. '' /> Add axis titles and a chart title After step 17, your chart will look like the bottom chart of Figure 2 in the journal paper. Figure 3.5h After step 17, your chart will look like the bottom chart of Figure 2 in the journal paper. statistically significantWhen a relationship between two or more variables is unlikely to be due to chance, given the assumptions made about the variables (for example, having the same mean). Statistical significance does not tell us whether there is a causal link between the variables. To assess whether the difference in mean prices before and after the tax could have happened by chance due to the samples chosen (and there are no differences in the population means), we could calculate the p-value. (Here, ‘population means’ refer to the mean prices before/after the tax that we would calculate if we had all prices for all stores in Berkeley.) The authors of the journal article calculate p-values, and use the idea of statistical significance to interpret them. Whenever they get a p-value of less than 5%, they conclude that the assumption of no differences in the population is unlikely to be true: they say that the price difference is statistically significant. If they get a p-value higher than 5%, they say that the difference is not statistically significant, meaning that they think it could be due to chance variation in prices. Using a particular cutoff level for the p-value, and concluding that a result is only statistically significant if the p-value is below the cutoff, is common in statistical studies, and 5% is often used as the cutoff level. But this approach has been criticized recently by statisticians and social scientists because the cutoff level is quite arbitrary. Instead of using a cutoff, we prefer to calculate p-values and use them to assess the strength of the evidence against our assumption that there are no differences in the population means. Whether the statistical evidence is strong enough for us to draw a conclusion about a policy, such as a sugar tax, will always be a matter of judgement. According to the journal paper, the p-value is 0.02 for large supermarkets, and 0.99 for pharmacies. Based on these p-values and your chart from Question 4, what can you conclude about the difference in means? (You may find the discussion in Part 2.3 helpful.) Part 3.2 Before-and-after comparisons with prices in other areas When looking for any price patterns, it is possible that the observed changes in Berkeley were not solely due to the tax, but instead were also influenced by other events that happened in Berkeley and in neighbouring areas. To investigate whether this is the case, the researchers conducted another differences-in-differences analysis, using a different treatment and control group: The treatment group: Beverages in Berkeley The control group: Beverages in surrounding areas. The researchers collected price data from stores in the surrounding areas and compared them with prices in Berkeley. If prices changed in a similar way in nearby areas (which were not subject to the tax), then what we observed in Berkeley may not be primarily a result of the tax. We will be using the data the researchers collected to make our own comparisons. Download the following files: The Excel file containing the price data they collected, including information on the date (year and month), location (Berkeley or Non-Berkeley), beverage group (soda, fruit drinks, milk substitutes, milk, and water), and average price for that month. ‘S5 Table’ comparing the neighbourhood characteristics of the Berkeley and non-Berkeley stores. Based on ‘S5 Table’, do you think the researchers chose suitable comparison stores? Why or why not? We will now plot a line chart similar to Figure 3 in the journal paper, to compare prices of similar goods in different locations and see how they have changed over time. To do this, we will need to summarize the data using Excel’s PivotTable option, so that there is one value (the mean price) for each location and type of good in each month. Assess the effects of a tax on prices: Create a table similar to Figure 3.6 to show the average price in each month for taxed and non-taxed beverages, according to location. Use ‘year and month’ as the row variables, and ‘tax’ and ‘location’ as the column variables. (You may find Excel walk-through 3.2 helpful.) Plot the four columns of your table on the same line chart, with average price on the vertical axis and time (months) on the horizontal axis. Describe any differences you see between the prices of non-taxed goods in Berkeley and those outside Berkeley, both before the tax (January 2013 to December 2014) and after the tax (March 2015 onwards). Do the same for prices of taxed goods. Based on your chart, is it reasonable to conclude that the sugar tax had an effect on prices? Non-taxed Taxed Year/Month Berkeley Non-Berkeley Berkeley Non-Berkeley January 2013 February 2013 March 2013 … December 2013 January 2014 … February 2016 The average price of taxed and non-taxed beverages, according to location and month. Figure 3.6 The average price of taxed and non-taxed beverages, according to location and month. How strong is the evidence that the sugar tax affected prices? According to the journal paper, when comparing the mean Berkeley and non-Berkeley price of sugary beverages after the tax, the p-value is smaller than 0.00001, and it is 0.63 for non-sugary beverages after the tax. What do the p-values tell us about the difference in means and the effect of the sugar tax on the price of sugary beverages? (You may find the discussion in Part 2.3 helpful.) The aim of the sugar tax was to decrease consumption of sugary beverages. Figure 3.7 shows the mean number of calories consumed and the mean volume consumed before and after the tax. The researchers reported the p-values for the difference in means before and after the tax in the last column. Usual intake Pre-tax (Nov–Dec 2014), n = 623 Post-tax (Nov–Dec 2015), n = 613 Pre-tax–post-tax difference Caloric intake (kilocalories/capita/day) Taxed beverages 45.1 38.7 −6.4, p = 0.56 Non-taxed beverages 115.7 147.6 31.9, p = 0.04 Volume of intake (grams/capita/day) Taxed beverages 121.0 97.0 −24.0, p = 0.24 Non-taxed beverages 1,839.4 1,896.5 57.1, p = 0.22 Models account for age, gender, race/ethnicity, income level, and educational attainment. n is the sample size at each round of the survey after excluding participants with missing values on self-reported race/ethnicity, age, education, income, or monthly intake of sugar-sweetened beverages. Changes in prices, sales, consumer spending, and beverage consumption one year after a tax on sugar-sweetened beverages in Berkeley, California, US: A before-and-after study. Figure 3.7 Changes in prices, sales, consumer spending, and beverage consumption one year after a tax on sugar-sweetened beverages in Berkeley, California, US: A before-and-after study. Lynn D. Silver, Shu Wen Ng, Suzanne Ryan-Ibarra, Lindsey Smith Taillie, Marta Induni, Donna R. Miles, Jennifer M. Poti, and Barry M. Popkin. 2017. Table 1 in ‘Changes in prices, sales, consumer spending, and beverage consumption one year after a tax on sugar-sweetened beverages in Berkeley, California, US: A before-and-after study’. PLoS Med 14 (4): e1002283. Based on Figure 3.7, what can you say about consumption behaviour in Berkeley after the tax? Suggest some explanations for the evidence. Read the ‘Limitations’ in the ‘Discussions’ section of the paper and discuss the limitations of this study. How could future studies on the sugar tax in Berkeley address these problems? (Some issues you may want to discuss are: the number of stores observed, number of people surveyed, and the reliability of the price data collected.) Suppose that you have the authority to conduct your own sugar tax natural experiment in two neighbouring towns, Town A and Town B. Outline how you would conduct the experiment to ensure that any changes in outcomes (prices, consumption of sugary beverages) are due to the tax and not due to other factors. (Hint: think about what factors you need to hold constant.)"
});
index.addDoc({
    id: 19,
    title: "Doing Economics: Empirical Project 3: Working in R",
    content: "Empirical Project 3 Working in R Download the code To download the code chunks used in this project, right-click on the download link and select ‘Save Link As…’. You’ll need to save the code download to your working directory, and open it in RStudio. Don’t forget to also download the data into your working directory by following the steps in this project. R-specific learning objectives In addition to the learning objectives for this project, in this section you will learn how to use the piping technique to run a sequence of functions. Getting started in R For this project you will need the following packages: tidyverse, to help with data manipulation readxl, to import an Excel spreadsheet mosaic, to help create frequency tables readstata13, to read in a Stata datafile. You will also use the ggplot2 package to produce accurate graphs, but that comes as part of the tidyverse package. If you need to install any of these packages, run the following code: install.packages(c(''readxl'', ''tidyverse'', ''mosaic'', ''readstata13'')) You can import these libraries now, or when they are used in the R walk-throughs below. library(readxl) library(tidyverse) library(mosaic) library(readstata13) Part 3.1 Before-and-after comparisons of retail prices We will first look at price data from the treatment group (stores in Berkeley) to see what happened to the price of sugary and non-sugary beverages after the tax. Download the data from the Global Food Research Program’s website, and select the ‘Berkeley Store Price Survey’ Excel dataset. Then upload the dataset into R. The first tab of the Excel file contains the data dictionary. Make sure you read the data description column carefully, and check that each variable is in the Data tab. R walk-through 3.1 Importing the datafile into R The data is in .xlsx format, so we use the readxl package to import it. We also load the tidyverse library as this includes packages that we will use later for drawing charts and making code easier to read. If you open the data in Excel, you will see that there are two tabs: ‘Data Dictionary’, which contains some information about the variables, and ‘Data’, which contains the actual data. Let’s import both into R, so that do not need to refer back to Excel for the additional information about variables. library(readxl) library(tidyverse) # Set your working directory to the correct folder. # Insert your file path for 'YOURFILEPATH'. setwd(''YOURFILEPATH'') var_info <- read_excel(''sps_public.xlsx'', sheet = ''Data Dictionary'') dat <- read_excel(''sps_public.xlsx'', sheet = ''Data'') Let’s use the str function to check that the variables were classified correctly. str(dat) ## Classes 'tbl_df', 'tbl' and 'data.frame': 2175 obs. of 12 variables: ## $ store_id : num 16 16 16 16 16 16 16 16 16 16 ... ## $ type : chr ''WATER'' ''TEA'' ''TEA'' ''WATER'' ... ## $ store_type : num 2 2 2 2 2 2 2 2 2 2 ... ## $ type2 : chr NA NA NA NA ... ## $ size : num 33.8 23 23 33.8 128 64 128 64 63.9 144 ... ## $ price : num 1.69 0.99 0.99 1.69 3.79 2.79 3.79 2.79 4.59 5.99 ... ## $ price_per_oz : num 0.05 0.043 0.043 0.05 0.0296 ... ## $ price_per_oz_c: num 5 4.3 4.3 5 2.96 ... ## $ taxed : num 0 1 1 0 0 0 0 0 0 1 ... ## $ supp : num 0 0 0 0 0 0 0 0 0 1 ... ## $ time : chr ''DEC2014'' ''DEC2014'' ''DEC2014'' ''DEC2014'' ... ## $ product_id : num 29 32 33 38 40 41 42 43 44 50 ... R classified all the variables containing numbers as numerical (num). However, for some of these variables (specifically, type, taxed, supp, store_id, store_type, type2 and product_id), the numbers actually represent categories (known as ‘factors’ in R). So let’s use the factor function to convert the variables to factor variables. dat$type <- factor(dat$type) dat$taxed <- factor(dat$taxed, labels = c(''not taxed'', ''taxed'')) dat$supp <- factor(dat$supp, labels = c(''Standard'', ''Supplemental'')) dat$store_id <- factor(dat$store_id) dat$store_type <- factor(dat$store_type, labels = c(''Large Supermarket'', ''Small Supermarket'', ''Pharmacy'', ''Gas Station'')) dat$type2 <- factor(dat$type2) dat$product_id <- factor(dat$product_id) You can see that we used the labels option to specify the names of different categories (where they are clearly defined). There is another variable, time, which is classified as a chr variable (chr stands for ‘characters’, meaning letters and numbers), but should be a factor variable. Before we do this, we use the unique command to check the categories of this variable. unique(dat$time) ## [1] ''DEC2014'' ''JUN2015'' ''MAR2015'' If you look at the timeline in the Silver et al. (2017) paper, you will notice that the third price survey was in March 2016, not in March 2015, so the data has been labelled incorrectly. We shall therefore change all the values MAR2015 to MAR2016. # Selects all observations with “time” equal to ''MAR2015''. dat$time[dat$time == ''MAR2015''] <- ''MAR2016'' We can now change time into a factor variable. dat$time <- factor(dat$time) Read ‘S1 Text’, from the journal paper’s supporting information, which explains how the Store Price Survey data was collected. In your own words, explain how the product information was recorded, and the measures that researchers took to ensure that the data was accurate and representative of the treatment group. What were some of the data collection issues that they encountered? Instead of using the name of the store, each store was given a unique ID number (recorded as store_id on the spreadsheet). Verify that the number of stores in the dataset is the same as that stated in the ‘S1 Text’ (26). Similarly, each product was given a unique ID number (product_id). How many different products are in the dataset? R walk-through 3.2 Counting the number of unique elements in a variable We use two functions here: unique to obtain a list of the unique elements of the variables of interest (dat$store_id and dat$product_id), then length to count how long the list is. We will create two variables, no_stores and no_products, that contain the number of stores and products respectively. no_stores <- length(unique(dat$store_id)) no_products <- length(unique(dat$product_id)) paste(''Stores:'', no_stores) ## [1] ''Stores: 26'' paste(''Products:'', no_products) ## [1] ''Products: 247'' Following the approach described in ‘S1 Text’, we will compare the variable price per ounce in US$ cents (price_per_oz_c). We will look at what happened to prices in the two treatment groups before the tax (time = DEC2014) and after the tax (time = JUN2015): treatment group one: large supermarkets (store_type = 1) treatment group two: pharmacies (store_type = 3). Before doing this analysis, we will use summary measures to see how many observations are in the treatment and control group, and how the two groups differ across some variables of interest. For example, if there are very few observations in a group, we might be concerned about the precision of our estimates and will need to interpret our results in light of this fact. We will now create frequency tables containing the summary measures that we are interested in. Create the following tables: A frequency table showing the number (count) of store observations (store type) in December 2014 and June 2015, with ‘store type’ as the row variable and ‘time period’ as the column variable. For each store type, is the number of observations similar in each time period? A frequency table showing the number of taxed and non-taxed beverages in December 2014 and June 2015, with ‘store type’ as the row variable and ‘taxed’ as the column variable. (‘Taxed’ equals 1 if the sugar tax applied to that product, and 0 if the tax did not apply). For each store type, is the number of taxed and non-taxed beverages similar? A frequency table showing the number of each product type (type), with ‘product type’ as the row variable and ‘time period’ as the column variable. Which product types have the highest number of observations and which have the lowest number of observations? Why might some products have more observations than others? R walk-through 3.3 Creating frequency tables Frequency table for store type and time period We use the tally function, which allows us to produce frequency tables using the format = ''count'' option. We start with the frequency table that shows the number of stores of different types in each time period. library(mosaic) tally(~store_type + time, data = dat, margins = TRUE, format = ''count'') ## time ## store_type DEC2014 JUN2015 MAR2016 Total ## Large Supermarket 177 209 158 544 ## Small Supermarket 407 391 327 1125 ## Pharmacy 87 102 73 262 ## Gas Station 73 96 75 244 ## Total 744 798 633 2175 There are fewer observations taken from gas stations and pharmacies and more from small supermarkets. Frequency table for store type and taxed Now we repeat the steps above to make a frequency table with store_type as the row variable and taxed as the column variable. We add +time to the code because we also want separate values for each time period. tally(~store_type + taxed + time, data = dat, margins = TRUE, format = ''count'') ## , , time = DEC2014 ## ## taxed ## store_type not taxed taxed Total ## Large Supermarket 92 85 177 ## Small Supermarket 196 211 407 ## Pharmacy 44 43 87 ## Gas Station 34 39 73 ## Total 366 378 744 ## ## , , time = JUN2015 ## ## taxed ## store_type not taxed taxed Total ## Large Supermarket 111 98 209 ## Small Supermarket 192 199 391 ## Pharmacy 52 50 102 ## Gas Station 44 52 96 ## Total 399 399 798 ## ## , , time = MAR2016 ## ## taxed ## store_type not taxed taxed Total ## Large Supermarket 88 70 158 ## Small Supermarket 154 173 327 ## Pharmacy 36 37 73 ## Gas Station 31 44 75 ## Total 309 324 633 ## ## , , time = Total ## ## taxed ## store_type not taxed taxed Total ## Large Supermarket 291 253 544 ## Small Supermarket 542 583 1125 ## Pharmacy 132 130 262 ## Gas Station 109 135 244 ## Total 1074 1101 2175 Frequency table for product type and time period Now we make a frequency table with product type (type) as the row variable and time period (time) as the column variable. tally(~type + time, data = dat, margins = TRUE, format = ''count'') ## time ## type DEC2014 JUN2015 MAR2016 Total ## ENERGY 56 58 49 163 ## ENERGY-DIET 49 54 35 138 ## JUICE 70 64 52 186 ## JUICE DRINK 19 17 6 42 ## MILK 63 61 53 177 ## SODA 239 262 215 716 ## SODA-DIET 128 174 127 429 ## SPORT 11 16 12 39 ## SPORT-DIET 2 2 0 4 ## TEA 52 45 41 138 ## TEA-DIET 6 6 8 20 ## WATER 48 38 34 120 ## WATER-SWEET 1 1 1 3 ## Total 744 798 633 2175 This table shows that there were no observations for the category Sport-diet in March 2016. As this is a drink which even in the other months has very few observations, it may be a product that is offered only in one shop, and it is possible that this shop was not visited in March 2016. The product may also be a seasonal product that is not available in March. It is also likely that Water-sweet is offered in only one shop. conditional meanAn average of a variable, taken over a subgroup of observations that satisfy certain conditions, rather than all observations. Besides counting the number of observations in a particular group, we can also calculate the mean by only using observations that satisfy certain conditions (known as the conditional mean). In this case, we are interested in comparing the mean price of taxed and untaxed beverages, before and after the tax. Calculate and compare conditional means: Create a table similar to Figure 3.1, showing the average price per ounce (in cents) for taxed and untaxed beverages separately, with ‘store type’ as the row variable, and ‘taxed’ and ‘time’ as the column variables. To follow the methodology used in the journal paper, make sure to only include products that are present in all time periods, and non-supplementary products (supp = 0). Without doing any calculations, summarize any differences or general patterns between December 2014 and June 2015 that you find in the table. Would we be able to assess the effect of sugar taxes on product prices by comparing the average price of untaxed goods with that of taxed goods in any given period? Why or why not? Non-taxed Taxed Store type Dec 2014 Jun 2015 Dec 2014 Jun 2015 1 3 The average price of taxed and non-taxed beverages, according to time period and store type. Figure 3.1 The average price of taxed and non-taxed beverages, according to time period and store type. R walk-through 3.4 Calculating conditional means Calculating conditional means is not a straightforward task (in R or in any other statistical program), but is, however, a common data cleaning operation you will encounter. Here is one way to do this. In order to identify products (product_id) that have observations for all three periods (DEC2014 , JUN2015 and MAR2016), we will first create a new variable called period_test, which takes the value 1 (or TRUE) if we have observations in all periods for a product in a particular store, and 0 (FALSE) otherwise. These true/false variables are called ‘boolean variables’. The easiest way to create this variable is through a loop. For each store and product ID, we will check whether there are observations in all periods (i.e. the variable time has the time periods DEC2014 , JUN2015 and MAR2016), and temporarily store this information in the variable test. We then transfer this information to the rows of the new period_test variable that correspond to that store and product ID. dat$period_test <- NA # List of all store IDs sid_list = unique(dat$store_id) # List of all product IDs pid_list = unique(dat$product_id) for (s in sid_list) { for (p in pid_list) { temp <- subset(dat, product_id == p & store_id == s) temp_time <- temp$time test <- ( any(temp_time == ''DEC2014'') & any(temp_time == ''JUN2015'') & any(temp_time == ''MAR2016'')) dat$period_test[dat$product_id == p & dat$store_id == s] <- test } } Now we can use the period_test variable to remove all products that have not been observed in all three periods. We will store the remaining products in a new dataframe, dat_c. dat_c <- subset(dat, (period_test == TRUE & supp == ''Standard'')) Now we can calculate the means of price_per_oz by grouping the data according to store_type , taxed , and time. One way to achieve this is to use a technique called piping: table_res <- dat_c %>% group_by(taxed, store_type, time) %>% summarize(n = length(price_per_oz), avg.price = mean(price_per_oz)) %>% spread(time, avg.price) %>% print() ## # A tibble: 8 x 6 ## # Groups: taxed, store_type [8] ## taxed store_type n DEC2014 JUN2015 MAR2016 ## <fct> <fct> <int> <dbl> <dbl> <dbl> ## 1 not taxed Large Supermarket 36 0.112 0.115 0.117 ## 2 not taxed Small Supermarket 70 0.137 0.138 0.134 ## 3 not taxed Pharmacy 18 0.152 0.161 0.154 ## 4 not taxed Gas Station 12 0.169 0.170 0.170 ## 5 taxed Large Supermarket 36 0.156 0.169 0.167 ## 6 taxed Small Supermarket 101 0.159 0.160 0.155 ## 7 taxed Pharmacy 18 0.182 0.191 0.186 ## 8 taxed Gas Station 22 0.194 0.203 0.192 Piping is a very useful technique in R for data analysis. If we have to perform a sequence of commands on the same object, then we can simplify the code by writing the sequence as a single command. We use the punctuation %>% to link the commands together. In this code above, we 1) took the data in dat_c and grouped them according to the variables taxed, store_type , and time, 2) summarized the data by calculating the mean of price_per_oz and the number of observations in each group, 3) rearranged the results table so that time is the column variable and the values inside the table are the means of price_per_oz. The University of Manchester’s Econometric Computing Learning Resource provides a more detailed introduction to piping. Use the S3 Table in the journal paper to check how closely your summary data match those in the paper.1 You should find that your results for Large Supermarkets and Pharmacies match, but the other store types have discrepancies. In R walk-through 3.5 we will discuss these differences in more detail. In order to make a before-and-after comparison, we will make a chart similar to Figure 2 in the journal paper, to show the change in prices for each store type. Using your table from Question 3: Calculate the change in the mean price after the tax (price in June 2015 minus price in December 2014) for taxed and untaxed beverages, by store type. Using the values you calculated in Question 4(a), plot a column chart to show this information (as done in Figure 2 of the journal paper) with store type on the horizontal axis and price change on the vertical axis. Label each axis and data series appropriately. You should get the same values as shown in Figure 2. R walk-through 3.5 Making a column chart to compare two groups Calculate price differences by store type Let’s calculate the two price differences (June 2015 minus December 2014 and March 2016 minus December 2014), and store them as D1 and D2 respectively: table_res$D1 <- table_res$JUN2015 - table_res$DEC2014 table_res$D2 <- table_res$MAR2016 - table_res$DEC2014 print(''Group Means'') ## [1] ''Group Means'' table_res ## # A tibble: 8 x 8 ## # Groups: taxed, store_type [8] ## taxed store_type n DEC2014 JUN2015 MAR2016 D1 D2 ## <fct> <fct> <int> <dbl> <dbl> <dbl> <dbl> <dbl> ## 1 not taxed Large Supermar~ 36 0.112 0.115 0.117 2.88e-3 0.00510 ## 2 not taxed Small Supermar~ 70 0.137 0.138 0.134 1.46e-3 -0.00304 ## 3 not taxed Pharmacy 18 0.152 0.161 0.154 8.80e-3 0.00240 ## 4 not taxed Gas Station 12 0.169 0.170 0.170 2.90e-4 0.00106 ## 5 taxed Large Supermar~ 36 0.156 0.169 0.167 1.31e-2 0.0107 ## 6 taxed Small Supermar~ 101 0.159 0.160 0.155 1.44e-3 -0.00359 ## 7 taxed Pharmacy 18 0.182 0.191 0.186 8.97e-3 0.00448 ## 8 taxed Gas Station 22 0.194 0.203 0.192 9.25e-3 -0.00174 Plot a column chart for average price changes To display D1 and D2 in a column chart, we will use the ggplot2 package (which is part of the tidyverse package we loaded for R walk-through 3.1). Let’s start with displaying the average price change from December 2014 to June 2015 (which is stored in D1): ggplot(table_res, aes(fill = taxed, y = D1, x = store_type)) + geom_bar(position = ''dodge'', stat = ''identity'') + # Add the axes labels labs(y = ''Price change (US$/oz)'', x = ''Store type'') + # Add the title and legend labels scale_fill_discrete(name = ''Beverages'', labels = c(''Non-taxed'', ''Taxed'')) + ggtitle(''Average price change from Dec 2014 to Jun 2015'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-02.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-02-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-02-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-02-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-02.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Average price change from December 2014 to June 2015. '' /> Average price change from December 2014 to June 2015. Figure 3.2 Average price change from December 2014 to June 2015. Now we do the same for the price change from Dec 2014 to Mar 2016: ggplot(table_res, aes(fill = taxed, y = D2, x = store_type)) + geom_bar(position = ''dodge'', stat = ''identity'') + # Add the axes labels labs(y = ''Price change (US$/oz)'', x = ''Store type'') + # Add the title and legend labels scale_fill_discrete(name = ''Beverages'', labels = c(''Non-taxed'', ''Taxed'')) + ggtitle(''Average price change from Dec 2014 to Mar 2016'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Average price change from December 2014 to March 2016. '' /> Average price change from December 2014 to March 2016. Figure 3.3 Average price change from December 2014 to March 2016. statistically significantWhen a relationship between two or more variables is unlikely to be due to chance, given the assumptions made about the variables (for example, having the same mean). Statistical significance does not tell us whether there is a causal link between the variables. To assess whether the difference in mean prices before and after the tax could have happened by chance due to the samples chosen (and there are no differences in the population means), we could calculate the p-value. (Here, ‘population means’ refer to the mean prices before/after the tax that we would calculate if we had all prices for all stores in Berkeley.) The authors of the journal article calculate p-values, and use the idea of statistical significance to interpret them. Whenever they get a p-value of less than 5%, they conclude that the assumption of no differences in the population is unlikely to be true: they say that the price difference is statistically significant. If they get a p-value higher than 5%, they say that the difference is not statistically significant, meaning that they think it could be due to chance variation in prices. Using a particular cutoff level for the p-value, and concluding that a result is only statistically significant if the p-value is below the cutoff, is common in statistical studies, and 5% is often used as the cutoff level. But this approach has been criticized recently by statisticians and social scientists because the cutoff level is quite arbitrary. Instead of using a cutoff, we prefer to calculate p-values and use them to assess the strength of the evidence against our assumption that there are no differences in the population means. Whether the statistical evidence is strong enough for us to draw a conclusion about a policy, such as a sugar tax, will always be a matter of judgement. According to the journal paper, the p-value is 0.02 for large supermarkets, and 0.99 for pharmacies. Based on these p-values and your chart from Question 4, what can you conclude about the difference in means? (Hint: You may find the discussion in Part 2.3 helpful.) Extension R walk-through 3.6 Calculating the p-value for price changes In this walk-through, we show the calculations required to obtain the p- values in Table S3 of the Silver et al. (2017) paper. Since the p-values are already provided, this walk-through is only for those who want to see how these p-values were calculated. For the categories of Large Supermarkets and Pharmacies, we conduct a hypothesis test, which tests the null hypothesis that the price difference between June 2015 and December 2014 (and March 2016 and December 2014) for the taxed and untaxed beverages in the two store types could be due to chance. We are interested in whether the difference in average price between JUN2015 and DEC2014 (or MAR2016 and DEC2014) for one group (say, Large Supermarket and taxed) is zero (i.e. there is no difference in the means of the two populations). Note that we are dealing with paired observations (the same product in both time periods). Let’s use the price difference between June 2015 and December 2014 in Large Supermarkets for taxed beverages as an example. First, we extract the prices for both periods (the vectors p1 and p2) and then calculate the difference, element by element (stored as d_t). p1 <- dat_c$price_per_oz[ dat_c$store_type == ''Large Supermarket'' & dat_c$taxed == ''taxed'' & dat_c$time == ''DEC2014''] p2 <- dat_c$price_per_oz[ dat_c$store_type == ''Large Supermarket'' & dat_c$taxed == ''taxed'' & dat_c$time == ''JUN2015''] # Price difference for taxed products d_t <- p2 - p1 All three new variables are vectors with 36 elements. For d_t to correctly represent the price difference for a particular product in a particular store, we need to be certain that each element in both vectors corresponds to the same product in the same store. To check that the elements match, we will extract the store and product IDs along with the prices, and compare the ordering in p1_alt and p2_alt. p1_alt <- dat_c[ dat_c$store_type == ''Large Supermarket'' & dat_c$taxed == ''taxed'' & dat_c$time == ''DEC2014'', c(''product_id'', ''store_id'', ''price_per_oz'')] p2_alt <- dat_c[ dat_c$store_type == ''Large Supermarket'' & dat_c$taxed == ''taxed'' & dat_c$time == ''JUN2015'', c(''product_id'', ''store_id'', ''price_per_oz'')] You can see that the ordering matches, since the original datafile was ordered in a way (first according to time, then store_id and then product_id ) that in this instance guarantees identical ordering. The average value of the price difference is 0.0131222, and our task is to evaluate whether this is likely to be due to sampling variation (given the assumption that there is no difference between the populations) or not. To do this, we can use the t.test function, which provides the associated p-value. standard errorA measure of the degree to which the sample mean deviates from the population mean. It is calculated by dividing the standard deviation of the sample by the square root of the number of observations. Alternatively, we can calculate the respective test statistic manually, which also requires us to calculate the standard error of this value: t <- mean(d_t) / sqrt(var(d_t) / (length(d_t))) Alternatively, we can use the available t.test function in R, which gives exactly the same results but has the advantage of directly obtaining p-values and confidence intervals. # Recognize that the differences come from paired samples. t.test(p2, p1, paired = TRUE) ## ## Paired t-test ## ## data: p2 and p1 ## t = 4.7681, df = 35, p-value = 3.226e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.007535221 0.018709202 ## sample estimates: ## mean of the differences ## 0.01312221 # Or get the same result using the function directly on d # t.test(d) To compare this result to the journal paper, look at the extract from Table S3 (the section on Large Supermarkets) shown in Figure 3.4 below. The cell with ‘**’ shows the mean price difference of 1.31 cents ($0.0131). Large supermarkets(n = 6) Taxed beverage price(36 sets) Untaxed beverage price(36 sets) Taxed – untaxed difference cents/oz cents/oz cents/oz Round 1: December 2014 15.62 11.19 Round 2: June 2015 16.93 11.48 Round 3: March 2016 16.68 11.70 Mean change (March 2016–Dec 2014) 1.07, (p=0.01) 0.51, (p=0.01) 0.56, (p=0.22) Mean change(June 2015–Dec 2014) 1.31, (p<0.001)** 0.29, (p=0.08) 1.02, (p=0.002) Table S3 in Silver et al. (2017), showing means and confidence intervals. Figure 3.4 Table S3 in Silver et al. (2017), showing means and confidence intervals. n = number of stores of each type. In our test output we get a very small p-value (0.0000323) which in the table is indicated by the double asterisk. Tests for other store types are calculated similarly, by changing the data extracted to p1 and p2. Let’s do that for one more example: the price difference between June 2015 and December 2014 in Large Supermarkets for untaxed beverages. p1 <- dat_c$price_per_oz[ dat_c$store_type == ''Large Supermarket'' & dat_c$taxed == ''not taxed'' & dat_c$time == ''DEC2014''] p2 <- dat_c$price_per_oz[ dat_c$store_type == ''Large Supermarket'' & dat_c$taxed == ''not taxed'' & dat_c$time == ''JUN2015''] d_nt <- p2 - p1 t.test(p2, p1, paired = TRUE) ## ## Paired t-test ## ## data: p2 and p1 ## t = 1.8179, df = 35, p-value = 0.07765 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.0003367942 0.0061057804 ## sample estimates: ## mean of the differences ## 0.002884493 You should be able to recognize the mean difference, the p-value, and the confidence interval in the excerpt of Table S3 provided in Figure 3.4. Let’s also replicate the last section of Table S3, which shows the difference between the price changes in taxed and untaxed products, that is, we want to know whether d_tAND d_nt have different means. We will apply the two sample hypothesis tests, but this time for unpaired data, as the products differ across samples. t.test(d_t,d_nt) ## ## Welch two sample t-test ## ## data: d_t and d_nt ## t = 3.2227, df = 55.955, p-value = 0.002119 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.003873834 0.016601603 ## sample estimates: ## mean of x mean of y ## 0.013122212 0.002884493 Again you should be able to identify the corresponding entries in Table S3 shown in Figure 3.4. The main entry in the table is 1.02, indicating that the means of the two groups differ by 1.02 cents. This is confirmed in our calculations, as $0.01312 − $0.00288 is about $0.0102 or 1.02 cents. The p-value of 0.002 is also the same as the one in Table S3. Part 3.2 Before-and-after comparisons with prices in other areas When looking for any price patterns, it is possible that the observed changes in Berkeley were not solely due to the tax, but instead were also influenced by other events that happened in Berkeley and in neighbouring areas. To investigate whether this is the case, the researchers conducted another differences-in-differences analysis, using a different treatment and control group: The treatment group: Beverages in Berkeley The control group: Beverages in surrounding areas. The researchers collected price data from stores in the surrounding areas and compared them with prices in Berkeley. If prices changed in a similar way in nearby areas (which were not subject to the tax), then what we observed in Berkeley may not be primarily a result of the tax. We will be using the data the researchers collected to make our own comparisons. Download the following files: The Berkeley Point-of-Sale Stata file on the Global Food Research Program’s website, containing the price data they collected, including information on the date (year and month), location (Berkeley or Non-Berkeley), beverage group (soda, fruit drinks, milk substitutes, milk and water), and the average price for that month. Stata is another popular statistical software package, and the data is provided as a .dta file. ‘S5 Table’ comparing the neighbourhood characteristics of the Berkeley and non-Berkeley stores. Based on ‘S5 Table’, do you think the researchers chose suitable comparison stores? Why or why not? We will now plot a line chart similar to Figure 3 in the journal paper, to compare prices of similar goods in different locations and see how they have changed over time. To do this, we will need to summarize the data so that there is one value (the mean price) for each location and type of good in each month. Assess the effects of a tax on prices: Create a table similar to the one provided in Figure 3.5 to show the average price in each month for taxed and non-taxed beverages, according to location. Use ‘year and month’ as the row variables, and ‘tax’ and ‘location’ as the column variables. (Hint: You may find R walk-through 3.4 helpful.) Plot the four columns of your table on the same line chart, with average price on the vertical axis and time (months) on the horizontal axis. Describe any differences you see between the prices of non-taxed goods in Berkeley and those outside Berkeley, both before the tax (January 2013 to December 2014) and after the tax (March 2015 onwards). Do the same for prices of taxed goods. Based on your chart, is it reasonable to conclude that the sugar tax had an effect on prices? Non-taxed Taxed Year/Month Berkeley Non-Berkeley Berkeley Non-Berkeley January 2013 February 2013 March 2013 … December 2013 January 2014 … February 2016 The average price of taxed and non-taxed beverages, according to location and month. Figure 3.5 The average price of taxed and non-taxed beverages, according to location and month. R walk-through 3.7 Importing data from a Stata file and plotting a line chart Import data and create a table of average monthly prices To import data from a Stata file (.dta format) we need the readstata13 package. library(readstata13) PoSd <- read.dta13(''public_use_weighted_prices2.dta'') Before proceeding, use the command str(PoSd) to look at the structure of this dataset (output not shown here). You will see that for each month and location (Berkeley or Non-Berkeley), there are prices for a variety of beverage categories, and we know whether the category is taxed or not. For any particular time-location-tax status combination we want the average price of all products. To make the summary table, we use the piping technique again: table_test <- PoSd %>% group_by(year, month, location, tax) %>% summarize(avg.price = mean(price)) %>% spread(location, avg.price) %>% print() ## # A tibble: 78 x 5 ## # Groups: year, month [39] ## year month tax Berkeley `Non-Berkeley` ## <dbl> <dbl> <chr> <dbl> <dbl> ## 1 2013 1 Non-taxed 5.72 5.35 ## 2 2013 1 Taxed 8.69 7.99 ## 3 2013 2 Non-taxed 5.81 5.36 ## 4 2013 2 Taxed 8.65 8.18 ## 5 2013 3 Non-taxed 5.86 5.42 ## 6 2013 3 Taxed 8.82 8.19 ## 7 2013 4 Non-taxed 5.86 5.64 ## 8 2013 4 Taxed 9.02 8.25 ## 9 2013 5 Non-taxed 5.79 5.18 ## 10 2013 5 Taxed 8.68 7.76 ## # ... with 68 more rows We have the necessary information, but the data is not quite in the right shape yet, because we need to separate the Taxed from the Non-taxed data (which we do using the subset function). tax_table <- subset(table_test, tax == ''Taxed'') ntax_table <- subset(table_test, tax == ''Non-taxed'') Plot a line chart Now we can create a line chart. Before we do this, we will convert the Berkeley and Non-Berkeley columns of both datasets to time series data using the ts function, which will make plotting a little easier. We can then plot tax_table$Berkeley and add lines for the three other variables. # Use inverted commas ('') to refer to the 'Non-Berkeley' # variable, since R interprets the hyphen as a minus sign. tax_table$Berkeley <- ts(tax_table$Berkeley, start = c(2013, 1), end = c(2016, 3), frequency = 12) tax_table$'Non-Berkeley' <- ts(tax_table$'Non-Berkeley', start = c(2013, 1), end = c(2016, 3), frequency = 12) ntax_table$Berkeley <- ts(ntax_table$Berkeley, start = c(2013, 1), end = c(2016, 3), frequency = 12) ntax_table$'Non-Berkeley' <- ts(ntax_table$'Non-Berkeley', start = c(2013, 1), end = c(2016, 3), frequency = 12) plot(tax_table$Berkeley, col = ''deepskyblue4'', lwd = 2, ylab = ''Average price'', xlab = ''Time'', ylim = c(4, 12)) # n creates a line break. title(''Average price of taxed and non-taxed beverages n in Berkeley and non-Berkeley areas'') lines(tax_table$'Non-Berkeley', col = ''deeppink'', lwd = 2) lines(ntax_table$Berkeley, col = ''darkgreen'', lwd = 2) lines(ntax_table$'Non-Berkeley', col = ''darkorange'', lwd = 2) # Add vertical lines abline(v = 2015.1, col = ''grey'') abline(v = 2015.3, col = ''grey'') # Add labels text(2014.6, 4, ''Pre-tax'') text(2015.8, 4, ''Post-tax'') legend(2013.1, 12, lwd = 2, lty = 1, cex = 0.8, legend = c(''Taxed (Berkeley)'', ''Taxed (non-Berkeley)'', ''Non-taxed (Berkeley)'', ''Non-taxed (non-Berkeley)''), col = c(''deepskyblue4'', ''deeppink'', ''darkgreen'', ''darkorange'')) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-03-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Average price of taxed and non-taxed beverages in Berkeley and non-Berkeley areas. '' /> Average price of taxed and non-taxed beverages in Berkeley and non-Berkeley areas. Figure 3.6 Average price of taxed and non-taxed beverages in Berkeley and non-Berkeley areas. How strong is the evidence that the sugar tax affected prices? According to the journal paper, when comparing the mean Berkeley and non-Berkeley price of sugary beverages after the tax, the p-value is smaller than 0.00001, and it is 0.63 for non-sugary beverages after the tax. What do the p-values tell us about the difference in means and the effect of the sugar tax on the price of sugary beverages? (Hint: You may find the discussion in Part 2.3 helpful.) The aim of the sugar tax was to decrease consumption of sugary beverages. Figure 3.7 shows the mean number of calories consumed and the mean volume consumed before and after the tax. The researchers reported the p-values for the difference in means before and after the tax in the last column. Usual intake Pre-tax (Nov–Dec 2014), n = 623 Post-tax (Nov–Dec 2015), n = 613 Pre-tax–post-tax difference Caloric intake (kilocalories/capita/day) Taxed beverages 45.1 38.7 −6.4, p = 0.56 Non-taxed beverages 115.7 147.6 31.9, p = 0.04 Volume of intake (grams/capita/day) Taxed beverages 121.0 97.0 −24.0, p = 0.24 Non-taxed beverages 1,839.4 1,896.5 57.1, p = 0.22 Models account for age, gender, race/ethnicity, income level, and educational attainment. n is the sample size at each round of the survey after excluding participants with missing values on self-reported race/ethnicity, age, education, income, or monthly intake of sugar-sweetened beverages. Changes in prices, sales, consumer spending, and beverage consumption one year after a tax on sugar-sweetened beverages in Berkeley, California, US: A before-and-after study. Figure 3.7 Changes in prices, sales, consumer spending, and beverage consumption one year after a tax on sugar-sweetened beverages in Berkeley, California, US: A before-and-after study. Lynn D. Silver, Shu Wen Ng, Suzanne Ryan-Ibarra, Lindsey Smith Taillie, Marta Induni, Donna R. Miles, Jennifer M. Poti, and Barry M. Popkin. 2017. Table 1 in ‘Changes in prices, sales, consumer spending, and beverage consumption one year after a tax on sugar-sweetened beverages in Berkeley, California, US: A before-and-after study’. PLoS Med 14 (4): e1002283. Based on Figure 3.7, what can you say about consumption behaviour in Berkeley after the tax? Suggest some explanations for the evidence. Read the ‘Limitations’ in the ‘Discussions’ section of the paper and discuss the strengths and limitations of this study. How could future studies on the sugar tax in Berkeley address these problems? (Some issues you may want to discuss are: the number of stores observed, number of people surveyed, and the reliability of the price data collected.) Suppose that you have the authority to conduct your own sugar tax natural experiment in two neighbouring towns, Town A and Town B. Outline how you would conduct the experiment to ensure that any changes in outcomes (prices, consumption of sugary beverages) are due to the tax and not due to other factors. (Hint: think about what factors you need to hold constant.) Lynn D. Silver, Shu Wen Ng, Suzanne Ryan-Ibarra, Lindsey Smith Taillie, Marta Induni, Donna R. Miles, Jennifer M. Poti, and Barry M. Popkin. 2017. S3 Table in ‘Changes in prices, sales, consumer spending, and beverage consumption one year after a tax on sugar-sweetened beverages in Berkeley, California, US: A before-and-after study’. PLoS Med 14 (4): e1002283. ↩"
});
index.addDoc({
    id: 20,
    title: "Doing Economics: Empirical Project 3: Working in Google Sheets",
    content: "Empirical Project 3 Working in Google Sheets Google Sheets-specific learning objectives In addition to the learning objectives for this project, in this section you will learn how to create summary tables using Google Sheets’ PivotTable option. Part 3.1 Before-and-after comparisons of retail prices We will first look at price data from the treatment group (stores in Berkeley) to see what happened to the price of sugary and non-sugary beverages after the tax. Download the data from the Global Food Research Program’s website, and select the ‘Berkeley Store Price Survey’ Google Sheets dataset. The first tab of the Google Sheets file contains the data dictionary. Make sure you read the data description column carefully, and check that each variable is in the Data tab. Read ‘S1 Text’, from the journal paper’s supporting information, which explains how the Store Price Survey data was collected. In your own words, explain how the product information was recorded, and the measures that researchers took to ensure that the data was accurate and representative of the treatment group. What were some of the data collection issues that they encountered? Instead of using the name of the store, each store was given a unique ID number (recorded as store_id on the spreadsheet). Using Google Sheets’ filter function, verify that the number of stores in the dataset is the same as that stated in the ‘S1 Text’ (26). Similarly, each product was given a unique ID number (product_id). How many different products are in the dataset? Following the approach described in ‘S1 Text’, we will compare the variable price per ounce in US$ cents (price_per_oz_c). We will look at what happened to prices in the two treatment groups before the tax (time = DEC2014) and after the tax (time = JUN2015): treatment group one: large supermarkets (store_type = 1) treatment group two: pharmacies (store_type = 3). Before doing this analysis, we will use summary measures to see how many observations are in the treatment and control group, and how the two groups differ across some variables of interest. For example, if there are very few observations in a group, we might be concerned about the precision of our estimates and will need to interpret our results in light of this fact. Instead of calculating summary measures one by one (as we did in Empirical Project 2), we will use Google Sheets’ PivotTable option to make frequency tables containing the summary measures that we are interested in. The tables should be in a different tab to the data (either all in the same tab, or in separate tabs). Use Google Sheets’ PivotTable option to create the following tables: A frequency table showing the number (count) of store observations (store type) in December 2014 and June 2015, with ‘store type’ as the row variable and ‘time period’ as the column variable. For each store type, is the number of observations similar in each time period? A frequency table showing the number of taxed and non-taxed beverages in December 2014 and June 2015, with ‘store type’ as the row variable and ‘taxed’ as the column variable. (‘Taxed’ equals 1 if the sugar tax applied to that product, and 0 if the tax did not apply). For each store type, is the number of taxed and non-taxed beverages similar? A frequency table showing the number of each product type (type), with ‘product type’ as the row variable and ‘time period’ as the column variables. Which product types have the highest number of observations and which have the lowest number of observations? Why might some products have more observations than others? Google Sheets walk-through 3.1 Making a frequency table using the PivotTable option <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to make a frequency table using Google Sheets’ PivotTable option. '' /> Figure 3.2 How to make a frequency table using Google Sheets’ PivotTable option. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : The data will look like this. We will be making a pivot table using Column C (store type) and Column K (time). It will show how many observations of each store type there are in 2 time periods (Dec2014 and Jun2015). '' /> The data The data will look like this. We will be making a pivot table using Column C (store type) and Column K (time). It will show how many observations of each store type there are in 2 time periods (Dec2014 and Jun2015). Figure 3.2a The data will look like this. We will be making a pivot table using Column C (store type) and Column K (time). It will show how many observations of each store type there are in 2 time periods (Dec2014 and Jun2015). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Insert a blank pivot table : After step 2, a new sheet named ‘Pivot Table’ will appear. '' /> Insert a blank pivot table After step 2, a new sheet named ‘Pivot Table’ will appear. Figure 3.2b After step 2, a new sheet named ‘Pivot Table’ will appear. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Insert a blank pivot table : A sidebar will appear in the newly created sheet. The pivot table is currently blank. In order to create the table, we will select the relevant row variable(s), column variable(s), and values in the sidebar. '' /> Insert a blank pivot table A sidebar will appear in the newly created sheet. The pivot table is currently blank. In order to create the table, we will select the relevant row variable(s), column variable(s), and values in the sidebar. Figure 3.2c A sidebar will appear in the newly created sheet. The pivot table is currently blank. In order to create the table, we will select the relevant row variable(s), column variable(s), and values in the sidebar. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Choose the variables to put in the pivot table : After step 5, your pivot table will look like the one above. '' /> Choose the variables to put in the pivot table After step 5, your pivot table will look like the one above. Figure 3.2d After step 5, your pivot table will look like the one above. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-02-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the values of each variable : The table shown does not have any blank cells, but if there are any, then you can remove them by filtering the data. You can also filter the data so that your table will show specific time periods only. '' /> Filter the values of each variable The table shown does not have any blank cells, but if there are any, then you can remove them by filtering the data. You can also filter the data so that your table will show specific time periods only. Figure 3.2e The table shown does not have any blank cells, but if there are any, then you can remove them by filtering the data. You can also filter the data so that your table will show specific time periods only. conditional meanAn average of a variable, taken over a subgroup of observations that satisfy certain conditions, rather than all observations. Besides counting the number of observations in a particular group, we can also use the PivotTable option to calculate the mean by only using observations that satisfy certain conditions (known as the conditional mean). In this case, we are interested in comparing the mean price of taxed and non-taxed beverages, before and after the tax. Calculate and compare conditional means: Create a table similar to Figure 3.3, showing the average price per ounce (in cents) for taxed and non-taxed beverages separately, with ‘store type’ as the row variable, and ‘taxed’ and ‘time’ as the column variables. To follow the methodology used in the journal paper, make sure to only include products that are present in all time periods, and non-supplementary products (supp = 0). Without doing any calculations, summarize any differences or general patterns between December 2014 and June 2015 that you find in the table. Would we be able to assess the effect of sugar taxes on product prices by comparing the average price of non-taxed goods with that of taxed goods in any given period? Why or why not? Non-taxed Taxed Store type Dec 2014 Jun 2015 Dec 2014 Jun 2015 1 3 The average price of taxed and non-taxed beverages, according to time period and store type. Figure 3.3 The average price of taxed and non-taxed beverages, according to time period and store type. Google Sheets walk-through 3.2 Making a pivot table with more than two variables <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to make a pivot table with more than two variables. '' /> Figure 3.4 How to make a pivot table with more than two variables. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : The data will look like this. We will be making a pivot table using Column C (store type), Column K (time), and Column I (taxed). It will show the average price of taxed and non-taxed beverages in Dec2014 and Jun2015, according to store type. '' /> The data The data will look like this. We will be making a pivot table using Column C (store type), Column K (time), and Column I (taxed). It will show the average price of taxed and non-taxed beverages in Dec2014 and Jun2015, according to store type. Figure 3.4a The data will look like this. We will be making a pivot table using Column C (store type), Column K (time), and Column I (taxed). It will show the average price of taxed and non-taxed beverages in Dec2014 and Jun2015, according to store type. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Count the number of times each product appears in the dataset : We only want to look at products that were present in all time periods, to ensure we are comparing the same group of products over time. We will create a new variable (called ‘Number’, shown in Column M) that shows how many periods of data are available for each product. The COUNTIFS function will help us count the number of observations that satisfy certain conditions. '' /> Count the number of times each product appears in the dataset We only want to look at products that were present in all time periods, to ensure we are comparing the same group of products over time. We will create a new variable (called ‘Number’, shown in Column M) that shows how many periods of data are available for each product. The COUNTIFS function will help us count the number of observations that satisfy certain conditions. Figure 3.4b We only want to look at products that were present in all time periods, to ensure we are comparing the same group of products over time. We will create a new variable (called ‘Number’, shown in Column M) that shows how many periods of data are available for each product. The COUNTIFS function will help us count the number of observations that satisfy certain conditions. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Insert a blank pivot table : We will make and store the frequency table in a new tab in the spreadsheet. '' /> Insert a blank pivot table We will make and store the frequency table in a new tab in the spreadsheet. Figure 3.4c We will make and store the frequency table in a new tab in the spreadsheet. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Fill in the Pivot Table : After step 6, your pivot table will look like the one above. By default, Google Sheets uses all the available data. '' /> Fill in the Pivot Table After step 6, your pivot table will look like the one above. By default, Google Sheets uses all the available data. Figure 3.4d After step 6, your pivot table will look like the one above. By default, Google Sheets uses all the available data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Round cell values to two decimal places : To make the table easier to read, we will round the cell values to two decimal places. '' /> Round cell values to two decimal places To make the table easier to read, we will round the cell values to two decimal places. Figure 3.4e To make the table easier to read, we will round the cell values to two decimal places. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the values inside the table : We will filter the data according to the values of ‘store_type’, ‘time’, ‘Number’ and ‘supp’. '' /> Filter the values inside the table We will filter the data according to the values of ‘store_type’, ‘time’, ‘Number’ and ‘supp’. Figure 3.4f We will filter the data according to the values of ‘store_type’, ‘time’, ‘Number’ and ‘supp’. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the values inside the table : We will filter the data so that only the store types we want (1 and 3) are visible. '' /> Filter the values inside the table We will filter the data so that only the store types we want (1 and 3) are visible. Figure 3.4g We will filter the data so that only the store types we want (1 and 3) are visible. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-h.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-h-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-h-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-h-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-h.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the values of each variable : We will filter the data so that only the time periods we want (Dec2014 and Jun2015) are visible. '' /> Filter the values of each variable We will filter the data so that only the time periods we want (Dec2014 and Jun2015) are visible. Figure 3.4h We will filter the data so that only the time periods we want (Dec2014 and Jun2015) are visible. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-i.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-i-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-i-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-i-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-i.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the values of each variable : We now filter the data so that only products that are available in all time periods (Number = 3) are visible. '' /> Filter the values of each variable We now filter the data so that only products that are available in all time periods (Number = 3) are visible. Figure 3.4i We now filter the data so that only products that are available in all time periods (Number = 3) are visible. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-j.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-j-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-j-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-j-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-j.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the values of each variable : We filter the data so that only non-supplementary products (supp = 0) are visible. '' /> Filter the values of each variable We filter the data so that only non-supplementary products (supp = 0) are visible. Figure 3.4j We filter the data so that only non-supplementary products (supp = 0) are visible. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-k.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-k-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-k-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-k-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-04-k.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Remove the grand total row and column : After step 15, your table should look like the one above. '' /> Remove the grand total row and column After step 15, your table should look like the one above. Figure 3.4k After step 15, your table should look like the one above. In order to make a before-and-after comparison, we will make a chart similar to Figure 2 in the journal paper, to show the change in prices for each store type. Using your table from Question 3: Calculate the change in the mean price after the tax (price in June 2015 minus price in December 2014) for taxed and non-taxed beverages, by store type. Using the values you calculated in Question 4(a), plot a column chart to show this information (as done in Figure 2 of the journal paper) with store type on the horizontal axis and price change on the vertical axis. Label each axis and data series appropriately. You should get the same values as shown in Figure 2. Google Sheets walk-through 3.3 Making a column chart to compare two groups <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to make a column chart to compare two groups. '' /> Figure 3.5 How to make a column chart to compare two groups. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create a table showing differences in means : We will display the calculated differences in the table highlighted in blue. The labels on the rows are in the same order as in the pivot table, but the rows are flipped, since 0 corresponds to ‘Non-taxed’ and 1 to ‘Taxed’ in the pivot table. '' /> Create a table showing differences in means We will display the calculated differences in the table highlighted in blue. The labels on the rows are in the same order as in the pivot table, but the rows are flipped, since 0 corresponds to ‘Non-taxed’ and 1 to ‘Taxed’ in the pivot table. Figure 3.5a We will display the calculated differences in the table highlighted in blue. The labels on the rows are in the same order as in the pivot table, but the rows are flipped, since 0 corresponds to ‘Non-taxed’ and 1 to ‘Taxed’ in the pivot table. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create a table showing differences in means : Fill in the table by using cell formulas to calculate the differences required. After step 2, your table will look like the one shown above. '' /> Create a table showing differences in means Fill in the table by using cell formulas to calculate the differences required. After step 2, your table will look like the one shown above. Figure 3.5b Fill in the table by using cell formulas to calculate the differences required. After step 2, your table will look like the one shown above. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a column chart : We will use the table we created to make a column chart. '' /> Draw a column chart We will use the table we created to make a column chart. Figure 3.5c We will use the table we created to make a column chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-03-05-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add chart and axis titles, and move the chart legend : After step 7, your chart will look like the bottom chart of Figure 2 in the journal paper. '' /> Add chart and axis titles, and move the chart legend After step 7, your chart will look like the bottom chart of Figure 2 in the journal paper. Figure 3.5d After step 7, your chart will look like the bottom chart of Figure 2 in the journal paper. statistically significantWhen a relationship between two or more variables is unlikely to be due to chance, given the assumptions made about the variables (for example, having the same mean). Statistical significance does not tell us whether there is a causal link between the variables. To assess whether the difference in mean prices before and after the tax could have happened by chance due to the samples chosen (and there are no differences in the population means), we could calculate the p-value. (Here, ‘population means’ refer to the mean prices before/after the tax that we would calculate if we had all prices for all stores in Berkeley.) The authors of the journal article calculate p-values, and use the idea of statistical significance to interpret them. Whenever they get a p-value of less than 5%, they conclude that the assumption of no differences in the population is unlikely to be true: they say that the price difference is statistically significant. If they get a p-value higher than 5%, they say that the difference is not statistically significant, meaning that they think it could be due to chance variation in prices. Using a particular cutoff level for the p-value, and concluding that a result is only statistically significant if the p-value is below the cutoff, is common in statistical studies, and 5% is often used as the cutoff level. But this approach has been criticized recently by statisticians and social scientists because the cutoff level is quite arbitrary. Instead of using a cutoff, we prefer to calculate p-values and use them to assess the strength of the evidence against our assumption that there are no differences in the population means. Whether the statistical evidence is strong enough for us to draw a conclusion about a policy, such as a sugar tax, will always be a matter of judgement. According to the journal paper, the p-value is 0.02 for large supermarkets, and 0.99 for pharmacies. Based on these p-values and your chart from Question 4, what can you conclude about the difference in means? (You may find the discussion in Part 2.3 helpful.) Part 3.2 Before-and-after comparisons with prices in other areas When looking for any price patterns, it is possible that the observed changes were not due to the tax, but instead were due to other events that happened in Berkeley and in neighbouring areas. If prices changed in a similar way in nearby areas, then what we observed in Berkeley may not be a result of the tax. To investigate whether this is the case, the researchers collected price data from stores in the surrounding areas and compared them with prices in Berkeley. Download the following files: The Excel file containing the price data they collected, including information on the date (year and month), location (Berkeley or Non-Berkeley), beverage group (soda, fruit drinks, milk substitutes, milk, and water), and the average price for that month. ‘S5 Table’ comparing the neighbourhood characteristics of the Berkeley and non-Berkeley stores. Based on ‘S5 Table’, do you think the researchers chose suitable comparison stores? Why or why not? We will now plot a line chart similar to Figure 3 in the journal paper, to compare prices of similar goods in different locations and see how they have changed over time. To do this, we will need to summarize the data using Google Sheets’ PivotTable option, so that there is one value (the mean price) for each location and type of good in each month. Assess the effects of a tax on prices: Create a table similar to Figure 3.6 to show the average price in each month for taxed and non-taxed beverages, according to location. Use ‘year and month’ as the row variables, and ‘tax’ and ‘location’ as the column variables. (You may find Google Sheets walk-through 3.2 helpful.) Plot the four columns of your table on the same line chart, with average price on the vertical axis and time (months) on the horizontal axis. Describe any differences you see between the prices of non-taxed goods in Berkeley and those outside Berkeley, both before the tax (January 2013 to December 2014) and after the tax (March 2015 onwards). Do the same for prices of taxed goods. Based on your chart, is it reasonable to conclude that the sugar tax had an effect on prices? Non-taxed Taxed Year/Month Berkeley Non-Berkeley Berkeley Non-Berkeley January 2013 February 2013 March 2013 … December 2013 January 2014 … February 2016 The average price of taxed and non-taxed beverages, according to location and month. Figure 3.6 The average price of taxed and non-taxed beverages, according to location and month. How strong is the evidence that the sugar tax affected prices? According to the journal paper, when comparing the mean Berkeley and non-Berkeley price of sugary beverages after the tax, the p-value is smaller than 0.00001, and it is 0.63 for non-sugary beverages after the tax. What do the p-values tell us about the difference in means and the effect of the sugar tax on the price of sugary beverages? (You may find the discussion in Part 2.3 helpful.) The aim of the sugar tax was to decrease consumption of sugary beverages. Figure 3.7 shows the mean number of calories consumed and the mean volume consumed before and after the tax. The researchers reported the p-values for the difference in means before and after the tax in the last column. Usual intake Pre-tax (Nov–Dec 2014), n = 623 Post-tax (Nov–Dec 2015), n = 613 Pre-tax–post-tax difference Caloric intake (kilocalories/capita/day) Taxed beverages 45.1 38.7 −6.4, p = 0.56 Non-taxed beverages 115.7 147.6 31.9, p = 0.04 Volume of intake (grams/capita/day) Taxed beverages 121.0 97.0 −24.0, p = 0.24 Non-taxed beverages 1,839.4 1,896.5 57.1, p = 0.22 Models account for age, gender, race/ethnicity, income level, and educational attainment. n is the sample size at each round of the survey after excluding participants with missing values on self-reported race/ethnicity, age, education, income, or monthly intake of sugar-sweetened beverages. Changes in prices, sales, consumer spending, and beverage consumption one year after a tax on sugar-sweetened beverages in Berkeley, California, US: A before-and-after study. Figure 3.7 Changes in prices, sales, consumer spending, and beverage consumption one year after a tax on sugar-sweetened beverages in Berkeley, California, US: A before-and-after study. Lynn D. Silver, Shu Wen Ng, Suzanne Ryan-Ibarra, Lindsey Smith Taillie, Marta Induni, Donna R. Miles, Jennifer M. Poti, and Barry M. Popkin. 2017. Table 1 in ‘Changes in prices, sales, consumer spending, and beverage consumption one year after a tax on sugar-sweetened beverages in Berkeley, California, US: A before-and-after study’. PLoS Med 14 (4): e1002283. Based on Figure 3.7, what can you say about consumption behaviour in Berkeley after the tax? Suggest some explanations for the evidence. Read the ‘Limitations’ in the ‘Discussions’ section of the paper and discuss the limitations of this study. How could future studies on the sugar tax in Berkeley address these problems? (Some issues you may want to discuss are: the number of stores observed, number of people surveyed, and the reliability of the price data collected.) Suppose that you have the authority to conduct your own sugar tax natural experiment in two neighbouring towns, Town A and Town B. Outline how you would conduct the experiment to ensure that any changes in outcomes (prices, consumption of sugary beverages) are due to the tax and not due to other factors. (Hint: think about what factors you need to hold constant.)"
});
index.addDoc({
    id: 21,
    title: "Doing Economics: Empirical Project 3 Solutions",
    content: "Empirical Project 3 Solutions These are not model answers. They are provided to help students, including those doing the project outside a formal class, to check their progress while working through the questions using the Excel, R, or Google Sheets walk-throughs. There are also brief notes for the more interpretive questions. Students taking courses using Doing Economics should follow the guidance of their instructors. Part 3.1 Before-and-after comparisons of retail prices Three Store Price Surveys (SPS) were conducted in a sample of stores to collect data on beverages, one before the tax was implemented, and two after. The sample of stores contains stores most visited by participants. To ensure that the sample is representative, additional stores within different categories of stores were added via random selection. Some selected stores refused to participate in the data collection, in which case randomly selected stores of the same type and neighbourhood were used as replacements. A list of top-selling beverages was identified. Price data for each beverage was collected in each store by trained data collectors who followed standardized procedures. Some stores did not have any beverages in the list. Additional beverages similar to those in the list were therefore included in a supplementary panel. Data were entered into a database using a tablet computer and paper forms. To avoid mistakes in transferring data from paper forms to the computer, the data on paper forms were double-entered. The two copies were compared and if results were dissimilar, the corresponding value in the paper forms was entered again. There are 247 different products in the dataset. Solution figure 3.1 shows the frequency table for all store types in December 2014 and June 2015. The number of observations for each store type is similar in each time period. Store type 4 is associated with the largest percentage change over time of about 32%. Store Type Dec 2014 Jun 2015 Total 1 177 209 386 2 407 391 798 3 87 102 189 4 73 96 169 Grand total 744 798 1,542 Frequency table: All stores in December 2014 and June 2015. Solution figure 3.1 Frequency table: All stores in December 2014 and June 2015. Solution figures 3.2 and 3.3 show the frequency tables for taxed and non-taxed beverages for December 2014 and June 2015 separately. In both periods, the number of taxed and non-taxed beverages is similar for each store type. Store Type No Tax Tax Total 1 92 85 177 2 196 211 407 3 44 43 87 4 34 39 73 Grand total 366 378 744 Numbers of taxed and untaxed beverages by store type, December 2014. Solution figure 3.2 Numbers of taxed and untaxed beverages by store type, December 2014. Store Type No Tax Tax Total 1 111 98 209 2 192 199 391 3 52 50 102 4 44 52 96 Grand total 399 399 798 Numbers of taxed and untaxed beverages by store type, June 2015. Solution figure 3.3 Numbers of taxed and untaxed beverages by store type, June 2015. Solution figure 3.4 shows the number of product types available in December 2014 and June 2015. The SODA type has the highest number of observations. The SPORT-DIET type has the lowest number of observations. Beverages belonging to more popular types are more likely to be in the panel of beverages. More popular types therefore tend to have more observations, since they are likely to be available in a greater number of stores. Row Labels Dec 2014 Jun 2015 Total Energy 56 58 114 Energy-diet 49 54 103 Juice 70 64 134 Juice Drink 19 17 36 Milk 63 61 124 Soda 239 262 501 Soda-diet 128 174 302 Sport 11 16 27 Sport-diet 2 2 4 Tea 52 45 97 Tea-diet 6 6 12 Water 48 38 86 Water-sweet 1 1 2 Grand total 744 798 1,542 Product types available, December 2014 and June 2015. Solution figure 3.4 Product types available, December 2014 and June 2015. Solution figure 3.5 shows the average price per oz (ounce) for taxed and non-taxed beverages. Non-taxed Taxed Store type Dec 2014 Jun 2015 Dec 2014 Jun 2015 1 11.19 11.48 15.62 16.93 3 15.20 16.08 18.18 19.08 Average price per ounce of taxed and non-taxed beverages, by time period and store type. Solution figure 3.5 Average price per ounce of taxed and non-taxed beverages, by time period and store type. Average prices increase over time for both types, whether taxed or not. This suggests that there is an upward time trend in prices that is independent of the tax. For both groups, before the tax was implemented, the average price for the taxed (treatment) group is higher than that for the non-taxed (control) group. This suggests that the beverages subject to the tax were the more expensive beverages. No, because the difference in average price between the non-taxed group and the taxed group in any given period can be due to group-specific factors other than the tax. The difference in average price between the treatment and control groups in December 2014 before the implementation of the taxes suggests that the two groups are fundamentally different. The difference in June 2015 could be due to the differences between the groups rather than the taxes. Solution figure 3.6 shows the change in the mean price after the tax by store type. Non-taxed Taxed Large supermarkets 0.29 1.31 Pharmacies 0.88 0.90 Change in the mean price per oz ounce for taxed and non-taxed beverages, by store type. Solution figure 3.6 Change in the mean price per oz ounce for taxed and non-taxed beverages, by store type. Solution figure 3.7 shows the mean change in price per oz for taxed and non-taxed beverages, by store type. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-03-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-03-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-03-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-03-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-03-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Mean change in price per oz for taxed and non-taxed beverages, by store type. '' /> Mean change in price per oz for taxed and non-taxed beverages, by store type. Solution figure 3.7 Mean change in price per oz for taxed and non-taxed beverages, by store type. Note: When comparing these values to those in Silver et al. (2016), you will find that the data match for the ‘Large Supermarket’ and ‘Pharmacy store’ types, but not for the ‘Small Supermarket’ and ‘Gas Station store’ types. These discrepancies are due to the differences in categories used by Silver et al. (2016), for example, Silver et al. use ‘Independent corner stores and independent gas stations’ and ‘Small chain supermarkets and chain gas stations’ instead of ‘Small supermarkets’ and ‘Gas stations’. The p-value for large supermarkets is quite small, indicating that the assumption that there are no differences in the population is unlikely to be true. Thus it is likely that the sugar tax had some effect on prices. On the other hand, the p-value for pharmacies is quite large, so there is reasonably strong evidence in favour of our assumption that there are no differences in the populations. Part 3.2 Before-and-after comparisons with prices in other areas Ideally, we would like the characteristics of the non-Berkeley stores to be the same as those of the Berkeley stores, so that patterns observed for the non-Berkeley stores can inform us about the patterns of Berkeley stores, if there had not been tax changes. If the two regions are sufficiently similar, then the differences in changes in the mean prices can be attributed to the tax policy. The researchers chose suitable comparison stores, as the stores’ characteristics are very similar to those in Berkeley. Solution figure 3.8 indicates the average price in each month for taxed and non-taxed beverages, according to location. Non-taxed Taxed Year/Month Berkeley Non-Berkeley Berkeley Non-Berkeley 2013 1 5.72 5.35 8.69 7.99 2 5.81 5.37 8.66 8.19 3 5.86 5.42 8.82 8.19 4 5.86 5.65 9.02 8.25 5 5.83 5.21 8.73 7.81 6 5.79 5.06 8.61 7.46 7 5.97 5.15 8.32 7.27 8 5.90 5.14 8.92 7.57 9 5.91 5.15 9.13 7.90 10 5.87 5.24 9.10 7.85 11 6.08 5.37 9.23 8.07 12 6.09 5.34 9.06 7.89 2014 1 6.12 5.37 8.98 8.00 2 6.20 5.53 9.20 7.95 3 6.47 5.84 9.53 8.17 4 6.38 5.80 9.64 8.35 5 6.52 5.73 9.66 8.50 6 6.56 5.80 9.19 8.11 7 6.48 5.59 9.29 8.24 8 6.37 5.66 9.34 8.10 9 6.65 5.84 9.32 8.60 10 6.38 5.60 9.48 8.58 11 5.85 5.65 8.03 8.47 12 6.19 5.52 9.67 8.53 2015 1 6.41 5.85 10.02 8.82 2 6.39 5.66 9.24 8.49 3 6.51 5.71 10.02 8.82 4 6.48 5.83 10.38 8.76 5 6.64 5.82 10.34 8.94 6 6.64 5.83 10.42 8.69 7 6.37 5.73 10.58 8.90 8 6.45 5.69 11.10 9.03 9 6.51 5.67 10.44 8.71 10 6.54 5.67 10.70 8.79 11 6.67 5.85 10.71 8.98 12 6.56 5.76 10.54 8.59 2016 1 6.58 5.87 10.57 8.96 2 6.55 5.76 10.81 8.73 Grand total 6.27 5.58 9.58 8.35 Average prices of taxed and non-taxed beverages in Berkeley vs non-Berkeley stores. Solution figure 3.8 Average prices of taxed and non-taxed beverages in Berkeley vs non-Berkeley stores. Solution figure 3.9 shows the average price per month for taxed and non-taxed beverages, according to location. Non-taxed goods in Berkeley are more expensive than those outside Berkeley. The difference in prices before the tax indicates there are fundamental differences between the two regions irrespective of the tax policy. The difference stays roughly the same after the tax. This suggests that there were probably no other changes in the period that affected the difference in prices. Taxed goods in Berkeley are more expensive than those outside Berkeley. The difference in prices increased after the tax was implemented (March 2015 onwards). The prices of taxed goods and non-taxed goods in non-Berkeley regions have similar time trends. The time trends of goods in non-Berkeley regions are also similar to those in Berkeley. If we strip away the time trend, the prices of taxed and non-taxed goods in non-Berkeley regions remain roughly the same before and after the tax. This means that any events that took place during the period did not affect the level of prices in Berkeley and its neighbouring areas. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-03-09.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-03-09-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-03-09-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-03-09-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-03-09.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Average prices of taxed and non-taxed beverages in Berkeley vs non-Berkeley stores. '' /> Average prices of taxed and non-taxed beverages in Berkeley vs non-Berkeley stores. Solution figure 3.9 Average prices of taxed and non-taxed beverages in Berkeley vs non-Berkeley stores. It is reasonable to conclude that the sugar tax had an effect on prices. The difference between the prices of the taxed goods in Berkeley and those outside Berkeley after the tax was implemented reflects both the effect of the tax and the effects of other differences between the two regions. The difference between the prices before the tax reflects the effects of differences other than the tax policy between the two regions. The difference in prices after the implementation of the tax, subtracting the difference in prices before the tax, gives us the effect of the tax. The increase in the difference in prices suggests that the tax has a positive effect on prices. Note: When you compare this figure to Figure 3 in the Silver et al. paper you will notice that it is somewhat different. The effect of the tax is much more clearly visible in the paper. The reason for this difference is that the authors adjusted the prices for a number of factors (for example, see the notes to Figure 3) before they created the line chart. The adjustment process they used, however, is beyond the scope of this project. The difference between the mean Berkeley and non-Berkeley price of non-sugary beverages after the tax reflects the effect of being in Berkeley as opposed to being in non-Berkeley regions. The difference between the mean Berkeley and non-Berkeley price of sugary beverages after the tax reflects not only the effect of being in Berkeley but also the effect of the tax. The difference in differences is the effect of the tax. Based on the p-value, we can reasonably conclude that the differences between the mean Berkeley and non-Berkeley prices of non-sugary beverages after the tax are likely to be due to chance. In other words, there is reasonably strong evidence in favour of our assumption that there are no differences in the population means of these two groups. Other than the tax, there were no events that took place specifically in Berkeley that could have driven the results. The p-value for the mean Berkeley and non-Berkeley prices of sugary beverages after the tax is quite small, so it is unlikely to see the differences we observe in the samples if in truth (i.e. in the population) there was no difference in the means. Since our assumption that the before-tax mean Berkeley and non-Berkeley price of non-sugary beverages are the same is likely to be true, the difference between the mean Berkeley and non-Berkeley price of sugary beverages after the tax is the effect of the tax. We can therefore be quite confident that the tax had an effect on prices. Except for caloric intake of non-taxed beverages, the p-values of the other entries are fairly large. This implies that the differences in consumption behaviour we observe are likely to have occurred even if, in truth, there was no difference (the null hypothesis being no difference). Beverages form a small proportion of the total budget of consumers. Consumers are unlikely to change their consumption by much given the tiny effect the price rise has on their budgets. The two major ingredients in sugary beverages, water and sugar, are craved by the human body due to their importance for survival. Beverage companies have devoted tremendous efforts to make their beverages attractive to consumers. Many beverages have become part of people’s daily routines and have also become viewed as necessities for certain occasions. These factors mean that the demand for sugary beverages is price inelastic (not responsive to price changes). People take time to change their consumption habits. Consumption patterns may change slowly over a long period of time. We need data for later periods to study the long-term effect of the tax. Some of the limitations listed in the paper are: The study cannot fully distinguish the effect of the tax from the effect of other events, such as political campaigns related to the tax, which took place over the same period. The study cannot therefore establish the ceteris paribus causal effect of the tax. A more distant control (for example, another location that was not subject to these events) could be used to capture better the combined effects of the events and the tax. The study cannot tell whether the changes in behaviours were due to anticipation of the tax or changes in prices and sales. The sample of stores is too small and not representative. Consumption of SSB is small and the effect size is small relative to the high standard error. Future studies should use larger and more representative samples. There are limitations in the data from independently-owned small corner stores. The possible effect of shifts in purchases to these stores cannot be studied using the data in the study. A tax change could be implemented unexpectedly to reduce the effects of events such as campaigns in the periods of observation and to prevent behaviours from adjusting even before the experiment begins. If possible, ensure that there are no other policy changes that could affect the outcomes in the observation period. Also ensure that the treatment and control groups stay the same over the period, for example, by preventing the relocation of stores in response to the tax change. If possible, it is also important to prevent people from the taxed area from going to the neighbouring area to buy sugary drinks (which would be contrary to the purpose of the tax). Listing these factors helps to clarify the difficulties involved in establishing the conditions for a clean natural experiment in the real-world setting of policy implementation. In most countries, for example, it is not possible to prevent the relocation of stores and it would not be desirable to do so."
});
index.addDoc({
    id: 22,
    title: "Doing Economics: Empirical Project 4: Measuring wellbeing",
    content: "Empirical Project 4 Measuring wellbeing Learning objectives In this project you will: check datasets for missing data (Part 4.1) sort data and assign ranks based on values (Parts 4.1 and 4.2) distinguish between time series and cross sectional data, and plot appropriate charts for each type of data (Parts 4.1 and 4.2) calculate the geometric mean and explain how it differs from the arithmetic mean (Part 4.2) construct indices using the geometric mean, and use index values to rank observations (Part 4.2) explain the difference between two measures of wellbeing (GDP per capita and the Human Development Index) (Part 4.2). Key concepts Concepts needed for this project: mean. Concepts introduced in this project: index, time series data, cross-sectional data, geometric mean, and the natural log transformation. Introduction CORE projects This empirical project is related to material in: Unit 4 of Economy, Society, and Public Policy Unit 3 of The Economy. GDP per capita is a widely used summary measure of incomes in a country. It is calculated by dividing gross domestic product (GDP)—the total value of all the goods and services produced in a country in a given period, such as a year—by the population of the country. GDP per capita is therefore a measure of average annual income in a country. To read more about GDP per capita, see Section 1.3 of Economy, Society, and Public Policy. GDP per capita is commonly used in economics to compare living standards across countries or measure progress in living standards over time. The rationale is that higher income and expenditure means a greater ability to spend on goods and services, which in turn increases material wellbeing. Since material wellbeing can contribute to non-material wellbeing, we might also expect countries with higher GDP per capita to have higher non-material wellbeing. But how do we measure non-material wellbeing? And does a higher GDP per capita necessarily mean a higher non-material wellbeing? indexAn index is formed by aggregating the values of multiple items into a single value, and is used as a summary measure of an item of interest. Example: The HDI is a summary measure of wellbeing, and is calculated by aggregating the values for life expectancy, expected years of schooling, mean years of schooling, and gross national income per capita. To answer these questions, we will first learn how different variables can be summarized in an index by looking at GDP and its components. Indices are often used to summarize a number of different variables, all describing a common object or phenomenon, into a single number that can give an overall picture of what is happening. You may be familiar with the consumer price index (CPI), a common measure of inflation. We will then learn how indices of non-material wellbeing are constructed, and compare an index of material wellbeing (GDP per capita) with an index of non-material wellbeing (the Human Development Index). Working in Excel Working in R Working in Google Sheets"
});
index.addDoc({
    id: 23,
    title: "Doing Economics: Empirical Project 4: Working in Excel",
    content: "Empirical Project 4 Working in Excel Excel-specific learning objectives In addition to the learning objectives for this project, in this section you will learn how to generate new variables using cell formulas. Part 4.1 GDP and its components as a measure of material wellbeing Learning objectives for this part check datasets for missing data sort data and assign ranks based on values distinguish between time series and cross sectional data, and plot appropriate charts for each type of data. The GDP data we will look at is from the United Nations’ National Accounts Main Aggregates Database, which contains estimates of total GDP and its components for all countries over the period 1970–Present. We will look at how GDP and its components have changed over time, and investigate the usefulness of GDP per capita as a measure of wellbeing. To answer the questions below, download the data and make sure you understand how the measure of total GDP is constructed. Go to the United Nations’ National Accounts Main Aggregates Database website. On the right-hand side of the page, under ‘Data Availability’, click ‘Downloads’. Under the subheading ‘GDP and its breakdown at constant 2010 prices in US Dollars’, select the Excel file ‘All countries for all years – sorted alphabetically’. Save it in an easily accessible location, such as a folder on your Desktop or in your personal folder. You can see from the tab ‘Download-GDPconstant-USD-countr’ that some countries have missing data for some of the years. Data may be missing due to political reasons (for example, countries formed after 1970) or data availability issues. Make and fill a frequency table similar to Figure 4.1, showing the number of years that data is available for each country in the category ‘Final consumption expenditure’. How many countries have data for the entire period (1970 to the latest year available)? Do you think that missing data is a serious issue in this case? Country Number of years of GDP data Number of years of GDP data available for each country. Figure 4.1 Number of years of GDP data available for each country. Excel walk-through 4.1 Making a frequency table <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to make a frequency table. '' /> Figure 4.2 How to make a frequency table. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the data looks like. Column B has country names, Column C has the different components of GDP, and Column D onwards has the values of the GDP components for a particular year. '' /> The data This is what the data looks like. Column B has country names, Column C has the different components of GDP, and Column D onwards has the values of the GDP components for a particular year. Figure 4.2a This is what the data looks like. Column B has country names, Column C has the different components of GDP, and Column D onwards has the values of the GDP components for a particular year. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Count the years of data available in each row : We will first count the number of years of data available using the COUNTA function, which tells us how many cells in a given selection are non-empty (i.e. have data). We will store this information in Column AY, as shown. Note: The numbers you get may be slightly different if you are using the latest data. '' /> Count the years of data available in each row We will first count the number of years of data available using the COUNTA function, which tells us how many cells in a given selection are non-empty (i.e. have data). We will store this information in Column AY, as shown. Note: The numbers you get may be slightly different if you are using the latest data. Figure 4.2b We will first count the number of years of data available using the COUNTA function, which tells us how many cells in a given selection are non-empty (i.e. have data). We will store this information in Column AY, as shown. Note: The numbers you get may be slightly different if you are using the latest data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the data : We only want to know how many years of ‘Final consumption expenditure’ are available for each country, so we need to filter out the rest of the data. '' /> Filter the data We only want to know how many years of ‘Final consumption expenditure’ are available for each country, so we need to filter out the rest of the data. Figure 4.2c We only want to know how many years of ‘Final consumption expenditure’ are available for each country, so we need to filter out the rest of the data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the data : After step 7, the data will be filtered so there is only one number for each country. Your table is ready to copy and paste into a new tab. '' /> Filter the data After step 7, the data will be filtered so there is only one number for each country. Your table is ready to copy and paste into a new tab. Figure 4.2d After step 7, the data will be filtered so there is only one number for each country. Your table is ready to copy and paste into a new tab. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Paste the data in a new tab : The table will now look like Figure 4.1. The final step is to count the number of countries with missing data. '' /> Paste the data in a new tab The table will now look like Figure 4.1. The final step is to count the number of countries with missing data. Figure 4.2e The table will now look like Figure 4.1. The final step is to count the number of countries with missing data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-02-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Count the number of countries with missing data : To count the number of countries with missing data, use the COUNTIF function. Excel will count the number of cells in a given selection that meet the specified criteria. Note: If you are using the latest data, you should change the number in the COUNTIF function from ‘47’ to the maximum number of years available (the latest year in your data minus 1970 plus 1), so the numbers you get may be slightly different. '' /> Count the number of countries with missing data To count the number of countries with missing data, use the COUNTIF function. Excel will count the number of cells in a given selection that meet the specified criteria. Note: If you are using the latest data, you should change the number in the COUNTIF function from ‘47’ to the maximum number of years available (the latest year in your data minus 1970 plus 1), so the numbers you get may be slightly different. Figure 4.2f To count the number of countries with missing data, use the COUNTIF function. Excel will count the number of cells in a given selection that meet the specified criteria. Note: If you are using the latest data, you should change the number in the COUNTIF function from ‘47’ to the maximum number of years available (the latest year in your data minus 1970 plus 1), so the numbers you get may be slightly different. If you add up the data on the right-hand side of this equation, you may find that it does not add up to the reported GDP value. The UN notes this discrepancy in Section J, item 17 of the ‘Methodology for the national accounts’: ‘The sums of com­ponents in the tables may not necessarily add up to totals shown because of rounding’. There are three different ways in which countries calculate GDP for their national accounts, but we will focus on the expenditure approach, which calculates gross domestic product (GDP) as: GDP=Household consumption expenditure+General government final consumption expenditure+Gross capital formation+(Exports of goods and services − imports of goods and services)% <![CDATA[ begin{align*} text{GDP} &= text{Household consumption expenditure}  &+ text{General government final consumption expenditure}  &+ text{Gross capital formation}  &+ text{(Exports of goods and services − imports of goods and services)} end{align*} %]]> Gross capital formation refers to the creation of fixed assets in the economy (such as the construction of buildings, roads, and new machinery) and changes in inventories (stocks of goods held by firms). Rather than looking at exports and imports separately, we usually look at the difference between them (exports minus imports), also known as net exports. Choose three countries that have GDP data over the entire period (1970 to the latest year available). For each country, create a new row that shows the values of net exports in each year. Make sure to give each row an appropriate label. Now, we will create charts to show the GDP components in order to look for general patterns over time and make comparisons between countries. Evaluate the components over time, for two countries of your choice. Create a new row for each of the four components of GDP (Household consumption expenditure, General government final consumption expenditure, Gross capital formation, Net exports). To make the charts easier to read, convert the values into billions (for example, 4.38 billion instead of 4,378,772,008). Round your values to two decimal places. Plot a separate line chart for each country, showing the value of the four components of GDP on the vertical axis and time (the years 1970–Present) on the horizontal. (Use more than one line chart per country if necessary, to show the data more clearly). Name each component in the chart legend appropriately. Which of the components would you expect to move together (increasing or decreasing together) or move in opposite directions, and why? Using your charts from Question 3(b), describe any patterns you find in the relationship between the components. Does the data support your hypothesis about the behaviour of the components? For each country, describe any patterns you find in the movement of components over time. What factors could explain the patterns that you find within countries, and any differences between countries (for example, economic or political events)? You may find it helpful to research the history of the countries you have chosen. Extension: For one country, add data labels to your chart to indicate the relevant events that happened in that year. Excel walk-through 4.2 Adding data labels to a chart <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to add data labels to a chart. '' /> Figure 4.3 How to add data labels to a chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This example uses GDP data from Afghanistan. The data has been filtered so that only the components of interest are visible (you should filter your data so that the four components given in Question 3(a) are visible). Note: The numbers in your dataset may differ slightly if you are using the latest data. '' /> The data This example uses GDP data from Afghanistan. The data has been filtered so that only the components of interest are visible (you should filter your data so that the four components given in Question 3(a) are visible). Note: The numbers in your dataset may differ slightly if you are using the latest data. Figure 4.3a This example uses GDP data from Afghanistan. The data has been filtered so that only the components of interest are visible (you should filter your data so that the four components given in Question 3(a) are visible). Note: The numbers in your dataset may differ slightly if you are using the latest data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a line chart : After step 3, the line chart will look like the one shown above. '' /> Draw a line chart After step 3, the line chart will look like the one shown above. Figure 4.3b After step 3, the line chart will look like the one shown above. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add a data label to a particular point in the chart : Instead of adding data labels to every point in the chart, you can choose to label specific points only. After step 4, only the selected data point will be labelled. '' /> Add a data label to a particular point in the chart Instead of adding data labels to every point in the chart, you can choose to label specific points only. After step 4, only the selected data point will be labelled. Figure 4.3c Instead of adding data labels to every point in the chart, you can choose to label specific points only. After step 4, only the selected data point will be labelled. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-03-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the text in the label and reposition the label : By default, Excel will label data points with the horizontal axis and vertical axis values. You can change the label to text by clicking and typing into the box. '' /> Change the text in the label and reposition the label By default, Excel will label data points with the horizontal axis and vertical axis values. You can change the label to text by clicking and typing into the box. Figure 4.3d By default, Excel will label data points with the horizontal axis and vertical axis values. You can change the label to text by clicking and typing into the box. Another way to visualize the GDP data is to look at each component as a proportion of total GDP. Use the same countries that you chose for Question 3. time series dataA time series is a set of time-ordered observations of a variable taken at successive, in most cases regular, periods or points of time. Example: The population of a particular country in the years 1990, 1991, 1992, … , 2015 is time series data.cross-sectional dataData that is collected from participants at one point in time or within a relatively short time frame. In contrast, time series data refers to data collected by following an individual (or firm, country, etc.) over a course of time. Example: Data on degree courses taken by all the students in a particular university in 2016 is considered cross-sectional data. In contrast, data on degree courses taken by all students in a particular university from 1990 to 2016 is considered time series data. For each country, create a new row in Excel to show the sum of all four components (remember that this total may not add up to the reported GDP). Next, create a new row for each component, showing its proportion of total GDP (as a value ranging from 0 to 1), rounded to two decimal places. (Hint: to calculate the proportion of a component, divide the value of that component by the sum of all four components.) Plot a separate line chart for each country, showing the proportion of the component of GDP on the vertical axis and time (the years 1970 to the latest year available) on the horizontal axis. Describe any patterns in the proportion of spending over time for each country, and compare these patterns across countries. Compared to the charts in Question 3, what are some advantages of this method for making comparisons over time and between countries? So far, we have done comparisons of time series data, which is a collection of values for the same variables and subjects, taken at different points in time (for example, GDP of a particular country, measured each year). We will now make some charts using cross-sectional data, which is a collection of values for the same variables for different subjects, usually taken at the same time. Choose three developed countries, three countries in economic transition, and three developing countries (for a list of these countries, see Tables A-C in the UN country classification document). For each country, calculate each component as a proportion of GDP for the year 2015 only. (You may find it helpful to copy and paste the relevant data into a new Excel sheet.) Now create a stacked bar chart that shows the composition of GDP in 2015 on the horizontal axis, and country on the vertical axis. Arrange the columns so that the countries in a particular category are grouped together. (See the walk-through in Figure 3.8 of Economy, Society, and Public Policy.) Describe the differences (if any) between the spending patterns of developed, economic transition, and developing countries. GDP per capita is often used to indicate material wellbeing instead of GDP, because it accounts for differences in population across countries. Refer to the following articles to help you to answer the questions: ‘The economics of well-being’ in the Harvard Business Review ‘Statistical Insights: what does GDP per capita tell us about households’ material well-being?’ in the OECD Insights. Discuss the usefulness and limitations of GDP per capita as a measure of material wellbeing. Based on the arguments in the articles, do you think GDP per capita is an appropriate measure of both material wellbeing and overall wellbeing? Why or why not? Part 4.2 The HDI as a measure of wellbeing Learning objectives for this part sort data and assign ranks based on values distinguish between time series and cross sectional data, and plot appropriate charts for each type of data calculate the geometric mean and explain how it differs from the arithmetic mean construct indices using the geometric mean, and use index values to rank observations explain the difference between two measures of wellbeing (GDP per capita and the Human Development Index). In Part 4.1 we looked at GDP per capita as a measure of material wellbeing. While income has a major influence on wellbeing because it allows us to buy the goods and services we need or enjoy, it is not the only determinant of wellbeing. Many aspects of our wellbeing cannot be bought, for example, good health or having more time to spend with friends and family. We are now going to look at the Human Development Index (HDI), a measure of wellbeing that includes non-material aspects, and make comparisons with GDP per capita (a measure of material wellbeing). GDP per capita is a simple index calculated as the sum of its elements, whereas the HDI is more complex. Instead of using different types of expenditure or output to measure wellbeing or living standards, the HDI consists of three dimensions associated with wellbeing: a long and healthy life (health) knowledge (education) a decent standard of living (income). We will first learn about how the HDI is constructed, and then use this method to construct indices of wellbeing according to criteria of our choice. The HDI data we will look at is from the Human Development Report 2016 by the United Nations Development Programme (UNDP). To answer the questions below, download the data and technical notes from the report: Go to the UNDP’s website. Click ‘Table 1: Human Development Index and its components’ on the left-hand side of the page. Then right-click on the ‘Download Data’ box on the top right-hand side of the page and select ‘Save Link As…’. Save the file in an easily accessible location, and make sure to give it a suitable name. The ‘Technical notes’ give a diagrammatic presentation of how the HDI is constructed from four indicators. Refer to the technical notes and the spreadsheet you have downloaded. For each indicator, explain whether you think it is a good measure of the dimension, and suggest alternative indicators, if any. (For example, is GNI per capita a good measure of the dimension ‘a decent standard of living’?) Figure 4.4 shows the minimum and maximum values for each indicator. Discuss whether you think these are reasonable. (You can read the justification for these values in the technical notes.) Dimension Indicator Minimum Maximum Health Life expectancy (years) 20 85 Education Expected years of schooling (years) 0 18 Mean years of schooling (years) 0 15 Standard of living Gross national income per capita (2011 PPP $) 100 75,000 Maximum and minimum values for each indicator in the HDI. Figure 4.4 Maximum and minimum values for each indicator in the HDI. United Nations Development Programme. 2016. ‘Technical notes’ in Human Development Report 2016: p. 2. We are now going to apply the method for constructing the HDI, by recalculating the HDI from its indicators. We will use the formula below, and the minimum and maximum values in the table in Figure 4.4. These are taken from page 2 of the technical notes, which you can refer to for additional details. The HDI indicators are measured in different units and have different ranges, so in order to put them together into a meaningful index, we need to normalize the indicators using the following formula: Dimension index=actual value − minimum valuemaximum value − minimum valuetext{Dimension index} = frac{text{actual value − minimum value}}{text{maximum value − minimum value}} Doing so will give a value in between 0 and 1 (inclusive), which will allow comparison between different indicators. Refer to Figure 4.4 and calculate the dimension index for each of the dimensions in separate columns in the same spreadsheet tab as your data: Using the HDI indicator data in Column E, calculate the dimension index for a long and healthy life (health). Using the HDI indicator data in Columns G and I, calculate the dimension index for knowledge (education). Note that the knowledge dimension index is the average of the dimension index for expected years of schooling for those entering school, and mean years of schooling for adults aged 25 or older. Using the HDI indicator data in Column K, calculate the dimension index for a decent standard of living (income). Note (from the technical notes) that you should calculate the GNI index using the natural log of the values. (See the ‘Find out more’ box below for an explanation of the natural log and how to calculate it in Excel.) Find out more The natural log: What it means, and how to calculate it in Excel The natural log turns a linear variable into a concave variable, as shown in Figure 4.5. For any value of income on the horizontal axis, the natural log of that value on the vertical axis is smaller. At first, the difference between income and log income is not that big (for example, an income of 2 corresponds to a natural log of 0.7), but the difference becomes bigger as we move rightwards along the horizontal axis (for example, when income is 100,000, the natural log is only 11.5). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Comparing income with the natural logarithm of income. '' /> Comparing income with the natural logarithm of income. Figure 4.5 Comparing income with the natural logarithm of income. The reason why natural logs are useful in economics is because they can represent variables that have diminishing marginal returns: an additional unit of input results in a smaller increase in the total output than did the previous unit. (If you have studied production functions, then the shape of the natural log function might look familiar.) When applied to the concept of wellbeing, the ‘input’ is income, and the ‘output’ is material wellbeing. It makes intuitive sense that a $100 increase in per capita income will have a much greater effect on wellbeing for a poor country compared to a rich country. Using the natural log of income incorporates this notion into the index we create. Conversely, the notion of diminishing marginal returns (the larger the value of the input, the smaller the contribution of an additional unit of input) is not captured by GDP per capita, which uses actual income and not its natural log. Doing so makes the assumption that a $100 increase in per capita income has the same effect on wellbeing for rich and poor countries. Excel’s LN function calculates the natural log of a value for you. Simply type ‘=LN(’, then the number you would like to take the natural log of, then ‘)’ and press Enter. If you have a scientific calculator, you can check that the calculation is correct by using the ‘ln’ or ‘log’ key. geometric meanA summary measure calculated by multiplying N numbers together and then taking the Nth root of this product. The geometric mean is useful when the items being averaged have different scoring indices or scales, because it is not sensitive to these differences, unlike the arithmetic mean. For example, if education ranged from 0 to 20 years and life expectancy ranged from 0 to 85 years, life expectancy would have a bigger influence on the HDI than education if we used the arithmetic mean rather than the geometric mean. Conversely, the geometric mean treats each criteria equally. Example: Suppose we use life expectancy and mean years of schooling to construct an index of wellbeing. Country A has life expectancy of 40 years and a mean of 6 years of schooling. If we used the arithmetic mean to make an index, we would get (40 + 6)/2 = 23. If we used the geometric mean, we would get (40 × 6)1/2 = 15.5. Now suppose life expectancy doubled to 80 years. The arithmetic mean would be (80 + 6)/2 = 43, and the geometric mean would be (80 × 6)1/2 = 21.9. If, instead, mean years of schooling doubled to 12 years, the arithmetic mean would be (40 + 12)/2 = 26, and the geometric mean would be (40 × 12)1/2 = 21.9. This example shows that the arithmetic mean can be ‘unfair’ because proportional changes in one variable (life expectancy) have a larger influence over the index than changes in the other variable (years of schooling). The geometric mean gives each variable the same influence over the value of the index, so doubling the value of one variable would have the same effect on the index as doubling the value of another variable. Now that you know about the natural log, you might want to go back to Question 3(c) in Part 4.1, and create a new chart using the natural log scale. The natural log is used in economics because it approximates percentage changes i.e. log(x) – log(y) is a close approximation to the percentage change between x and y. So, using the natural log scale, you will be able to ‘read off’ the relative growth rates from the slopes of the different series you have plotted. For example, a 0.01 change in the vertical axis value corresponds to a 1% change in that variable. This will allow you to compare the growth rates of the different components of GDP. Now, we can combine these dimensional indices to give the HDI. The HDI is the geometric mean of the three dimension indices (IHealth = Life expectancy index, IEducation = Education index, and IIncome = GNI index): HDI =(IHealth×IEducation×IIncome)1/3text{HDI }=(text{I}_{text{Health}} times text{I}_{text{Education}} times text{I}_{text{Income}})^{1/3} Use the formula above and the data in the spreadsheet to calculate the HDI for all the countries excluding those in the ‘Other countries or territories’ category. You should get the same values as those in Column C, rounded to 3 decimal places. Excel walk-through 4.3 Calculating the HDI <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to calculate the HDI. '' /> Figure 4.6 How to calculate the HDI. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : The HDI data looks like this. The HDI value for each country is in Column C, and its four components are in Columns E, G, I, and K. Note: If you are using the latest data, the numbers in your dataset may differ slightly, and your spreadsheet may only have one tab. '' /> The data The HDI data looks like this. The HDI value for each country is in Column C, and its four components are in Columns E, G, I, and K. Note: If you are using the latest data, the numbers in your dataset may differ slightly, and your spreadsheet may only have one tab. Figure 4.6a The HDI data looks like this. The HDI value for each country is in Column C, and its four components are in Columns E, G, I, and K. Note: If you are using the latest data, the numbers in your dataset may differ slightly, and your spreadsheet may only have one tab. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate dimension indices of each component. : First, we will calculate the dimension indices of each component of the HDI, and store this information in Columns Q to T, as shown above. '' /> Calculate dimension indices of each component. First, we will calculate the dimension indices of each component of the HDI, and store this information in Columns Q to T, as shown above. Figure 4.6b First, we will calculate the dimension indices of each component of the HDI, and store this information in Columns Q to T, as shown above. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate dimension indices of each component : After step 4, you will have dimension indices for each component. Make sure to use the LN function to calculate the dimension index for income. '' /> Calculate dimension indices of each component After step 4, you will have dimension indices for each component. Make sure to use the LN function to calculate the dimension index for income. Figure 4.6c After step 4, you will have dimension indices for each component. Make sure to use the LN function to calculate the dimension index for income. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the index value for education : To calculate the HDI, we need the index value for education, which we get by taking the average of the two dimension indices for schooling. '' /> Calculate the index value for education To calculate the HDI, we need the index value for education, which we get by taking the average of the two dimension indices for schooling. Figure 4.6d To calculate the HDI, we need the index value for education, which we get by taking the average of the two dimension indices for schooling. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the HDI : After step 6, you will have the correct HDI value for most countries. For the other countries, we need to recalculate the dimension index for expected years of schooling, in order to get the correct value. '' /> Calculate the HDI After step 6, you will have the correct HDI value for most countries. For the other countries, we need to recalculate the dimension index for expected years of schooling, in order to get the correct value. Figure 4.6e After step 6, you will have the correct HDI value for most countries. For the other countries, we need to recalculate the dimension index for expected years of schooling, in order to get the correct value. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Amend the formula for expected years of schooling to give the correct HDI values : The problem with some countries is that expected years of schooling is greater than the value chosen as the maximum (18). In these cases, replacing the dimension index with 1 solves the problem. '' /> Amend the formula for expected years of schooling to give the correct HDI values The problem with some countries is that expected years of schooling is greater than the value chosen as the maximum (18). In these cases, replacing the dimension index with 1 solves the problem. Figure 4.6f The problem with some countries is that expected years of schooling is greater than the value chosen as the maximum (18). In these cases, replacing the dimension index with 1 solves the problem. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Round the calculated values to 3 decimal places : To make the HDI values easier to read, we will round them to 3 decimal places, as was done in Column C. '' /> Round the calculated values to 3 decimal places To make the HDI values easier to read, we will round them to 3 decimal places, as was done in Column C. Figure 4.6g To make the HDI values easier to read, we will round them to 3 decimal places, as was done in Column C. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-h.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-h-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-h-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-h-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-06-h.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Round the calculated values to 3 decimal places : After step 10, your HDI values should look the same as those in Column C. '' /> Round the calculated values to 3 decimal places After step 10, your HDI values should look the same as those in Column C. Figure 4.6h After step 10, your HDI values should look the same as those in Column C. The HDI is one way to measure wellbeing, but you may think that it does not use the most appropriate measures for the non-material aspects of wellbeing (health and education). Now we will use the same method to create our own index of non-material wellbeing (an ‘alternative HDI’), using different indicators. You can find alternative indicators to measure health and education on the UNDP website, in the file ‘Download 2018 Human Development Data Bank’ (left-hand side of the page). (The first column of the spreadsheet, ‘dimension’, tells you whether the indicators refer to ‘Health’, ‘Education’, or other categories). Create an alternative index of wellbeing: Choose two to three indicators to measure health, and two to three indicators to measure education. Explain why you have chosen these indicators. Carefully copy and paste the column(s) containing these indicator values into new columns of your spreadsheet from Question 4 (containing your HDI calculations), making sure to match the data to the correct country. Choose a reasonable maximum and minimum value for each indicator and justify your choices. Using the indicators and maximum and minimum values you have chosen, calculate the alternative HDI as done in Questions 3 and 4. Remember to include the existing income index from Question 2. Since you have chosen more than one indicator per dimension, make sure to average the dimension indices as done in Question 3(b). Also ensure that higher indicator values always represent better outcomes. Create a new column showing each country’s rank according to your alternative HDI, where 1 is assigned to the country with the highest value. Compare your ranking to the HDI rank (Column A). Are the rankings generally similar, or very different? (See Excel walk-through 4.4 on how to do this.) Excel walk-through 4.4 Ranking data Follow the walk-through in the CORE video, or in Figure 4.7, to find out how to rank data in Excel. How to rank data <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to use Excel’s RANK function. '' /> Figure 4.7 How to use Excel’s RANK function. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : In this example, the created index is in Column Q (titled ‘Own index’) and was calculated as the dimension index of Life expectancy at birth. Note: If you are using the latest data, the numbers in your dataset may differ slightly, and your spreadsheet may only have one tab. '' /> The data In this example, the created index is in Column Q (titled ‘Own index’) and was calculated as the dimension index of Life expectancy at birth. Note: If you are using the latest data, the numbers in your dataset may differ slightly, and your spreadsheet may only have one tab. Figure 4.7a In this example, the created index is in Column Q (titled ‘Own index’) and was calculated as the dimension index of Life expectancy at birth. Note: If you are using the latest data, the numbers in your dataset may differ slightly, and your spreadsheet may only have one tab. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Rank the index values from largest to smallest : We will rank the values of our own index from largest to smallest, and store this information in Column R. '' /> Rank the index values from largest to smallest We will rank the values of our own index from largest to smallest, and store this information in Column R. Figure 4.7b We will rank the values of our own index from largest to smallest, and store this information in Column R. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-07-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Compare the rankings with the HDI index rankings : There are many ways to compare your ranking with the HDI ranking; two ways are shown here. '' /> Compare the rankings with the HDI index rankings There are many ways to compare your ranking with the HDI ranking; two ways are shown here. Figure 4.7c There are many ways to compare your ranking with the HDI ranking; two ways are shown here. Compare your alternative index to the HDI: The UN classifies countries into four groups depending on their HDI, as shown in Figure 4.8. Would the classification of any country change under your alternative HDI? Based on your answers to Questions 5(d) and 6(a), do you think that the HDI is a robust measure of non-material wellbeing? (In other words, does changing the indicators used in the HDI give similar conclusions about the non-material wellbeing of countries?) Classification HDI Very high human development 0.800 and above High human development 0.700–0.799 Medium human development 0.550–0.699 Low human development Below 0.550 Classification of countries according to their HDI value. Figure 4.8 Classification of countries according to their HDI value. United Nations Development Programme. 2016. ‘Technical notes’ in Human Development Report 2016: p.3. We will now investigate whether HDI and GDP per capita give similar information about overall wellbeing, by comparing a country’s rank in both measures. The spreadsheet you downloaded for Question 5 (containing alternative indicators) also has information on GDP per capita, measured in 2011 constant prices in US dollars. This indicator is called ‘GDP per capita (2011 PPP $)’ in Column C of that spreadsheet. To answer Question 7, first filter the data so that only rows containing this indicator are visible, then copy and paste this data into a new column in the spreadsheet containing your HDI calculations, making sure to match the data to the correct country. Evaluate GDP per capita and the HDI as measures of overall wellbeing: Create a new column showing each country’s rank according to GDP per capita, where 1 is assigned to the country with the highest value. Show this rank on a scatterplot, with GDP per capita rank on the vertical axis and HDI rank on the horizontal axis. (See Excel walk-through 1.7 for a step-by-step explanation of how to create scatterplots in Excel.) Does there appear to be a strong correlation between these two variables? Give an explanation for the observed pattern in your chart. Create a table similar to Figure 4.9. Using your answers to Question 7(a), fill each box with three countries. You can use the UN’s definition of ‘high’ for the HDI, as in Figure 4.8, and choose a reasonable definition of ‘high’ for GDP. Based on this table, which country or countries would you prefer to grow up in, and why? Explain the differences between HDI and GDP as measures of wellbeing. You may want to consider the way each measure includes income in its calculation (the actual value or a transformation), and the inclusion of other aspects of wellbeing. HDI Low High GDP Low High Classification of countries according to their HDI and GDP values. Figure 4.9 Classification of countries according to their HDI and GDP values. The HDI is one way to measure wellbeing, but there are many other ways to measure wellbeing. What are the strengths and limitations of the HDI as a measure of wellbeing? Find some alternative measures of non-material wellbeing that we could use alongside the HDI to provide a more comprehensive picture of wellbeing. For each measure, evaluate the elements used to construct the measure, and discuss the additional information we can learn from it. (You may find it helpful to read Our World in Data’s page on happiness and life satisfaction, particularly the section ‘Correlates, Determinants, and Consequences’.)"
});
index.addDoc({
    id: 24,
    title: "Doing Economics: Empirical Project 4: Working in R",
    content: "Empirical Project 4 Working in R Download the code To download the code chunks used in this project, right-click on the download link and select ‘Save Link As…’. You’ll need to save the code download to your working directory, and open it in RStudio. Don’t forget to also download the data into your working directory by following the steps in this project. R-specific learning objectives In addition to the learning objectives for this project, in this section you will learn how to convert (‘reshape’) data from wide to long format and vice versa. Getting started in R For this project you will need the following packages: tidyverse, to help with data manipulation readxl, to import an Excel spreadsheet reshape2, to manipulate datasets. You will also use the ggplot2 package to produce accurate graphs, but that comes as part of the tidyverse package. If you need to install these packages, run the following code: install.packages(c(''readxl'', ''tidyverse'', ''reshape2'')) You can import these libraries now, or when they are used in the R walk-throughs below. library(readxl) library(tidyverse) library(reshape2) Part 4.1 GDP and its components as a measure of material wellbeing Learning objectives for this part check datasets for missing data sort data and assign ranks based on values distinguish between time series and cross sectional data, and plot appropriate charts for each type of data. The GDP data we will look at is from the United Nations’ National Accounts Main Aggregates Database, which contains estimates of total GDP and its components for all countries over the period 1970 to present. We will look at how GDP and its components have changed over time, and investigate the usefulness of GDP per capita as a measure of wellbeing. To answer the questions below, download the data and make sure you understand how the measure of total GDP is constructed. Go to the United Nations’ National Accounts Main Aggregates Database website. On the right-hand side of the page, under ‘Data Availability’, click ‘Downloads’. Under the subheading ‘GDP and its breakdown at constant 2010 prices in US Dollars’, select the Excel file ‘All countries for all years – sorted alphabetically’. Save it in an easily accessible location, such as a folder on your Desktop or in your personal folder. R walk-through 4.1 Importing the Excel file (.xlsx or .xls format) into R First, use setwd to tell R where the datafile is stored. To avoid having to repeatedly use setwd to tell R where your files are, keep all the files you need in that folder, including the Excel sheet you just downloaded. Replace ‘YOURFILEPATH’ with the full filepath which points to the folder with your datafile. If you don’t know how to find the path to that folder, see the ‘Technical Reference’ section. setwd(''YOURFILEPATH'') Then use the function readxl (part of the tidyverse suite of packages) to import the datafile. Before importing the file into R, open the file in Excel to see how the data is organized in the spreadsheet, and note that: There is a heading that we don’t need, followed by a blank row. The data we need starts on row three. # Load the library library(tidyverse) library(readxl) # Excel filename UN = read_excel(''Download-GDPconstant-USD-countries.xls'', # Sheet name sheet = ''Download-GDPconstant-USD-countr'', # Number of rows to skip skip = 2) head(UN) ## # A tibble: 6 x 50 ## CountryID Country IndicatorName `1970` `1971` `1972` `1973` `1974` ## <dbl> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> ## 1 4 Afghani~ Final consumption~ 5.56e9 5.33e9 5.20e9 5.75e9 6.15e9 ## 2 4 Afghani~ Household consump~ 5.07e9 4.84e9 4.70e9 5.21e9 5.59e9 ## 3 4 Afghani~ General governmen~ 3.72e8 3.82e8 4.02e8 4.21e8 4.31e8 ## 4 4 Afghani~ Gross capital for~ 9.85e8 1.05e9 9.19e8 9.19e8 1.18e9 ## 5 4 Afghani~ Gross fixed capit~ 9.85e8 1.05e9 9.19e8 9.19e8 1.18e9 ## 6 4 Afghani~ Exports of goods ~ 1.12e8 1.45e8 1.73e8 2.18e8 3.00e8 ## # ... with 42 more variables: `1975` <dbl>, `1976` <dbl>, `1977` <dbl>, ## # `1978` <dbl>, `1979` <dbl>, `1980` <dbl>, `1981` <dbl>, `1982` <dbl>, ## # `1983` <dbl>, `1984` <dbl>, `1985` <dbl>, `1986` <dbl>, `1987` <dbl>, ## # `1988` <dbl>, `1989` <dbl>, `1990` <dbl>, `1991` <dbl>, `1992` <dbl>, ## # `1993` <dbl>, `1994` <dbl>, `1995` <dbl>, `1996` <dbl>, `1997` <dbl>, ## # `1998` <dbl>, `1999` <dbl>, `2000` <dbl>, `2001` <dbl>, `2002` <dbl>, ## # `2003` <dbl>, `2004` <dbl>, `2005` <dbl>, `2006` <dbl>, `2007` <dbl>, ## # `2008` <dbl>, `2009` <dbl>, `2010` <dbl>, `2011` <dbl>, `2012` <dbl>, ## # `2013` <dbl>, `2014` <dbl>, `2015` <dbl>, `2016` <dbl> You can see from the tab ‘Download-GDPconstant-USD-countr’ that some countries have missing data for some of the years. Data may be missing due to political reasons (for example, countries formed after 1970) or data availability issues. Make and fill a frequency table similar to Figure 4.1, showing the number of years that data is available for each country in the category ‘Final consumption expenditure’. How many countries have data for the entire period (1970 to the latest year available)? Do you think that missing data is a serious issue in this case? Country Number of years of GDP data Number of years of GDP data available for each country. Figure 4.1 Number of years of GDP data available for each country. R walk-through 4.2 Making a frequency table We want to create a table showing how many years of Final consumption expenditure data are available for each country. Looking at the dataset’s current format, you can see that countries and indicators (for example, Afghanistan and Final consumption expenditure) are row variables, while year is the column variable. This data is organized in ‘wide’ format (each individual’s information is in a single row). For many data operations and making charts it is more convenient to have indicators as column variables, so we would like Final consumption expenditure to be a column variable, and year to be the row variable. Each observation would represent the value of an indicator for a particular country and year. This data is organized in ‘long’ format (each individual’s information is in multiple rows). To change data from wide to long format, we use the melt command from the package reshape2. The melt command is very powerful and useful, as you will find many large datasets are in wide format. In this case, it takes the data in Column 4 to the last column (these columns indicate the years) and uses them to create two new columns: one column (variable) contains the name of the row variable (the year) and the other column (value) contains the associated value. Compare long_UN to wide_UN to understand how the melt command works. To learn more about organizing data in R, see the R for Data Science website. library(reshape2) wide_UN <- UN # Keep all data except for column 1 (CountryID) wide_UN = wide_UN[, -1] # id.vars are the names of the column variables. long_UN = melt(wide_UN, id.vars = c(''Country'', ''IndicatorName''), value.vars = 4:ncol(UN)) head(long_UN) ## Country ## 1 Afghanistan ## 2 Afghanistan ## 3 Afghanistan ## 4 Afghanistan ## 5 Afghanistan ## 6 Afghanistan ## IndicatorName ## 1 Final consumption expenditure ## 2 Household consumption expenditure (including Non-profit institutions serving households) ## 3 General government final consumption expenditure ## 4 Gross capital formation ## 5 Gross fixed capital formation (including Acquisitions less disposals of valuables) ## 6 Exports of goods and services ## variable value ## 1 1970 5559066266 ## 2 1970 5065088737 ## 3 1970 372478456 ## 4 1970 984580895 ## 5 1970 984580895 ## 6 1970 112390156 Our new ‘long’ format dataset is called long_UN. During the reshaping process, a new variable called variable was created which contains years. We will use the names function to rename it as Year. names(long_UN)[names(long_UN) == ''variable''] <- ''Year'' To create the required table, we only need Final consumption expenditure of each country, which we extract using the subset function. cons = subset(long_UN, IndicatorName == ''Final consumption expenditure'') Now we create the table showing the number of missing years by country, using the piping operator (%>%) from the tidyverse package. This operator allows us to perform multiple functions, one after another. # Use the pipe operator (%>%) from the tidyverse package. # This means: use the result of the current line # as the first argument in the next line's function. missing_by_country = cons %>% group_by(Country) %>% summarize(available_years=sum(!is.na(value))) %>% print() ## # A tibble: 220 x 2 ## Country available_years ## <chr> <int> ## 1 Afghanistan 47 ## 2 Albania 47 ## 3 Algeria 47 ## 4 Andorra 47 ## 5 Angola 47 ## 6 Anguilla 47 ## 7 Antigua and Barbuda 47 ## 8 Argentina 47 ## 9 Armenia 27 ## 10 Aruba 47 ## # ... with 210 more rows Translating the code in words: Take the variable cons (cons %>%) and group the observations by country (group_by(Country)), then take this result (%>%) and produce a table (summarize(...)) that shows the variable available_years (which is the sum (sum(...)) of the variable !is.na(value)). To understand what !is.na(value) means, recall that value contains the numerical values for the variable of interest. When an observation is missing, it is recorded as NA. The function is.na(value) will return a value of 1 (or TRUE) if the value is missing and 0 (or FALSE) otherwise. We add a ! in front since we want the function to return a 1 if the observation exists and a 0 otherwise. For R, ! means ‘not’ so we get a 1 if the particular observation is not missing. Now we can establish how many of the 220 countries in the dataset have complete information. A dataset is complete if it has the maximum number of available observations (max(missing_by_country$available_years)). sum(missing_by_country$available_years == max( missing_by_country$available_years)) ## [1] 179 If you add up the data on the right-hand side of this equation, you may find that it does not add up to the reported GDP value. The UN notes this discrepancy in Section J, item 17 of the ‘Methodology for the national accounts’: ‘The sums of com­ponents in the tables may not necessarily add up to totals shown because of rounding’. There are three different ways in which countries calculate GDP for their national accounts, but we will focus on the expenditure approach, which calculates gross domestic product (GDP) as: % <![CDATA[ begin{align*} text{GDP} &= text{Household consumption expenditure}  &+ text{General government final consumption expenditure}  &+ text{Gross capital formation}  &+ text{(Exports of goods and services − imports of goods and services)} end{align*} %]]> Gross capital formation refers to the creation of fixed assets in the economy (such as the construction of buildings, roads, and new machinery) and changes in inventories (stocks of goods held by firms). Rather than looking at exports and imports separately, we usually look at the difference between them (exports minus imports), also known as net exports. Choose three countries that have GDP data over the entire period (1970 to the latest year available). For each country, create a variable that shows the values of net exports in each year. R walk-through 4.3 Creating new variables We will use Brazil, the US, and China as examples. Before we select these three countries, we will calculate the net exports (exports minus imports) for all countries, as we need that information in R walk-through 4.4. We will also shorten the names of the variables we need, to make the code easier to read. # Shorten the names of the variables we need # When a string straddles two lines of code # we need to wrap it into the 'strwrap' function long_UN$IndicatorName[long_UN$IndicatorName == strwrap(''Household consumption expenditure (including Non-profit institutions serving households)'')] <- ''HH.Expenditure'' long_UN$IndicatorName[long_UN$IndicatorName == ''General government final consumption expenditure''] <- ''Gov.Expenditure'' long_UN$IndicatorName[long_UN$IndicatorName == ''Final consumption expenditure''] <- ''Final.Expenditure'' long_UN$IndicatorName[long_UN$IndicatorName == ''Gross capital formation''] <- ''Capital'' long_UN$IndicatorName[long_UN$IndicatorName == ''Imports of goods and services''] <- ''Imports'' long_UN$IndicatorName[long_UN$IndicatorName == ''Exports of goods and services''] <- ''Exports'' long_UN still has several rows for a particular country and year (one for each indicator). We will reshape this data using the dcast function to ensure that we have only one row per country and per year. We then add a new column called Net.Exports containing the calculated net exports. # We need to cast (reshape) the long_UN data to a dataframe # We use the dcast function (used for dataframes) table_UN <- dcast(long_UN, Country + Year ~ IndicatorName) # Add a new column for net exports (= exports – imports) table_UN$Net.Exports <- table_UN[, ''Exports'']-table_UN[, ''Imports''] Let us select our three chosen countries to check that we calculated net exports correctly. sel_countries = c(''Brazil'', ''United States'', ''China'') # Using our long format dataset, we get imports, exports, # and year for these countries. sel_UN1 = subset(table_UN, subset = (Country %in% sel_countries), select = c(''Country'', ''Year'', ''Exports'', ''Imports'', ''Net.Exports'')) head(sel_UN1) ## Country Year Exports Imports Net.Exports ## 1223 Brazil 1970 12337240060 20187929130 -7850689070 ## 1224 Brazil 1971 13016975734 24162976191 -11146000457 ## 1225 Brazil 1972 16162334455 29025977711 -12863643256 ## 1226 Brazil 1973 18466228366 34950588132 -16484359766 ## 1227 Brazil 1974 18897150611 44821362355 -25924211744 ## 1228 Brazil 1975 21084064715 42838470795 -21754406080 Now we will create charts to show the GDP components in order to look for general patterns over time and make comparisons between countries. Evaluate the components over time, for two countries of your choice. Create a new row for each of the four components of GDP (Household consumption expenditure, General government final consumption expenditure, Gross capital formation, Net exports). To make the charts easier to read, convert the values into billions (for example, 4.38 billion instead of 4,378,772,008). Round your values to two decimal places. Plot a separate line chart for each country, showing the value of the four components of GDP on the vertical axis and time (the years 1970–Present) on the horizontal. (Use more than one line chart per country if necessary, to show the data more clearly). Name each component in the chart legend appropriately. Which of the components would you expect to move together (increasing or decreasing together) or move in opposite directions, and why? Using your charts from Question 3(a), describe any patterns you find in the relationship between the components. Does the data support your hypothesis about the behaviour of the components? For each country, describe any patterns you find in the movement of components over time. What factors could explain the patterns that you find within countries, and any differences between countries (for example, economic or political events)? You may find it helpful to research the history of the countries you have chosen. Extension: For one country, add data labels to your chart to indicate the relevant events that happened in that year. R walk-through 4.4 Plotting and annotating time series data Extract the relevant data We will work with the long_UN dataset, as the long format is well suited to produce charts with the ggplot package. In this example, we use the US and China (saved as the dataset comp). # Select our chosen countries comp = subset(long_UN, Country %in% c(''United States'', ''China'')) # value in billion of USD comp$value = comp$value / 1e9 comp = subset(comp, select = c(''Country'', ''Year'', ''IndicatorName'', ''value''), subset = IndicatorName %in% c(''Gov.Expenditure'', ''HH.Expenditure'', ''Capital'', ''Imports'', ''Exports'')) Plot a line chart We can now plot this data using the ggplot library. library(ggplot2) # ggplot allows us to build a chart step-by-step. pl = ggplot(subset(comp, Country == ''United States''), # Base chart, defining x (horizontal) and y (vertical) # axis variables aes(x = Year, y = value)) # Specify a line chart, with a different colour for each # indicator name and line size = 1 pl = pl + geom_line(aes(group = IndicatorName, color = IndicatorName), size = 1) # Display the chart pl <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-02.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-02-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-02-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-02-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-02.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The US’s GDP components (expenditure approach). '' /> The US’s GDP components (expenditure approach). Figure 4.2 The US’s GDP components (expenditure approach). There are plenty of problems with this chart: we cannot read the horizontal axis, because it labels every year the vertical axis label is uninformative there is no chart title the grey (default) background makes the chart difficult to read the legend is uninformative. To improve this chart, we add features to the already existing figure pl. pl = pl + scale_x_discrete(breaks=seq(1970, 2016, by = 10)) pl = pl + scale_y_continuous(name=''Billion US$'') pl = pl + ggtitle(''GDP components over time'') # Change the legend title and labels pl = pl + scale_colour_discrete(name = ''Components of GDP'', labels = c(''Gross capital formation'', ''Exports'', ''Government expenditure'', ''Household expenditure'', ''Imports'')) pl = pl + theme_bw() pl = pl + annotate(''text'', x = 37, y = 850, label = ''Great Recession'') pl <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The US’s GDP components (expenditure approach), amended chart. '' /> The US’s GDP components (expenditure approach), amended chart. Figure 4.3 The US’s GDP components (expenditure approach), amended chart. We can make a chart for more than one country simultaneously by repeating the code above, but without subsetting the data: # Repeat all steps without subsetting the data # Base line chart pl = ggplot(comp, aes(x = Year, y = value, color = IndicatorName)) pl = pl + geom_line(aes(group = IndicatorName), size = 1) pl = pl + scale_x_discrete( breaks = seq(1970, 2016, by = 10)) pl = pl + scale_y_continuous(name = ''Billion US$'') pl = pl + ggtitle(''GDP components over time'') pl = pl + scale_colour_discrete(name = ''Component'') pl = pl + theme_bw() # Make a separate chart for each country pl = pl + facet_wrap(~Country) pl = pl + scale_colour_discrete( name = ''Components of GDP'', labels = c(''Gross capital formation'', ''Exports'', ''Government expenditure'', ''Household expenditure'', ''Imports'')) pl <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''GDP components over time (China and the US). '' /> GDP components over time (China and the US). Figure 4.4 GDP components over time (China and the US). Another way to visualize the GDP data is to look at each component as a proportion of total GDP. Use the same countries that you chose for Question 3. For each country used in Question 3, create a new column showing the proportion of each component of total GDP (in other words, as a value ranging from 0 to 1). (Hint: to calculate the proportion of a component, divide the value of that component by the sum of all four components.) Plot a separate line chart for each country, showing the proportion of the component of GDP on the vertical axis and time (the years 1970 to the latest year available) on the horizontal axis. Describe any patterns in the proportion of spending over time for each country, and compare these patterns across countries. Compared to the charts in Question 3, what are some advantages of this method for making comparisons over time and between countries? R walk-through 4.5 Calculating new variables and plotting time series data Calculate proportion of total GDP We will use the comp dataset created in R walk-through 4.4. First we will calculate net exports, as that contributes to GDP. As the data is currently in long format, we will reshape the data into wide format so that the variables we need are in separate columns instead of separate rows (using the dcast function, as in R walk-through 4.3), calculate net exports, then transform the data back into long format using the melt function. # Reshape the data to wide format (indicators in columns) comp_wide <- dcast(comp, Country + Year ~ IndicatorName) head(comp_wide) ## Country Year Capital Exports Gov.Expenditure HH.Expenditure Imports ## 1 China 1970 67.58221 5.305242 19.30034 107.8411 6.119649 ## 2 China 1971 73.79977 6.318662 22.63929 112.3586 6.094361 ## 3 China 1972 70.62638 7.740001 23.77126 118.3906 7.510857 ## 4 China 1973 80.79658 10.810644 24.34177 126.7546 11.540975 ## 5 China 1974 83.38207 12.856408 26.14306 129.3434 16.041397 ## 6 China 1975 93.47130 13.309628 27.29335 134.5575 15.859461 # Add the new column for net exports = exports – imports comp_wide$Net.Exports <- comp_wide[, ''Exports''] - comp_wide[, ''Imports''] head(comp_wide) ## Country Year Capital Exports Gov.Expenditure HH.Expenditure Imports ## 1 China 1970 67.58221 5.305242 19.30034 107.8411 6.119649 ## 2 China 1971 73.79977 6.318662 22.63929 112.3586 6.094361 ## 3 China 1972 70.62638 7.740001 23.77126 118.3906 7.510857 ## 4 China 1973 80.79658 10.810644 24.34177 126.7546 11.540975 ## 5 China 1974 83.38207 12.856408 26.14306 129.3434 16.041397 ## 6 China 1975 93.47130 13.309628 27.29335 134.5575 15.859461 ## Net.Exports ## 1 -0.8144069 ## 2 0.2243011 ## 3 0.2291447 ## 4 -0.7303314 ## 5 -3.1849891 ## 6 -2.5498330 # Return to long format with the HH.expenditure, Capital, and Net Export variables comp2_wide <- subset(comp_wide, select = -c(Exports, Imports)) comp2 <- melt(comp2_wide, id.vars = c(''Year'', ''Country'')) Now we will create a new dataframe (props) also containing the proportions for each GDP component (proportion), using the piping operator to link functions together. props = comp2 %>% group_by(Country, Year) %>% mutate(proportion = value / sum(value)) In words, we did the following: Take the comp2 dataframe and create groups by country and year (for example, all indicators for China in 1970). Then create a new variable (mutate) called proportion, which divides the variable value of an indicator by the sum of all value for that group (for example, all indicators for China in 1970). The result is then saved in props. Look at the props dataframe to confirm that the above command has achieved the desired result. Plot a line chart Now we redo the line chart from R walk-through 4.4 using the variable props. # Base line chart pl = ggplot(props, aes(x = Year, y = proportion, color = variable)) pl = pl + geom_line(aes(group = variable), size = 1) pl = pl + scale_x_discrete(breaks = seq(1970, 2016, by = 10)) pl = pl + ggtitle(''GDP component proportions over time'') pl = pl + theme_bw() # Make a separate chart for each country pl = pl + facet_wrap(~Country) pl = pl + scale_colour_discrete( name = ''Components of GDP'', labels = c(''Gross capital formation'', ''Government expenditure'', ''Household expenditure'', ''Net Exports'')) pl time series dataA time series is a set of time-ordered observations of a variable taken at successive, in most cases regular, periods or points of time. Example: The population of a particular country in the years 1990, 1991, 1992, … , 2015 is time series data.cross-sectional dataData that is collected from participants at one point in time or within a relatively short time frame. In contrast, time series data refers to data collected by following an individual (or firm, country, etc.) over a course of time. Example: Data on degree courses taken by all the students in a particular university in 2016 is considered cross-sectional data. In contrast, data on degree courses taken by all students in a particular university from 1990 to 2016 is considered time series data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''GDP component proportions over time (China and the US). '' /> GDP component proportions over time (China and the US). Figure 4.5 GDP component proportions over time (China and the US). So far, we have done comparisons of time series data, which is a collection of values for the same variables and subjects, taken at different points in time (for example, GDP of a particular country, measured each year). We will now make some charts using cross-sectional data, which is a collection of values for the same variables for different subjects, usually taken at the same time. Choose three developed countries, three countries in economic transition, and three developing countries (for a list of these countries, see Tables A–C in the UN country classification document). For each country, calculate each component as a proportion of GDP for the year 2015 only. Now create a stacked bar chart that shows the composition of GDP in 2015 on the horizontal axis, and country on the vertical axis. Arrange the columns so that the countries in a particular category are grouped together. (See the walk-through in Figure 3.8 of Economy, Society, and Public Policy for an example of what your chart should look like.) Describe the differences (if any) between the spending patterns of developed, economic transition, and developing countries. R walk-through 4.6 Creating stacked bar charts Calculate proportion of total GDP This walk-through uses the following countries (chosen at random): developed countries: Germany, Japan, United States transition countries: Albania, Russian Federation, Ukraine developing countries: Brazil, China, India. The relevant data are still in the table_UN dataframe. Before we select these countries, we first calculate the required proportions for all countries. # Calculate proportions table_UN$p_Capital <- table_UN$Capital / (table_UN$Capital + table_UN$Final.Expenditure + table_UN$Net.Exports) table_UN$p_FinalExp <- table_UN$Final.Expenditure / (table_UN$Capital + table_UN$Final.Expenditure + table_UN$Net.Exports) table_UN$p_NetExports <- table_UN$Net.Exports / (table_UN$Capital + table_UN$Final.Expenditure + table_UN$Net.Exports) sel_countries <- c(''Germany'', ''Japan'', ''United States'', ''Albania'', ''Russian Federation'', ''Ukraine'', ''Brazil'', ''China'', ''India'') # Using our long format dataset, we select imports, # exports, and year for our chosen countries in 2015. # Select the columns we need sel_2015 <- subset(table_UN, subset = (Country %in% sel_countries) & (Year == 2015), select = c(''Country'', ''Year'', ''p_FinalExp'', ''p_Capital'', ''p_NetExports'')) Plot a stacked bar chart Now let’s create the bar chart. # Reshape the table into long format, then use ggplot sel_2015_m <- melt(sel_2015, id.vars = c(''Year'', ''Country'')) g <- ggplot(sel_2015_m, aes(x = Country, y = value, fill = variable)) + geom_bar(stat = ''identity'') + coord_flip() + ggtitle(''GDP component proportions in 2015'') + scale_fill_discrete(name = ''Components of GDP'', labels = c(''Final expenditure'', ''Gross capital formation'', ''Net Exports'')) + theme_bw() plot(g) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''GDP component proportions in 2015. '' /> GDP component proportions in 2015. Figure 4.6 GDP component proportions in 2015. Note that even when a country has a trade deficit (proportion of net exports < 0), the proportions will add up to 1, but the proportions of final expenditure and capital will add up to more than 1. We have not yet ordered the countries so that they form the pre-specified groups. To achieve this, we need to explicitly impose an ordering on the Country variable using the factor function. The countries will be ordered in the same order we used to define sel_countries. # Impose the order in the sel_countries object, then use ggplot sel_2015_m$Country <- factor(sel_2015_m$Country, levels = sel_countries) g <- ggplot(sel_2015_m, aes(x = Country, y = value, fill = variable)) + geom_bar(stat = ''identity'') + coord_flip() + ggtitle(''GDP component proportions in 2015 (ordered)'') + scale_fill_discrete(name = ''Components of GDP'', labels = c(''Final expenditure'', ''Gross capital formation'', ''Net Exports'')) + labels = c(''Final expenditure'', ''Gross capital formation'', ''Net Exports'')) + theme_bw() plot(g) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''GDP component proportions in 2015 (ordered). '' /> GDP component proportions in 2015 (ordered). Figure 4.7 GDP component proportions in 2015 (ordered). GDP per capita is often used to indicate material wellbeing instead of GDP, because it accounts for differences in population across countries. Refer to the following articles to help you to answer the questions: ‘The Economics of Well-being’ in the Harvard Business Review ‘Statistical Insights: What does GDP per capita tell us about households’ material well-being?’ in the OECD Insights. Discuss the usefulness and limitations of GDP per capita as a measure of material wellbeing. Based on the arguments in the articles, do you think GDP per capita is an appropriate measure of both material wellbeing and overall wellbeing? Why or why not? Part 4.2 The HDI as a measure of wellbeing Learning objectives for this part sort data and assign ranks based on values distinguish between time series and cross sectional data, and plot appropriate charts for each type of data calculate the geometric mean and explain how it differs from the arithmetic mean construct indices using the geometric mean, and use index values to rank observations explain the difference between two measures of wellbeing (GDP per capita and the Human Development Index). In Part 4.1 we looked at GDP per capita as a measure of material wellbeing. While income has a major influence on wellbeing because it allows us to buy the goods and services we need or enjoy, it is not the only determinant of wellbeing. Many aspects of our wellbeing cannot be bought, for example, good health or having more time to spend with friends and family. We are now going to look at the Human Development Index (HDI), a measure of wellbeing that includes non-material aspects, and make comparisons with GDP per capita (a measure of material wellbeing). GDP per capita is a simple index calculated as the sum of its elements, whereas the HDI is more complex. Instead of using different types of expenditure or output to measure wellbeing or living standards, the HDI consists of three dimensions associated with wellbeing: a long and healthy life (health) knowledge (education) a decent standard of living (income). We will first learn about how the HDI is constructed, and then use this method to construct indices of wellbeing according to criteria of our choice. The HDI data we will look at is from the Human Development Report 2016 by the United Nations Development Programme (UNDP). To answer the questions below, download the data and technical notes from the report: Go to the UNDP’s website. Click ‘Table 1: Human Development Index and its components’ on the left-hand side of the page. Then right-click on the ‘Download Data’ box on the top right-hand side of the page and select ‘Save Link As…’. Save the file in an easily accessible location, and make sure to give it a suitable name. The ‘Technical notes’ give a diagrammatic presentation of how the HDI is constructed from four indicators. Refer to the technical notes and the spreadsheet you have downloaded. For each indicator, explain whether you think it is a good measure of the dimension, and suggest alternative indicators, if any. (For example, is GNI per capita a good measure of the dimension ‘a decent standard of living’?) Figure 4.8 shows the minimum and maximum values for each indicator. Discuss whether you think these are reasonable. (You can read the justification for these values in the technical notes.) Dimension Indicator Minimum Maximum Health Life expectancy (years) 20 85 Education Expected years of schooling (years) 0 18 Mean years of schooling (years) 0 15 Standard of living Gross national income per capita (2011 PPP $) 100 75,000 Maximum and minimum values for each indicator in the HDI. Figure 4.8 Maximum and minimum values for each indicator in the HDI. United Nations Development Programme. 2016. ‘Technical notes’ in Human Development Report 2016: p. 2. We are now going to apply the method for constructing the HDI, by recalculating the HDI from its indicators. We will use the formula below, and the minimum and maximum values in the table in Figure 4.8. These are taken from page 2 of the technical notes, which you can refer to for additional details. The HDI indicators are measured in different units and have different ranges, so in order to put them together into a meaningful index, we need to normalize the indicators using the following formula: text{Dimension index } = frac{text{actual value − minimum value}}{text{maximum value − minimum value}} Doing so will give a value in between 0 and 1 (inclusive), which will allow comparison between different indicators. Refer to Figure 4.8 and calculate the dimension index for each of the dimensions: Using the HDI indicator data in Column E of the spreadsheet, calculate the dimension index for a long and healthy life (health). Using the HDI indicator data in Columns G and I of the spreadsheet, calculate the dimension index for knowledge (education). Note that the knowledge dimension index is the average of the dimension index for expected years of schooling for those entering school, and mean years of schooling for adults aged 25 or older. When calculating the index for expected years of schooling you need to restrict the values to take a maximum value of 1. Using the HDI indicator data in Column K of the spreadsheet, calculate the dimension index for a decent standard of living (income). Note (from the technical notes) that you should calculate the GNI index using the natural log of the values. (See the ‘Find out more’ box below for an explanation of the natural log and how to calculate it in R.) Find out more The natural log: What it means, and how to calculate it in R The natural log turns a linear variable into a concave variable, as shown in Figure 4.9. For any value of income on the horizontal axis, the natural log of that value on the vertical axis is smaller. At first, the difference between income and log income is not that big (for example, an income of 2 corresponds to a natural log of 0.7), but the difference becomes bigger as we move rightwards along the horizontal axis (for example, when income is 100,000, the natural log is only 11.5). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Comparing income with the natural logarithm of income. '' /> Comparing income with the natural logarithm of income. Figure 4.9 Comparing income with the natural logarithm of income. The reason why natural logs are useful in economics is because they can represent variables that have diminishing marginal returns: an additional unit of input results in a smaller increase in the total output than did the previous unit. (If you have studied production functions, then the shape of the natural log function might look familiar.) When applied to the concept of wellbeing, the ‘input’ is income, and the ‘output’ is material wellbeing. It makes intuitive sense that a $100 increase in per capita income will have a much greater effect on wellbeing for a poor country compared to a rich country. Using the natural log of income incorporates this notion into the index we create. Conversely, the notion of diminishing marginal returns (the larger the value of the input, the smaller the contribution of an additional unit of input) is not captured by GDP per capita, which uses actual income and not its natural log. Doing so makes the assumption that a $100 increase in per capita income has the same effect on wellbeing for rich and poor countries. The log function in R calculates the natural log of a value for you. To calculate the natural log of a value, x, type log(x). If you have a scientific calculator, you can check that the calculation is correct by using the ln or log key. Now that you know about the natural log, you might want to go back to Question 3(c) in Part 4.1, and create a new chart using the natural log scale. The natural log is used in economics because it approximates percentage changes i.e. log(x) – log(y) is a close approximation to the percentage change between x and y. So, using the natural log scale, you will be able to ‘read off’ the relative growth rates from the slopes of the different series you have plotted. For example, a 0.01 change in the vertical axis value corresponds to a 1% change in that variable. This will allow you to compare the growth rates of the different components of GDP. geometric meanA summary measure calculated by multiplying N numbers together and then taking the Nth root of this product. The geometric mean is useful when the items being averaged have different scoring indices or scales, because it is not sensitive to these differences, unlike the arithmetic mean. For example, if education ranged from 0 to 20 years and life expectancy ranged from 0 to 85 years, life expectancy would have a bigger influence on the HDI than education if we used the arithmetic mean rather than the geometric mean. Conversely, the geometric mean treats each criteria equally. Example: Suppose we use life expectancy and mean years of schooling to construct an index of wellbeing. Country A has life expectancy of 40 years and a mean of 6 years of schooling. If we used the arithmetic mean to make an index, we would get (40 + 6)/2 = 23. If we used the geometric mean, we would get (40 × 6)1/2 = 15.5. Now suppose life expectancy doubled to 80 years. The arithmetic mean would be (80 + 6)/2 = 43, and the geometric mean would be (80 × 6)1/2 = 21.9. If, instead, mean years of schooling doubled to 12 years, the arithmetic mean would be (40 + 12)/2 = 26, and the geometric mean would be (40 × 12)1/2 = 21.9. This example shows that the arithmetic mean can be ‘unfair’ because proportional changes in one variable (life expectancy) have a larger influence over the index than changes in the other variable (years of schooling). The geometric mean gives each variable the same influence over the value of the index, so doubling the value of one variable would have the same effect on the index as doubling the value of another variable. Now, we can combine these dimensional indices to give the HDI. The HDI is the geometric mean of the three dimension indices (IHealth = Life expectancy index, IEducation = Education index, and IIncome = GNI index): text{HDI }=(text{I}_{text{Health}} times text{I}_{text{Education}} times text{I}_{text{Income}})^{1/3} Use the formula above and the data in the spreadsheet to calculate the HDI for all the countries excluding those in the ‘Other countries or territories’ category. You should get the same values as those in Column C, rounded to three decimal places. R walk-through 4.7 Calculating the HDI We will use read_excel to import the data file, which we saved as ‘HDR_data.xlsx’ in our working directory. Before importing, look at the Excel file so that you understand its structure and how it corresponds to the code options used below. We will save the imported data as the dataframe HDR2018. # File path HDR2018 <- read_excel(''HDR_data.xlsx'', # Worksheet to import sheet = ''Table 1'', # Number of rows to skip skip = 2) head(HDR2018) ## # A tibble: 6 x 15 ## X__1 X__2 `Human Development ~ X__3 `Life expectancy a~ X__4 ## <chr> <chr> <chr> <lgl> <chr> <chr> ## 1 HDI r~ Country Value NA (years) <NA> ## 2 <NA> <NA> 2015 NA 2015 <NA> ## 3 <NA> VERY HIGH H~ <NA> NA <NA> <NA> ## 4 1 Norway 0.94942283449106446 NA 81.710999999999999 <NA> ## 5 2 Australia 0.93867953564660933 NA 82.537000000000006 <NA> ## 6 2 Switzerland 0.93913086905938037 NA 83.132999999999996 <NA> ## # ... with 9 more variables: `Expected years of schooling` <chr>, ## # X__5 <chr>, `Mean years of schooling` <chr>, X__6 <chr>, `Gross ## # national income (GNI) per capita` <chr>, X__7 <chr>, `GNI per capita ## # rank minus HDI rank` <chr>, X__8 <lgl>, `HDI rank` <chr> str(HDR2018) ## Classes 'tbl_df', 'tbl' and 'data.frame': 264 obs. of 15 variables: ## $ X__1 : chr ''HDI rank'' NA NA ''1'' ... ## $ X__2 : chr ''Country'' NA ''VERY HIGH HUMAN DEVELOPMENT'' ''Norway'' ... ## $ Human Development Index (HDI) : chr ''Value'' ''2015'' NA ''0.94942283449106446'' ... ## $ X__3 : logi NA NA NA NA NA NA ... ## $ Life expectancy at birth : chr ''(years)'' ''2015'' NA ''81.710999999999999'' ... ## $ X__4 : chr NA NA NA NA ... ## $ Expected years of schooling : chr ''(years)'' ''2015'' NA ''17.671869999999998'' ... ## $ X__5 : chr NA ''a'' NA NA ... ## $ Mean years of schooling : chr ''(years)'' ''2015'' NA ''12.746420000000001'' ... ## $ X__6 : chr NA ''a'' NA NA ... ## $ Gross national income (GNI) per capita: chr ''(2011 PPP $)'' ''2015'' NA ''67614.353480000005'' ... ## $ X__7 : chr NA NA NA NA ... ## $ GNI per capita rank minus HDI rank : chr NA ''2015'' NA ''5'' ... ## $ X__8 : logi NA NA NA NA NA NA ... ## $ HDI rank : chr NA ''2014'' NA ''1'' ... Looking at the HDR dataframe, there are rows that have information that isn’t data (for example, all the rows with an ‘NA’ in the first column), as well as variables/columns that do not contain data (for example, most columns beginning with an ‘X_’, though columns labelled X_1 and X_2 contain the HDI rank and the country names respectively). Cleaning up the dataframe can be easier to do in Excel by deleting irrelevant rows and columns, but one advantage of doing it in R is replicability. Suppose in a year’s time you carried out the analysis again with an updated spreadsheet containing new information. If you had done the cleaning in Excel, you would have to redo it from scratch, but if you had done it in R, you can simply rerun the code below. Firstly, we eliminate rows that do not have any numbers in the HDI rank column (or X_1 column). # Rename the first column, currently named X_1 names(HDR2018)[1] <- ''HDI.rank'' # Rename the second column, currently named X_2 names(HDR2018)[2] <- ''Country'' # Rename the last column, which contains the 2014 rank names(HDR2018)[names(HDR2018) == ''HDI rank''] <- ''HDI.rank.2014'' # Eliminate the row that contains the column title HDR2018 <- subset(HDR2018, !is.na(HDI.rank) & HDI.rank != ''HDI rank'') Then we eliminate columns that contain notes in the original spreadsheet (names starting with ‘X_’). # Check which variables do NOT (!) start with X_ sel_columns <- !startsWith(names(HDR2018), ''X_'') # Select the columns that do not start with X_ HDR2018 <- subset(HDR2018, select = sel_columns) str(HDR2018) ## Classes 'tbl_df', 'tbl' and 'data.frame': 188 obs. of 9 variables: ## $ HDI.rank : chr ''1'' ''2'' ''2'' ''4'' ... ## $ Country : chr ''Norway'' ''Australia'' ''Switzerland'' ''Germany'' ... ## $ Human Development Index (HDI) : chr ''0.94942283449106446'' ''0.93867953564660933'' ''0.93913086905938037'' ''0.9256689410716622'' ... ## $ Life expectancy at birth : chr ''81.710999999999999'' ''82.537000000000006'' ''83.132999999999996'' ''81.091999999999999'' ... ## $ Expected years of schooling : chr ''17.671869999999998'' ''20.43272'' ''16.040410000000001'' ''17.095939999999999'' ... ## $ Mean years of schooling : chr ''12.746420000000001'' ''13.1751'' ''13.37'' ''13.18762553'' ... ## $ Gross national income (GNI) per capita: chr ''67614.353480000005'' ''42822.19627'' ''56363.957799999996'' ''44999.647140000001'' ... ## $ GNI per capita rank minus HDI rank : chr ''5'' ''19'' ''7'' ''13'' ... ## $ HDI.rank.2014 : chr ''1'' ''3'' ''2'' ''4'' ... Let’s change some of the long variable names (those in columns 3–8) to shorter ones. names(HDR2018)[3] <- ''HDI'' names(HDR2018)[4] <- ''LifeExp'' names(HDR2018)[5] <- ''ExpSchool'' names(HDR2018)[6] <- ''MeanSchool'' names(HDR2018)[7] <- ''GNI.capita'' names(HDR2018)[8] <- ''GNI.HDI.rank'' Looking at the structure of the data, we see that R thinks that all the data are chr (character or text variables) because the original datafile contained non-numerical entries (these rows have now been deleted). Apart from the Country variable, which we want to be a factor variable (containing categories), all variables should be numeric. HDR2018$HDI.rank <- as.numeric(HDR2018$HDI.rank) HDR2018$Country <- as.factor(HDR2018$Country) HDR2018$HDI <- as.numeric(HDR2018$HDI) HDR2018$LifeExp <- as.numeric(HDR2018$LifeExp) HDR2018$ExpSchool <- as.numeric(HDR2018$ExpSchool) HDR2018$MeanSchool <- as.numeric(HDR2018$MeanSchool) HDR2018$GNI.capita <- as.numeric(HDR2018$GNI.capita) HDR2018$GNI.HDI.rank <- as.numeric(HDR2018$GNI.HDI.rank) HDR2018$HDI.rank.2014 <- as.numeric(HDR2018$HDI.rank.2014) str(HDR2018) ## Classes 'tbl_df', 'tbl' and 'data.frame': 188 obs. of 9 variables: ## $ HDI.rank : num 1 2 2 4 5 5 7 8 9 10 ... ## $ Country : Factor w/ 188 levels ''Afghanistan'',..: 125 9 163 64 47 151 120 81 76 32 ... ## $ HDI : num 0.949 0.939 0.939 0.926 0.925 ... ## $ LifeExp : num 81.7 82.5 83.1 81.1 80.4 ... ## $ ExpSchool : num 17.7 20.4 16 17.1 19.2 ... ## $ MeanSchool : num 12.7 13.2 13.4 13.2 12.7 ... ## $ GNI.capita : num 67614 42822 56364 45000 44519 ... ## $ GNI.HDI.rank : num 5 19 7 13 13 -3 8 11 20 12 ... ## $ HDI.rank.2014: num 1 3 2 4 6 4 6 8 9 9 ... Now we have a nice clean dataset that we can work with. We start by calculating the three indices, using the information given. For the education index we calculate the index for expected and mean schooling separately, then take the arithmetic mean to get I.Education. As some mean schooling observations exceed the specified ‘maximum’ value of 18, the calculated index values would be larger than 1. To avoid this, we use pmin to replace these observations with 18 to obtain an index value of 1. HDR2018$I.Health <- (HDR2018$LifeExp - 20) / (85 - 20) HDR2018$I.Education <- ((pmin(HDR2018$ExpSchool, 18) - 0) / (18 - 0) + (HDR2018$MeanSchool - 0) / (15 - 0)) / 2 HDR2018$I.Income <- (log(HDR2018$GNI.capita) - log(100)) / (log(75000) - log(100)) HDR2018$HDI.calc <- (HDR2018$I.Health * HDR2018$I.Education * HDR2018$I.Income)^(1/3) Now we can compare the HDI given in the table and our calculated HDI. HDR2018[, c(''HDI'', ''HDI.calc'')] ## # A tibble: 188 x 2 ## HDI HDI.calc ## <dbl> <dbl> ## 1 0.949 0.949 ## 2 0.939 0.939 ## 3 0.939 0.939 ## 4 0.926 0.926 ## 5 0.925 0.925 ## 6 0.925 0.927 ## 7 0.924 0.924 ## 8 0.923 0.923 ## 9 0.921 0.921 ## 10 0.920 0.920 ## # ... with 178 more rows The HDI is one way to measure wellbeing, but you may think that it does not use the most appropriate measures for the non-material aspects of wellbeing (health and education). Now we will use the same method to create our own index of non-material wellbeing (an ‘alternative HDI’), using different indicators. You can find alternative indicators to measure health and education on the UNDP website, in the file ‘Download 2018 Human Development Data Bank’ (left-hand side of the page). (The first column of the spreadsheet, ‘dimension’, tells you whether the indicators refer to ‘Health’, ‘Education’, or other categories). Create an alternative index of wellbeing. In particular, propose alternative Education and Health indices in (a) and (b), then combine these with the existing Income index in (c) to calculate an alternative HDI. Examine whether the changes caused substantial changes in country rankings in (d). Choose two to three indicators to measure health, and two to three indicators to measure education. Explain why you have chosen these indicators. Carefully merge the data into your existing data. Choose a reasonable maximum and minimum value for each indicator and justify your choices. Calculate your alternative versions of the education and health dimension indices. Since you have chosen more than one indicator for this dimension, make sure to average the dimension indices as done in Question 3(b). Also ensure that higher indicator values always represent better outcomes. Now calculate the alternative HDI as done in Questions 3 and 4. Remember to combine your alternative education and health indices with the existing income index from Question 2. Create a new variable showing each country’s rank according to your alternative HDI, where 1 is assigned to the country with the highest value. Compare your ranking to the HDI rank. Are the rankings generally similar, or very different? (See R walk-through 4.8 on how to do this.) R walk-through 4.8 Creating your own HDI Merge data and calculate alternative indices This example uses the following indicators: Education: Literacy rate, adult (% ages 15 and older); Gross enrolment ratio, tertiary (% of tertiary school-age population); Primary school teachers trained to teach (%) Health: Child malnutrition, stunting (moderate or severe) (% under age 5); Mortality rate, female adult (per 1,000 people); Mortality rate, male adult (per 1,000 people). First, we import the data and check that it has been imported correctly. You can see that each row represents a different country and indicator (indicator_name), and each column represents a different year. # Filename allHDR2018 <- read_excel(''2018_all_indicators.xlsx'', # Sheet to import sheet = ''Data'') head(allHDR2018) ## # A tibble: 6 x 34 ## dimension indicator_id indicator_name iso3 country_name `1990` `1991` ## <chr> <dbl> <chr> <chr> <chr> <dbl> <dbl> ## 1 Composit- 14206 HDI rank AFG Afghanistan NA NA ## 2 Composit- 14206 HDI rank ALB Albania NA NA ## 3 Composit- 14206 HDI rank DZA Algeria NA NA ## 4 Composit- 14206 HDI rank AND Andorra NA NA ## 5 Composit- 14206 HDI rank AGO Angola NA NA ## 6 Composit- 14206 HDI rank ATG Antigua and~ NA NA ## # ... with 27 more variables: `1992` <dbl>, `1993` <dbl>, `1993` <dbl>, ## # `1994` <dbl>, `1995` <dbl>, `1996` <dbl>, `1997` <dbl>,`1998` <dbl>, ## # `1999` <dbl>, `2000` <dbl>, `2001` <dbl>, `2002` <dbl>, `2003` <dbl>, ## # `2004` <dbl>, `2005` <dbl>, `2006` <dbl>, `2007` <dbl>, `2008` <dbl>, ## # `2009` <dbl>, `2010` <dbl>, `2011` <dbl>, `2012` <dbl>, `2013` <dbl>, ## # `2014` <dbl>, `2015` <dbl>, `2016` <dbl>, `2017` <dbl>, `9999` <dbl> str(HDR2018) ## Classes 'tbl_df', 'tbl' and 'data.frame': 25636 obs. of 34 variables: ## $ dimension : chr ''Composite indices'' ''Composite indices'' ''Composite indices'' ''Composite indices'' ... ## $ indicator_id : num 146206 146206 146206 146206 ... ## $ indicator_name : chr ''HDI rank'' ''HDI rank'' ''HDI rank'' ''HDI rank'' ... ## $ iso3 : chr ''AFG'' ''ALB'' ''DZA'' ''AND'' ... ## $ country_name : chr ''Afghanistan'' ''Albania'' ''Algeria'' ''Andorra'' ... ## $ 1990 : num NA NA NA NA ... ## $ 1991 : num NA NA NA NA ... ## $ 1992 : num NA NA NA NA ... ## $ 1993 : num NA NA NA NA ... ## $ 1994 : num NA NA NA NA ... ## $ 1995 : num NA NA NA NA ... ## $ 1996 : num NA NA NA NA ... ## $ 1997 : num NA NA NA NA ... ## $ 1998 : num NA NA NA NA ... ## $ 1999 : num NA NA NA NA ... ## $ 2000 : num NA NA NA NA ... ## $ 2001 : num NA NA NA NA ... ## $ 2002 : num NA NA NA NA ... ## $ 2003 : num NA NA NA NA ... ## $ 2004 : num NA NA NA NA ... ## $ 2005 : num NA NA NA NA ... ## $ 2006 : num NA NA NA NA ... ## $ 2007 : num NA NA NA NA ... ## $ 2008 : num NA NA NA NA ... ## $ 2009 : num NA NA NA NA ... ## $ 2010 : num NA NA NA NA ... ## $ 2011 : num NA NA NA NA ... ## $ 2012 : num NA NA NA NA ... ## $ 2013 : num NA NA NA NA ... ## $ 2014 : num NA NA NA NA ... ## $ 2015 : num NA NA NA NA ... ## $ 2016 : num NA NA NA NA ... ## $ 2017 : num NA NA NA NA ... ## $ 9999 : num NA NA NA NA ... Then we follow the same process as in R walk-through 4.7—getting the data for the indicators we want, reshaping it so that each indicator is in a different column (instead of a different row), and giving each indicator a shorter name. We will save this data as HDR2018w. Note that the variable 9999 refers to the latest year available, or the average taken over a range of years (the Excel file contains information on which year(s) were used). indicators <- c( ''Literacy rate, adult (% ages 15 and older)'', ''Gross enrolment ratio, tertiary (% of tertiary school-age population)'', ''Primary school teachers trained to teach (%)'', ''Child malnutrition, stunting (moderate or severe) (% under age 5)'', ''Mortality rate, female adult (per 1,000 people)'', ''Mortality rate, male adult (per 1,000 people)'' HDR2018l <- allHDR2018[ allHDR2018$indicator_name %in% indicators, ] HDR2018l <- subset(HDR2018l, # Indicate which variables to keep. select = c(''indicator_name'', ''country_name'', ''9999'')) HDR2018w <- dcast(HDR2018l, country_name ~ indicator_name, value.var = ''9999'') names(HDR2018.Edu)[1] <- ''HDI.rank'' # Rename the second column, currently named 'Very high …' names(HDR2018w)[1] <- ''Country'' names(HDR2018w)[2] <- ''Child.Malnu'' names(HDR2018w)[3] <- ''Tert.Enrol'' names(HDR2018w)[4] <- ''Adult.Lit'' names(HDR2018w)[5] <- ''Mortality.Female'' names(HDR2018w)[6] <- ''Mortality.Male'' names(HDR2018w)[7] <- ''Prim.Teacher'' str(HDR2018w) ## 'data.frame': 194 obs. of 7 variables: ## $ Country : chr ''Afghanistan'' ''Albania'' ''Algeria'' ''Andorra'' ... ## $ Child.Malnu : num 40.9 23.2 11.7 NA 37.6 NA NA 9.4 2 NA ... ## $ Tert.Enrol : num 8 61 43 NA 9 22 86 51 122 83 ... ## $ Adult.Lit : num 31.7 97.2 75.1 100 66 NA 98.1 99.7 NA NA ... ## $ Mortality.Female: num 202 50 83 NA 203 106 74 74 45 46 ... ## $ Mortality.Male : num 245 79 106 NA 277 151 151 174 76 86 ... ## $ Prim.Teacher : num NA NA 100 100 47 65 NA NA NA NA ... Looking at the structure (str( )), we can see that all indicators are correctly in numerical (num) format. Before we can calculate indices, we need to set minimum and maximum values, which we base on the minimum and maximum values in the sample. summary(HDR2018w) ## Country Child.Malnu Tert.Enrol Adult.Lit ## Length:194 Min. : 1.80 Min. : 2.00 Min. : 15.50 ## Class :character 1st Qu.:10.78 1st Qu.: 13.50 1st Qu.: 68.55 ## Mode :character Median :22.20 Median : 37.00 Median : 92.25 ## Mean :22.84 Mean : 40.36 Mean : 80.11 ## 3rd Qu.:32.65 3rd Qu.: 63.00 3rd Qu.: 97.35 ## Max. :55.90 Max. :122.00 Max. :100.00 ## NA's :62 NA's :43 NA's :60 ## Mortality.Female Mortality.Male Prim.Teacher ## Min. : 34.00 Min. : 63.0 Min. : 15.00 ## 1st Qu.: 66.75 1st Qu.:121.0 1st Qu.: 70.00 ## Median :100.50 Median :184.0 Median : 86.00 ## Mean :130.86 Mean :195.1 Mean : 81.23 ## 3rd Qu.:181.50 3rd Qu.:253.5 3rd Qu.: 99.00 ## Max. :463.00 Max. :555.0 Max. :100.00 ## NA's :14 NA's :14 NA's :77 As we want the observations to be inside the [min, max] interval, we choose the following [min, max] pairs for the education indicators: Adult.Lit: [15, 100], Tert.Enrol: [2, 122], and Prim.Teacher: [15, 100]. You may want to research why there can be countries with a tertiary enrolment ratio larger than 100%. Let’s calculate the alternative education index (I.Education.alt), taking an arithmetic average just as we did for I.Education in R walk-through 4.7. HDR2018w$I.Adult.Lit <- (HDR2018w$Adult.Lit-15) / (100-15) HDR2018w$I.Tert.Enrol<- (HDR2018w$Tert.Enrol-2) / (122-2) HDR2018w$I.Prim.Teacher <- (HDR2018w$Prim.Teacher-15) / (100-15) HDR2018w$I.Education.alt <- (HDR2018w$I.Adult.Lit + HDR2018w$I.Tert.Enrol + HDR2018w$I.Prim.Teacher) / 3 summary(HDR2018w$I.Education.alt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA's ## 0.1628 0.4222 0.5781 0.5862 0.7181 0.8973 113 You can see that we could not calculate this index for 113 countries, as at least one of the three values was missing. We repeat this procedure to calculate an alternative health index (I.Health.alt). The [min, max] pairs we use are: Child.Malnu: [1, 56], Mortality.Female: [34, 463], Mortality.Male: [63, 555]. HDR2018w$I.Child.MalNu <- (HDR2018w$Child.MalNu - 1) / (56 - 1) HDR2018w$I.Mortality.Female <- (HDR2018w$Mortality.Female - 34) / (463 - 34) HDR2018w$I.Mortality.Male <- (HDR2018w$Mortality.Male - 63) / (555 - 63) HDR2018w$I.Health.alt <- (HDR2018w$I.Child.MalNu + HDR2018w$I.Mortality.Female + HDR2018w$I.Mortality.Male) / 3 # Note that these are all 'bad' indicators in the sense that higher numbers indicate worse outcomes. # For all other indicators, larger numbers mean better outcomes. So we need to 'flip' the values of this indicator. HDR2018w$I.Health.alt <- (1 - HDR2018w$I.Health.alt) summary(HDR2018w$I.Health.alt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA's ## 0.1370 0.5235 0.7088 0.6655 0.8274 0.9766 64 Now we use the merge function to merge this variable into our existing HDR2018 dataframe. HDR2018 <- merge(HDR2018, HDR2018w) Calculate an alternative HDI Looking at HDR2018, you will see that alternative health and education indices have been added. Now we are in a position to calculate our own HDI (HDI.own). HDR2018$HDI.own <- (HDR2018$I.Health.alt * HDR2018$I.Education.alt * HDR2018$I.Income)^(1/3) summary(HDR2018$HDI.own) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA's ## 0.2647 0.4541 0.5705 0.5870 0.7460 0.8469 121 We have a substantial number of missing observations, leaving us with around 70 countries for which we could calculate the alternative HDI. Calculate ranks To compare the ranks of the two indices (the original HDI and our alternative HDI), we should only rank the countries that have observations for both indices. We will create a dataframe called HDR2018_sub that contains this subset of countries. HDR2018_sub <- subset(HDR2018, !is.na(HDI) & !is.na(HDI.own)) Let’s calculate the rank for our index. The rank function will assign rank 1 to the smallest index value, but we want the largest (best) index value to have the rank 1. We add - in front of the variable name to obtain the desired effect. HDR2018_sub$HDI.own.rank <- rank(-HDR2018_sub$HDI.own, na.last = ''keep'') HDR2018_sub$HDI.rank <- rank(-HDR2018_sub$HDI, na.last = ''keep'') Now we will use the ggplot function to make a scatterplot comparing the rank of the HDI with that of our own index. ggplot(HDR2018_sub, aes(x = HDI.rank, y = HDI.own.rank)) + # Use solid circles geom_point(shape = 16) + labs(y = ''Alternative HDI rank'', x = ''HDI rank'') + ggtitle(''Comparing ranks between HDI and HDI.own'') + theme_bw() <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-10.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-10-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-10-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-10-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-04-10.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Scatterplot of ranks for HDI and alternative HDI index. '' /> Scatterplot of ranks for HDI and alternative HDI index. Figure 4.10 Scatterplot of ranks for HDI and alternative HDI index. You can see that in general the rankings are similar. If they were identical, the points in the scatterplot would form a straight upward-sloping line. They do not form a straight line, but there is a very strong positive correlation. There are, however, a few countries where the alternative definitions have caused a change in ranking, so let’s use the head and tail functions to find out which countries these are. temp <- HDR2018_sub[ order(HDR2018_sub$HDI.rank - HDR2018_sub$HDI.own.rank), # Show selected variables c(''Country'', ''HDI.rank'', ''HDI.own.rank'')] # Show the countries with the largest fall in rank head(temp, 5) ## Country HDI.rank HDI.own.rank ## 98 Lesotho 52 70 ## 164 Sri Lanka 13 29 ## 104 Madagascar 54 68 ## 57 Eswatini 41 51 ## 154 Seychelles 7 17 # Show the countries with the largest increase in rank tail(temp, 5) ## Country HDI.rank HDI.own.rank ## 111 Mauritania 53 45 ## 171 Tanzania 49 41 ## 31 Cambodia 43 34 ## 165 Sudan 59 50 ## 120 Myanmar 45 33 Compare your alternative index to the HDI: The UN classifies countries into four groups depending on their HDI, as shown in Figure 4.11. Would the classification of any country change under your alternative HDI? Based on your answers to Questions 5(d) and 6(a), do you think that the HDI is a robust measure of non-material wellbeing? (In other words, does changing the indicators used in the HDI give similar conclusions about the non-material wellbeing of countries?) Classification HDI Very high human development 0.800 and above High human development 0.700–0.799 Medium human development 0.550–0.699 Low human development Below 0.550 Classification of countries according to their HDI value. Figure 4.11 Classification of countries according to their HDI value. United Nations Development Programme. 2016. ‘Technical notes’ in Human Development Report 2016: p.3. We will now investigate whether HDI and GDP per capita give similar information about overall wellbeing, by comparing a country’s rank in both measures. To answer Question 7, first add (merge) this data to the dataframe that contains your HDI calculations, making sure to match the data to the correct country. (R walk-through 4.8 shows you how to extract the indicator(s) you need and merge them with another dataset.) Evaluate GDP per capita and the HDI as measures of overall wellbeing: Create a new column showing each country’s rank according to GDP per capita, where 1 is assigned to the country with the highest value. Show this rank on a scatterplot, with GDP per capita rank on the vertical axis and HDI rank on the horizontal axis. (See R walk-through 4.8 for a step-by-step explanation of how to create scatterplots in R.) Does there appear to be a strong correlation between these two variables? Give an explanation for the observed pattern in your chart. Create a table similar to Figure 4.12 below. Using your answers to Question 7(a), fill each box with three countries. You can use the UN’s definition of ‘high’ for the HDI, as in Figure 4.11, and choose a reasonable definition of ‘high’ for GDP. Based on this table, which country or countries would you prefer to grow up in, and why? Explain the differences between HDI and GDP as measures of wellbeing. You may want to consider the way each measure includes income in its calculation (the actual value or a transformation), and the inclusion of other aspects of wellbeing. HDI Low High GDP Low High Classification of countries according to their HDI and GDP values. Figure 4.12 Classification of countries according to their HDI and GDP values. The HDI is one way to measure wellbeing, but there are many other ways to measure wellbeing. What are the strengths and limitations of the HDI as a measure of wellbeing? Find some alternative measures of non-material wellbeing that we could use alongside the HDI to provide a more comprehensive picture of wellbeing. For each measure, evaluate the elements used to construct the measure, and discuss the additional information we can learn from it. (You may find it helpful to read Our World in Data’s page on happiness and life satisfaction, particularly the section ‘Correlates, Determinants, and Consequences’.)"
});
index.addDoc({
    id: 25,
    title: "Doing Economics: Empirical Project 4: Working in Google Sheets",
    content: "Empirical Project 4 Working in Google Sheets Google Sheets-specific learning objectives In addition to the learning objectives for this project, in this section you will learn how to generate new variables using cell formulas. Part 4.1 GDP and its components as a measure of material wellbeing Learning objectives for this part check datasets for missing data sort data and assign ranks based on values distinguish between time series and cross sectional data, and plot appropriate charts for each type of data. The GDP data we will look at is from the United Nations’ National Accounts Main Aggregates Database, which contains estimates of total GDP and its components for all countries over the period 1970–Present. We will look at how GDP and its components have changed over time, and investigate the usefulness of GDP per capita as a measure of wellbeing. To answer the questions below, download the data and make sure you understand how the measure of total GDP is constructed. Go to the United Nations’ National Accounts Main Aggregates Database website. On the right-hand side of the page, under ‘Data Availability’, click ‘Downloads’. Under the subheading ‘GDP and its breakdown at constant 2010 prices in US Dollars’, select the Excel file ‘All countries for all years – sorted alphabetically’. Save it in an easily accessible location, such as a folder on your Desktop or in your personal folder. You can see from the tab ‘Download-GDPconstant-USD-countr’ that some countries have missing data for some of the years. Data may be missing due to political reasons (for example, countries formed after 1970) or data availability issues. Make and fill a frequency table similar to Figure 4.1, showing the number of years that data is available for each country in the category ‘Final consumption expenditure’. How many countries have data for the entire period (1970 to the latest year available)? Do you think that missing data is a serious issue in this case? Country Number of years of GDP data Number of years of GDP data available for each country. Figure 4.1 Number of years of GDP data available for each country. Google Sheets walk-through 4.1 Making a frequency table <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to make a frequency table. '' /> Figure 4.2 How to make a frequency table. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the data looks like. Column B has country names, Column C has the different components of GDP, and Column D onwards has the values of the GDP components for a particular year. '' /> The data This is what the data looks like. Column B has country names, Column C has the different components of GDP, and Column D onwards has the values of the GDP components for a particular year. Figure 4.2a This is what the data looks like. Column B has country names, Column C has the different components of GDP, and Column D onwards has the values of the GDP components for a particular year. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Count the years of data available in each row : We will first count the number of years of data available using the COUNTA function, which tells us how many cells in a given selection are non-empty (i.e. have data). We will store this information in Column AY, as shown. Note: The numbers you get may be slightly different if you are using the latest data. '' /> Count the years of data available in each row We will first count the number of years of data available using the COUNTA function, which tells us how many cells in a given selection are non-empty (i.e. have data). We will store this information in Column AY, as shown. Note: The numbers you get may be slightly different if you are using the latest data. Figure 4.2b We will first count the number of years of data available using the COUNTA function, which tells us how many cells in a given selection are non-empty (i.e. have data). We will store this information in Column AY, as shown. Note: The numbers you get may be slightly different if you are using the latest data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the data : We only want to know how many years of ‘Final consumption expenditure’ are available for each country, so we need to filter out the rest of the data. '' /> Filter the data We only want to know how many years of ‘Final consumption expenditure’ are available for each country, so we need to filter out the rest of the data. Figure 4.2c We only want to know how many years of ‘Final consumption expenditure’ are available for each country, so we need to filter out the rest of the data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the data : After step 8, the data will be filtered so there is only one number for each country. '' /> Filter the data After step 8, the data will be filtered so there is only one number for each country. Figure 4.2d After step 8, the data will be filtered so there is only one number for each country. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Paste the number of years in a new column : We will create a new column with the number of years, so that the columns of our table are in the same order as in Figure 4.1. We will create a copy of the original column, so that in case we make a mistake the original data is preserved. '' /> Paste the number of years in a new column We will create a new column with the number of years, so that the columns of our table are in the same order as in Figure 4.1. We will create a copy of the original column, so that in case we make a mistake the original data is preserved. Figure 4.2e We will create a new column with the number of years, so that the columns of our table are in the same order as in Figure 4.1. We will create a copy of the original column, so that in case we make a mistake the original data is preserved. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Copy the relevant data : Your table is ready to copy and paste into a new tab. '' /> Copy the relevant data Your table is ready to copy and paste into a new tab. Figure 4.2f Your table is ready to copy and paste into a new tab. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Paste the data in a new tab : The table will now look like Figure 4.1. The final step is to count the number of countries with missing data. '' /> Paste the data in a new tab The table will now look like Figure 4.1. The final step is to count the number of countries with missing data. Figure 4.2g The table will now look like Figure 4.1. The final step is to count the number of countries with missing data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-h.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-h-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-h-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-h-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-02-h.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Count the number of countries with missing data : To count the number of countries with missing data, use the COUNTIF function. This function will count the number of cells in a given selection that meet the specified criteria. Note: If you are using the latest data, you should change the number in the COUNTIF function from ‘47’ to the maximum number of years available (the latest year in your data minus 1970 plus 1), so the numbers you get may be slightly different. '' /> Count the number of countries with missing data To count the number of countries with missing data, use the COUNTIF function. This function will count the number of cells in a given selection that meet the specified criteria. Note: If you are using the latest data, you should change the number in the COUNTIF function from ‘47’ to the maximum number of years available (the latest year in your data minus 1970 plus 1), so the numbers you get may be slightly different. Figure 4.2h To count the number of countries with missing data, use the COUNTIF function. This function will count the number of cells in a given selection that meet the specified criteria. Note: If you are using the latest data, you should change the number in the COUNTIF function from ‘47’ to the maximum number of years available (the latest year in your data minus 1970 plus 1), so the numbers you get may be slightly different. If you add up the data on the right-hand side of this equation, you may find that it does not add up to the reported GDP value. The UN notes this discrepancy in Section J, item 17 of the ‘Methodology for the national accounts’: ‘The sums of com­ponents in the tables may not necessarily add up to totals shown because of rounding’. There are three different ways in which countries calculate GDP for their national accounts, but we will focus on the expenditure approach, which calculates gross domestic product (GDP) as: % <![CDATA[ begin{align*} text{GDP} &= text{Household consumption expenditure}  &+ text{General government final consumption expenditure}  &+ text{Gross capital formation}  &+ text{(Exports of goods and services − imports of goods and services)} end{align*} %]]> Gross capital formation refers to the creation of fixed assets in the economy (such as the construction of buildings, roads, and new machinery) and changes in inventories (stocks of goods held by firms). Rather than looking at exports and imports separately, we usually look at the difference between them (exports minus imports), also known as net exports. Choose three countries that have GDP data over the entire period (1970 to the latest year available). For each country, create a new row that shows the values of net exports in each year. Make sure to give each row an appropriate label. Now, we will create charts to show the GDP components in order to look for general patterns over time and make comparisons between countries. Evaluate the components over time, for two countries of your choice. Create a new row for each of the four components of GDP (Household consumption expenditure, General government final consumption expenditure, Gross capital formation, Net exports). To make the charts easier to read, convert the values into billions (for example, 4.38 billion instead of 4,378,772,008). Round your values to two decimal places. Plot a separate line chart for each country, showing the value of the four components of GDP on the vertical axis and time (the years 1970–Present) on the horizontal. (Use more than one line chart per country if necessary, to show the data more clearly). Name each component in the chart legend appropriately. Which of the components would you expect to move together (increasing or decreasing together) or move in opposite directions, and why? Using your charts from Question 3(b), describe any patterns you find in the relationship between the components. Does the data support your hypothesis about the behaviour of the components? For each country, describe any patterns you find in the movement of components over time. What factors could explain the patterns that you find within countries, and any differences between countries (for example, economic or political events)? You may find it helpful to research the history of the countries you have chosen. Extension: For one country, add data labels to your chart to indicate the relevant events that happened in that year. Google Sheets walk-through 4.2 Adding data labels to a chart <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to add data labels to a chart. '' /> Figure 4.3 How to add data labels to a chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This example uses GDP data from Afghanistan. The data has been filtered so that only the components of interest are visible (you should filter your data so that the four components given in Question 3(a) are visible). Note: The numbers in your dataset may differ slightly if you are using the latest data. '' /> The data This example uses GDP data from Afghanistan. The data has been filtered so that only the components of interest are visible (you should filter your data so that the four components given in Question 3(a) are visible). Note: The numbers in your dataset may differ slightly if you are using the latest data. Figure 4.3a This example uses GDP data from Afghanistan. The data has been filtered so that only the components of interest are visible (you should filter your data so that the four components given in Question 3(a) are visible). Note: The numbers in your dataset may differ slightly if you are using the latest data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a line chart : First, we will select the relevant data and draw a line chart. '' /> Draw a line chart First, we will select the relevant data and draw a line chart. Figure 4.3b First, we will select the relevant data and draw a line chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-03-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''View and add a data label to a particular point in the chart : You can use a text box (click ‘Insert’ then select ‘Drawing’) to label specific points only. '' /> View and add a data label to a particular point in the chart You can use a text box (click ‘Insert’ then select ‘Drawing’) to label specific points only. Figure 4.3c You can use a text box (click ‘Insert’ then select ‘Drawing’) to label specific points only. Another way to visualize the GDP data is to look at each component as a proportion of total GDP. Use the same countries that you chose for Question 3. For each country, create a new row in Google Sheets to show the sum of all four components (remember that this total may not add up to the reported GDP). Next, create a new row for each component, showing its proportion of total GDP (as a value ranging from 0 to 1), rounded to two decimal places. (Hint: to calculate the proportion of a component, divide the value of that component by the sum of all four components.) Plot a separate line chart for each country, showing the proportion of the component of GDP on the vertical axis and time (the years 1970 to the latest year available) on the horizontal axis. Describe any patterns in the proportion of spending over time for each country, and compare these patterns across countries. Compared to the charts in Question 3, what are some advantages of this method for making comparisons over time and between countries? time series dataA time series is a set of time-ordered observations of a variable taken at successive, in most cases regular, periods or points of time. Example: The population of a particular country in the years 1990, 1991, 1992, … , 2015 is time series data.cross-sectional dataData that is collected from participants at one point in time or within a relatively short time frame. In contrast, time series data refers to data collected by following an individual (or firm, country, etc.) over a course of time. Example: Data on degree courses taken by all the students in a particular university in 2016 is considered cross-sectional data. In contrast, data on degree courses taken by all students in a particular university from 1990 to 2016 is considered time series data. So far, we have done comparisons of time series data, which is a collection of values for the same variables and subjects, taken at different points in time (for example, GDP of a particular country, measured each year). We will now make some charts using cross-sectional data, which is a collection of values for the same variables for different subjects, usually taken at the same time. Choose three developed countries, three countries in economic transition, and three developing countries (for a list of these countries, see Tables A-C in the UN country classification document). For each country, calculate each component as a proportion of GDP for the year 2015 only. (You may find it helpful to copy and paste the relevant data into a new Google Sheets sheet.) Now create a stacked bar chart that shows the composition of GDP in 2015 on the horizontal axis, and country on the vertical axis. Arrange the columns so that the countries in a particular category are grouped together. (See the walk-through in Figure 3.8 of Economy, Society, and Public Policy.) Describe the differences (if any) between the spending patterns of developed, economic transition, and developing countries. GDP per capita is often used to indicate material wellbeing instead of GDP, because it accounts for differences in population across countries. Refer to the following articles to help you to answer the questions: ‘The economics of well-being’ in the Harvard Business Review ‘Statistical Insights: what does GDP per capita tell us about households’ material well-being?’ in the OECD Insights. Discuss the usefulness and limitations of GDP per capita as a measure of material wellbeing. Based on the arguments in the articles, do you think GDP per capita is an appropriate measure of both material wellbeing and overall wellbeing? Why or why not? Part 4.2 The HDI as a measure of wellbeing Learning objectives for this part sort data and assign ranks based on values distinguish between time series and cross sectional data, and plot appropriate charts for each type of data calculate the geometric mean and explain how it differs from the arithmetic mean construct indices using the geometric mean, and use index values to rank observations explain the difference between two measures of wellbeing (GDP per capita and the Human Development Index). In Part 4.1 we looked at GDP per capita as a measure of material wellbeing. While income has a major influence on wellbeing because it allows us to buy the goods and services we need or enjoy, it is not the only determinant of wellbeing. Many aspects of our wellbeing cannot be bought, for example, good health or having more time to spend with friends and family. We are now going to look at the Human Development Index (HDI), a measure of wellbeing that includes non-material aspects, and make comparisons with GDP per capita (a measure of material wellbeing). GDP per capita is a simple index calculated as the sum of its elements, whereas the HDI is more complex. Instead of using different types of expenditure or output to measure wellbeing or living standards, the HDI consists of three dimensions associated with wellbeing: a long and healthy life (health) knowledge (education) a decent standard of living (income). We will first learn about how the HDI is constructed, and then use this method to construct indices of wellbeing according to criteria of our choice. The HDI data we will look at is from the Human Development Report 2016 by the United Nations Development Programme (UNDP). To answer the questions below, download the data and technical notes from the report: Go to the UNDP’s website. Click ‘Table 1: Human Development Index and its components’ on the left-hand side of the page. Then right-click on the ‘Download Data’ box on the top right-hand side of the page and select ‘Save Link As…’. Save the file in an easily accessible location, and make sure to give it a suitable name. The ‘Technical notes’ give a diagrammatic presentation of how the HDI is constructed from four indicators. Refer to the technical notes and the spreadsheet you have downloaded. For each indicator, explain whether you think it is a good measure of the dimension, and suggest alternative indicators, if any. (For example, is GNI per capita a good measure of the dimension ‘a decent standard of living’?) Figure 4.4 shows the minimum and maximum values for each indicator. Discuss whether you think these are reasonable. (You can read the justification for these values in the technical notes.) Dimension Indicator Minimum Maximum Health Life expectancy (years) 20 85 Education Expected years of schooling (years) 0 18 Mean years of schooling (years) 0 15 Standard of living Gross national income per capita (2011 PPP $) 100 75,000 Maximum and minimum values for each indicator in the HDI. Figure 4.4 Maximum and minimum values for each indicator in the HDI. United Nations Development Programme. 2016. ‘Technical notes’ in Human Development Report 2016: p. 2. We are now going to apply the method for constructing the HDI, by recalculating the HDI from its indicators. We will use the formula below, and the minimum and maximum values in the table in Figure 4.4. These are taken from page 2 of the technical notes, which you can refer to for additional details. The HDI indicators are measured in different units and have different ranges, so in order to put them together into a meaningful index, we need to normalize the indicators using the following formula: text{Dimension index} = frac{text{actual value − minimum value}}{text{maximum value − minimum value}} Doing so will give a value in between 0 and 1 (inclusive), which will allow comparison between different indicators. Refer to Figure 4.4 and calculate the dimension index for each of the dimensions in separate columns in the same spreadsheet tab as your data: Using the HDI indicator data in Column E, calculate the dimension index for a long and healthy life (health). Using the HDI indicator data in Columns G and I, calculate the dimension index for knowledge (education). Note that the knowledge dimension index is the average of the dimension index for expected years of schooling for those entering school, and mean years of schooling for adults aged 25 or older. Using the HDI indicator data in Column K, calculate the dimension index for a decent standard of living (income). Note (from the technical notes) that you should calculate the GNI index using the natural log of the values. (See the ‘Find out more’ box below for an explanation of the natural log and how to calculate it in Google Sheets.) Find out more The natural log: What it means, and how to calculate it in Google Sheets The natural log turns a linear variable into a concave variable, as shown in Figure 4.5. For any value of income on the horizontal axis, the natural log of that value on the vertical axis is smaller. At first, the difference between income and log income is not that big (for example, an income of 2 corresponds to a natural log of 0.7), but the difference becomes bigger as we move rightwards along the horizontal axis (for example, when income is 100,000, the natural log is only 11.5). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-04-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Comparing income with the natural logarithm of income. '' /> Comparing income with the natural logarithm of income. Figure 4.5 Comparing income with the natural logarithm of income. The reason why natural logs are useful in economics is because they can represent variables that have diminishing marginal returns: an additional unit of input results in a smaller increase in the total output than did the previous unit. (If you have studied production functions, then the shape of the natural log function might look familiar.) When applied to the concept of wellbeing, the ‘input’ is income, and the ‘output’ is material wellbeing. It makes intuitive sense that a $100 increase in per capita income will have a much greater effect on wellbeing for a poor country compared to a rich country. Using the natural log of income incorporates this notion into the index we create. Conversely, the notion of diminishing marginal returns (the larger the value of the input, the smaller the contribution of an additional unit of input) is not captured by GDP per capita, which uses actual income and not its natural log. Doing so makes the assumption that a $100 increase in per capita income has the same effect on wellbeing for rich and poor countries. geometric meanA summary measure calculated by multiplying N numbers together and then taking the Nth root of this product. The geometric mean is useful when the items being averaged have different scoring indices or scales, because it is not sensitive to these differences, unlike the arithmetic mean. For example, if education ranged from 0 to 20 years and life expectancy ranged from 0 to 85 years, life expectancy would have a bigger influence on the HDI than education if we used the arithmetic mean rather than the geometric mean. Conversely, the geometric mean treats each criteria equally. Example: Suppose we use life expectancy and mean years of schooling to construct an index of wellbeing. Country A has life expectancy of 40 years and a mean of 6 years of schooling. If we used the arithmetic mean to make an index, we would get (40 + 6)/2 = 23. If we used the geometric mean, we would get (40 × 6)1/2 = 15.5. Now suppose life expectancy doubled to 80 years. The arithmetic mean would be (80 + 6)/2 = 43, and the geometric mean would be (80 × 6)1/2 = 21.9. If, instead, mean years of schooling doubled to 12 years, the arithmetic mean would be (40 + 12)/2 = 26, and the geometric mean would be (40 × 12)1/2 = 21.9. This example shows that the arithmetic mean can be ‘unfair’ because proportional changes in one variable (life expectancy) have a larger influence over the index than changes in the other variable (years of schooling). The geometric mean gives each variable the same influence over the value of the index, so doubling the value of one variable would have the same effect on the index as doubling the value of another variable. Google Sheets’ LN function calculates the natural log of a value for you. Simply type ‘=LN(’, then the number you would like to take the natural log of, then ‘)’ and press Enter. If you have a scientific calculator, you can check that the calculation is correct by using the ‘ln’ or ‘log’ key. Now that you know about the natural log, you might want to go back to Question 3(c) in Part 4.1, and create a new chart using the natural log scale. The natural log is used in economics because it approximates percentage changes i.e. log(x) – log(y) is a close approximation to the percentage change between x and y. So, using the natural log scale, you will be able to ‘read off’ the relative growth rates from the slopes of the different series you have plotted. For example, a 0.01 change in the vertical axis value corresponds to a 1% change in that variable. This will allow you to compare the growth rates of the different components of GDP. Now, we can combine these dimensional indices to give the HDI. The HDI is the geometric mean of the three dimension indices (IHealth = Life expectancy index, IEducation = Education index, and IIncome = GNI index): text{HDI }=(text{I}_{text{Health}} times text{I}_{text{Education}} times text{I}_{text{Income}})^{1/3} Use the formula above and the data in the spreadsheet to calculate the HDI for all the countries excluding those in the ‘Other countries or territories’ category. You should get the same values as those in Column C, rounded to 3 decimal places. Google Sheets walk-through 4.3 Calculating the HDI <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to calculate the HDI. '' /> Figure 4.6 How to calculate the HDI. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : The HDI data looks like this. The HDI value for each country is in Column C, and its four components are in Columns E, G, I, and K. Note: If you are using the latest data, the numbers in your dataset may differ slightly, and your spreadsheet may only have one tab. '' /> The data The HDI data looks like this. The HDI value for each country is in Column C, and its four components are in Columns E, G, I, and K. Note: If you are using the latest data, the numbers in your dataset may differ slightly, and your spreadsheet may only have one tab. Figure 4.6a The HDI data looks like this. The HDI value for each country is in Column C, and its four components are in Columns E, G, I, and K. Note: If you are using the latest data, the numbers in your dataset may differ slightly, and your spreadsheet may only have one tab. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate dimension indices of each component. : First, we will calculate the dimension indices of each component of the HDI, and store this information in Columns Q to T, as shown above. We used the numbers 20 and 85 in the formula for life expectancy because they refer to the minimum and maximum values of that component respectively. '' /> Calculate dimension indices of each component. First, we will calculate the dimension indices of each component of the HDI, and store this information in Columns Q to T, as shown above. We used the numbers 20 and 85 in the formula for life expectancy because they refer to the minimum and maximum values of that component respectively. Figure 4.6b First, we will calculate the dimension indices of each component of the HDI, and store this information in Columns Q to T, as shown above. We used the numbers 20 and 85 in the formula for life expectancy because they refer to the minimum and maximum values of that component respectively. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate dimension indices of each component : After step 4, you will have dimension indices for each component. Make sure to use the LN function to calculate the dimension index for income. '' /> Calculate dimension indices of each component After step 4, you will have dimension indices for each component. Make sure to use the LN function to calculate the dimension index for income. Figure 4.6c After step 4, you will have dimension indices for each component. Make sure to use the LN function to calculate the dimension index for income. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the index value for education : To calculate the HDI, we need the index value for education, which we get by taking the average of the two dimension indices for schooling. The index value for education is the average of the dimensions for expected and mean years of schooling. '' /> Calculate the index value for education To calculate the HDI, we need the index value for education, which we get by taking the average of the two dimension indices for schooling. The index value for education is the average of the dimensions for expected and mean years of schooling. Figure 4.6d To calculate the HDI, we need the index value for education, which we get by taking the average of the two dimension indices for schooling. The index value for education is the average of the dimensions for expected and mean years of schooling. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Amend the formula for expected years of schooling to give the correct HDI values : After step 6, you will have the correct HDI value for most countries. Some of your HDI values (Australia, in the example shown) will not be the same as the reported HDI. This is because some dimensions for that country can exceed the value chosen to be the maximum (e.g expected years of schooling), so the dimension index is greater than 1. For these countries, we need to recalculate the dimension index for expected years of schooling, in order to get the correct value. We can correct this by modifying the formula in Column R. '' /> Amend the formula for expected years of schooling to give the correct HDI values After step 6, you will have the correct HDI value for most countries. Some of your HDI values (Australia, in the example shown) will not be the same as the reported HDI. This is because some dimensions for that country can exceed the value chosen to be the maximum (e.g expected years of schooling), so the dimension index is greater than 1. For these countries, we need to recalculate the dimension index for expected years of schooling, in order to get the correct value. We can correct this by modifying the formula in Column R. Figure 4.6e After step 6, you will have the correct HDI value for most countries. Some of your HDI values (Australia, in the example shown) will not be the same as the reported HDI. This is because some dimensions for that country can exceed the value chosen to be the maximum (e.g expected years of schooling), so the dimension index is greater than 1. For these countries, we need to recalculate the dimension index for expected years of schooling, in order to get the correct value. We can correct this by modifying the formula in Column R. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Amend the formula for expected years of schooling to give the correct HDI values : The problem with some countries is that expected years of schooling is greater than the value chosen as the maximum (18). In these cases, replacing the dimension index with 1 solves the problem. After repeating steps 7 and 8 for all columns with values greater than one, you can see that the calculated HDI values now correspond to the values in Column C. '' /> Amend the formula for expected years of schooling to give the correct HDI values The problem with some countries is that expected years of schooling is greater than the value chosen as the maximum (18). In these cases, replacing the dimension index with 1 solves the problem. After repeating steps 7 and 8 for all columns with values greater than one, you can see that the calculated HDI values now correspond to the values in Column C. Figure 4.6f The problem with some countries is that expected years of schooling is greater than the value chosen as the maximum (18). In these cases, replacing the dimension index with 1 solves the problem. After repeating steps 7 and 8 for all columns with values greater than one, you can see that the calculated HDI values now correspond to the values in Column C. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-06-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Round the calculated values to 3 decimal places : To make the HDI values easier to read, we will round them to 3 decimal places, as was done in Column C. '' /> Round the calculated values to 3 decimal places To make the HDI values easier to read, we will round them to 3 decimal places, as was done in Column C. Figure 4.6g To make the HDI values easier to read, we will round them to 3 decimal places, as was done in Column C. The HDI is one way to measure wellbeing, but you may think that it does not use the most appropriate measures for the non-material aspects of wellbeing (health and education). Now we will use the same method to create our own index of non-material wellbeing (an ‘alternative HDI’), using different indicators. You can find alternative indicators to measure health and education on the UNDP website, in the file ‘Download 2018 Human Development Data Bank’ (left-hand side of the page). (The first column of the spreadsheet, ‘dimension’, tells you whether the indicators refer to ‘Health’, ‘Education’, or other categories). Create an alternative index of wellbeing: Choose two to three indicators to measure health, and two to three indicators to measure education. Explain why you have chosen these indicators. Carefully copy and paste the column(s) containing these indicator values into new columns of your spreadsheet from Question 4 (containing your HDI calculations), making sure to match the data to the correct country. Choose a reasonable maximum and minimum value for each indicator and justify your choices. Using the indicators and maximum and minimum values you have chosen, calculate the alternative HDI as done in Questions 3 and 4. Remember to include the existing income index from Question 2. Since you have chosen more than one indicator per dimension, make sure to average the dimension indices as done in Question 3(b). Also ensure that higher indicator values always represent better outcomes. Create a new column showing each country’s rank according to your alternative HDI, where 1 is assigned to the country with the highest value. Compare your ranking to the HDI rank (Column A). Are the rankings generally similar, or very different? (See Google Sheets walk-through 4.4 on how to do this.) Google Sheets walk-through 4.4 Ranking data <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to rank data '' /> Figure 4.7 How to rank data <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : In this example, the created index is in Column W (titled ‘Own HDI’) and was calculated as the dimension index of Life expectancy at birth. Note: If you are using the latest data, the numbers in your dataset may differ slightly, and your spreadsheet may only have one tab. '' /> The data In this example, the created index is in Column W (titled ‘Own HDI’) and was calculated as the dimension index of Life expectancy at birth. Note: If you are using the latest data, the numbers in your dataset may differ slightly, and your spreadsheet may only have one tab. Figure 4.7a In this example, the created index is in Column W (titled ‘Own HDI’) and was calculated as the dimension index of Life expectancy at birth. Note: If you are using the latest data, the numbers in your dataset may differ slightly, and your spreadsheet may only have one tab. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Rank the index values from largest to smallest : We will rank the values of our own index from largest to smallest, and store this information in Column X. '' /> Rank the index values from largest to smallest We will rank the values of our own index from largest to smallest, and store this information in Column X. Figure 4.7b We will rank the values of our own index from largest to smallest, and store this information in Column X. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-04-07-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Compare the rankings with the HDI index rankings : There are many ways to compare your ranking with the HDI ranking; two ways are shown here. '' /> Compare the rankings with the HDI index rankings There are many ways to compare your ranking with the HDI ranking; two ways are shown here. Figure 4.7c There are many ways to compare your ranking with the HDI ranking; two ways are shown here. Compare your alternative index to the HDI: The UN classifies countries into four groups depending on their HDI, as shown in Figure 4.8. Would the classification of any country change under your alternative HDI? Based on your answers to Questions 5(d) and 6(a), do you think that the HDI is a robust measure of non-material wellbeing? (In other words, does changing the indicators used in the HDI give similar conclusions about the non-material wellbeing of countries?) Classification HDI Very high human development 0.800 and above High human development 0.700–0.799 Medium human development 0.550–0.699 Low human development Below 0.550 Classification of countries according to their HDI value. Figure 4.8 Classification of countries according to their HDI value. United Nations Development Programme. 2016. ‘Technical notes’ in Human Development Report 2016: p.3. We will now investigate whether HDI and GDP per capita give similar information about overall wellbeing, by comparing a country’s rank in both measures. The spreadsheet you downloaded for Question 5 (containing alternative indicators) also has information on GDP per capita, measured in 2011 constant prices in US dollars. This indicator is called ‘GDP per capita (2011 PPP $)’ in Column C of that spreadsheet. To answer Question 7, first filter, then copy and paste this data into a new column in the spreadsheet containing your HDI calculations, making sure to match the data to the correct country. Evaluate GDP per capita and the HDI as measures of overall wellbeing: Create a new column showing each country’s rank according to GDP per capita, where 1 is assigned to the country with the highest value. Show this rank on a scatterplot, with GDP per capita rank on the vertical axis and HDI rank on the horizontal axis. (See Google Sheets walk-through 1.7 for a step-by-step explanation of how to create scatterplots in Google Sheets.) Does there appear to be a strong correlation between these two variables? Give an explanation for the observed pattern in your chart. Create a table similar to Figure 4.9. Using your answers to Question 7(a), fill each box with three countries. You can use the UN’s definition of ‘high’ for the HDI, as in Figure 4.8, and choose a reasonable definition of ‘high’ for GDP. Based on this table, which country or countries would you prefer to grow up in, and why? Explain the differences between HDI and GDP as measures of wellbeing. You may want to consider the way each measure includes income in its calculation (the actual value or a transformation), and the inclusion of other aspects of wellbeing. HDI Low High GDP Low High Classification of countries according to their HDI and GDP values. Figure 4.9 Classification of countries according to their HDI and GDP values. The HDI is one way to measure wellbeing, but there are many other ways to measure wellbeing. What are the strengths and limitations of the HDI as a measure of wellbeing? Find some alternative measures of non-material wellbeing that we could use alongside the HDI to provide a more comprehensive picture of wellbeing. For each measure, evaluate the elements used to construct the measure, and discuss the additional information we can learn from it. (You may find it helpful to read Our World in Data’s page on happiness and life satisfaction, particularly the section ‘Correlates, Determinants, and Consequences’.)"
});
index.addDoc({
    id: 26,
    title: "Doing Economics: Empirical Project 4 Solutions",
    content: "Empirical Project 4 Solutions These are not model answers. They are provided to help students, including those doing the project outside a formal class, to check their progress while working through the questions using the Excel, R, or Google Sheets walk-throughs. There are also brief notes for the more interpretive questions. Students taking courses using Doing Economics should follow the guidance of their instructors. Note These solutions are based on data downloaded in January 2018. Your solutions may differ slightly if using more updated data. Part 4.1 GDP and its components as a measure of material wellbeing As the actual solution table is very long, only the first and last 10 rows are provided here, in Solution figure 4.1. Country Number of years of GDP data Afghanistan 47 Albania 47 Algeria 47 Andorra 47 Angola 47 Anguilla 47 Antigua and Barbuda 47 Argentina 47 Armenia 27 Aruba 47 … … Vanuatu 47 Venezuela (Bolivarian Republic of) 47 Viet Nam 47 Yemen 28 Yemen Arab Republic (Former) 21 Yemen Democratic (Former) 21 Yugoslavia (Former) 21 Zambia 47 Zanzibar 27 Zimbabwe 47 Number of years of GDP data available for each country (1970–2016). Solution figure 4.1 Number of years of GDP data available for each country (1970–2016). 179 out of 220 countries have data for the entire period. We have missing data from 19% of countries. Countries with missing data may have distinct characteristics compared with other countries. For example, poorer countries do not have the resources to collect data. It is therefore likely that for some years, the data available will be for an unrepresentative sample of countries. No solution is provided. This example uses China and the US. No solution is provided. Solution figures 4.2 and 4.3 show the value of the four components of GDP for the US and for China. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-02.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-02-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-02-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-02-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-02.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The US’s GDP components (expenditure approach), 1970–2016. '' /> The US’s GDP components (expenditure approach), 1970–2016. Solution figure 4.2 The US’s GDP components (expenditure approach), 1970–2016. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''China’s GDP components (expenditure approach), 1970–2016. '' /> China’s GDP components (expenditure approach), 1970–2016. Solution figure 4.3 China’s GDP components (expenditure approach), 1970–2016. For countries that are growing, consumption expenditures and capital formation can be increasing together as the additional income is split between them. This pattern is shown in both countries in the period before the global financial crisis in 2008. In the data for the US, investment expenditure and consumption fall markedly in the financial crisis and both government spending and net export expenditure move in the opposite direction. (If you are interested in understanding what lies behind these patterns, see Units 13 and 14 in The Economy.) Note also that China’s net exports fall markedly during the global financial crisis: this does not reflect changes in the Chinese economy, but the fall in expenditure in China’s major trading partners, including the US. China’s economic growth started to accelerate in the late 1970s due to a stream of reforms to promote marketization, privatization, openness to trade, and other objectives. The government shifted the priority of its development strategies away from capital-intensive heavy industries and towards labour-intensive manufacturing industries that take better advantage of China’s abundance of labour. In December 2001, China joined the World Trade Organization, which accelerated its integration into the world economy. China’s comparative advantage in manufacturing due to its abundant and cheap labour has contributed to the rapid growth of net exports which remain a key driver of China’s growth today. The rapid economic growth has allowed all three components to increase simultaneously. The decentralization and the declining role of state-owned enterprises have contributed to the decreasing relative size of government expenditure. The United States, as a developed economy, has been growing at relatively slow rates between 1970 and 2016. The net exports component has been decreasing as production of many goods has moved to low-cost developing countries. Most of the gains in income are devoted to household consumption, which has been rising at a disproportionately high rate compared with the other components. From the charts, the most obvious difference between China and the US lies in the relative importance of capital formation, that is, investment expenditure on machinery, equipment, and buildings, including infrastructure. In particular, while capital formation is the largest component in China, it is a relatively small component in the US. In fact, capital formation in the US is only about one third of household consumption. An example for China is shown in Solution figure 4.4. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''China’s GDP components (expenditure approach), with annotations (1970–2016). '' /> China’s GDP components (expenditure approach), with annotations (1970–2016). Solution figure 4.4 China’s GDP components (expenditure approach), with annotations (1970–2016). China and the US are used as examples. No solution is provided. Solution figures 4.5 and 4.6 show the proportion of the component of GDP by year for China and for the US. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Share of components of GDP in China (1970–2016). '' /> Share of components of GDP in China (1970–2016). Solution figure 4.5 Share of components of GDP in China (1970–2016). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Share of components of GDP in the US (1970–2016). '' /> Share of components of GDP in the US (1970–2016). Solution figure 4.6 Share of components of GDP in the US (1970–2016). In China, household consumption accounted for 56% of GDP in 1970 but has since fallen to a low point of 36% in 2010, after which it stabilized. It was 37% in 2016. The share of capital formation fluctuated around 37% between 1970 and 2000. After 2000, capital formation experienced a period of rapid increase, reaching 48% in 2010 before stabilizing. The share of government consumption is relatively small, about 10% in 1970, and has been increasing slowly, reaching 14% in 2016. The net exports component was negative and decreasing prior to 1986. After 1986, net exports increased rapidly, reaching 8% of GDP in 2008, before remaining relatively stable. In the US, household consumption remains the largest component throughout the period, and increased from 60% in 1970 to 70% in 2016. Capital formation fluctuates without a clear trend. Government consumption fell from 25% in 1970 to 14% in 2016. The net exports component is negative and has been falling slightly. While the share of household consumption has been falling in China, it has been increasing in the US. Capital formation is relatively small in comparison to household consumption in the US. In China, however, capital formation rose more rapidly and in 2003 surpassed household consumption to become the largest component of GDP. While the share of government consumption in China has no clear trend, it has been falling in the US. It is difficult to tell the relative proportions (and thus the importance) of each component from the charts in Question 3, which are plotted using levels. The charts of levels show that the governments have been growing in size. When we look at the charts of proportions, we realize that the proportion of government consumption expenditure in the US has been falling, and has no clear trend in China. The charts in levels are more difficult to compare across countries because countries have different income levels. The charts of proportions have common units and bounds and are therefore easier for cross-country comparisons. The following countries are used as examples: developing economies: Brazil, China, India economies in transition: Albania, Russian Federation, Ukraine developed countries: Germany, Japan, United States. Solution figure 4.7 provides the calculation for each component as a proportion of GDP for 2015. Country Indicator name Proportion of GDP Brazil Household consumption expenditure 0.64 Brazil General government final consumption expenditure 0.19 Brazil Gross capital formation 0.17 Brazil Net exports 0.00 China Household consumption expenditure 0.37 China General government final consumption expenditure 0.13 China Gross capital formation 0.47 China Net exports 0.02 India Household consumption expenditure 0.55 India General government final consumption expenditure 0.10 India Gross capital formation 0.36 India Net exports −0.01 Albania Household consumption expenditure 0.78 Albania General government final consumption expenditure 0.11 Albania Gross capital formation 0.27 Albania Net exports −0.16 Russian Federation Household consumption expenditure 0.52 Russian Federation General government final consumption expenditure 0.17 Russian Federation Gross capital formation 0.20 Russian Federation Net exports 0.11 Ukraine Household consumption expenditure 0.69 Ukraine General government final consumption expenditure 0.22 Ukraine Gross capital formation 0.16 Ukraine Net exports −0.07 Germany Household consumption expenditure 0.55 Germany General government final consumption expenditure 0.19 Germany Gross capital formation 0.19 Germany Net exports 0.07 Japan Household consumption expenditure 0.57 Japan General government final consumption expenditure 0.20 Japan Gross capital formation 0.24 Japan Net exports 0.00 United States Household consumption expenditure 0.62 United States General government final consumption expenditure 0.13 United States Gross capital formation 0.19 United States Net exports 0.07 Share of each component of GDP for a selection of countries in 2015. Solution figure 4.7 Share of each component of GDP for a selection of countries in 2015. Solution figure 4.8 shows the composition of GDP in 2015 by country. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-08.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-08-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-08-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-08-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-08.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Composition of GDP in 2015. '' /> Composition of GDP in 2015. Solution figure 4.8 Composition of GDP in 2015. Developing countries spend less on household consumption and more on capital formation compared to the other two types of countries. The proportion of income spent on government consumption tends to be higher in developed countries. GDP per capita is the total value of all output (and thus income) in an economy divided by the population in a period of time. GDP per capita is by definition a measure of material wellbeing. The measure is based on the assumption that market prices of goods and services are good indicators of the amount of welfare they bring to economic agents. GDP per capita is relatively straightforward to calculate and use. GDP per capita is also a relatively objective measure. GDP per capita is the most widely-used measure and huge resources have been devoted to establishing the infrastructure for its computation. Even as a measure of material wellbeing, GDP per capita has limitations: Goods and services such as caring for family relatives are not accounted for by GDP per capita because they are not traded in markets and hence do not have prices. GDP per capita understates material wellbeing for this reason. Economic activities that produce pollution are included in GDP but the detriment they cause to material wellbeing is not. GDP per capita overstates material wellbeing for this reason. Income generated by production in a country may not remain in that country or get used by people living in the country. Some of the income may be appropriated by foreign companies repatriating profits from their affiliates. The measure gross national income per capita adjusts for this. GDP per capita is an average across the population and therefore does not reflect the inequality in material wellbeing within the country. A country can have a high GDP per capita while having income concentrated in a narrow section of the population. There is a wide scope for different answers here using the two sources listed as well as material in Section 4.14 of Economy, Society, and Public Policy. Part 4.2 The HDI as a measure of wellbeing There are many valid points that can be made about the suitability of these measures, for example: The gross national income of a country consists of its GDP, plus income earned by residents living in foreign countries, minus income earned domestically by non-residents. GNI per capita, despite having many limitations, is the best widely available measure of living standards we have. The expected years of schooling and mean years of schooling are not good measures of knowledge. Most importantly, the quality of education varies across countries. Life expectancy, especially in developing countries, can be driven by rapid changes in infant and child mortality, and therefore may fail to reflect longevity. A longer life does not necessarily mean a healthy life. These indicators all have limitations as measures of the relevant dimensions. There are many possible answers, for example: These values are completely reasonable. Starvation, diseases, land erosion, and the lack of incentives are day-to-day realities for the people of the poorest countries in the world. Due to low income, poor countries cannot afford healthcare and education. Poor healthcare and education, many economists argue, may be the cause of low income in the first place. (a)–(c) No solution is provided. For (b), if education is missing for one of the education variables, take the value of the available index as the education index. No solution is provided. For the indicators in this example, child malnutrition and mortality rate (male and female) are used for health. Adult literacy rate, tertiary enrol­ment, and primary school teachers trained to teach are used for education. Child malnutrition has a major impact on the welfare of children and on their physical and cognitive development. Child malnutrition is therefore a good indicator of the future health of children when they grow up and hence a good indicator of the health of the population. The mortality rate is driven by health. Although mortality rate is a determinant of life expectancy, the measure does not involve forming expectations of how long someone will live. The quantity of education can be a poor indicator of knowledge due to the varying quality of education across countries. Furthermore, most countries, including developing countries, have achieved very high primary and secondary enrolment rates, resulting in less variation across countries. The adult literacy rate is by definition a measure that better reflects the stock of knowledge in the economy. Primary school teachers trained to teach is a good and available measure of the quality of education. Tertiary education is typically financed by individuals rather than by governments. As a result, there is great variation across countries in tertiary enrolment. Universities, unlike primary and secondary schools, play a major role in pushing the frontier of research and innovation. The respective min and max functions were used to determine a reasonable minimum and maximum value for each indicator in Solution figure 4.9. Indicator Minimum Maximum Child malnutrition (% under age 5) 1.30 57.50 Female mortality rate (per 1,000 people) 32.24 612.37 Male mortality rate (per 1,000 people) 63.70 580.50 Adult literacy rate (% ages 15 and older) 19.13 99.89 Tertiary Enrolment (% of tertiary school-age population) 0.80 110.16 Primary school teachers trained to teach (% of primary school teachers) 5.86 100.00 Minimum and maximum values of the chosen indicators. Solution figure 4.9 Minimum and maximum values of the chosen indicators. Note that the tertiary enrolment can be greater than 100% due to students in tertiary education who are younger or older than the usual age at which people receive tertiary education (18–23 years old). The alternative HDI can only be calculated for a limited number of countries that have full data for all the variables chosen. When choosing the indicators, missing data did not seem to be an issue, as only a few countries lack data for each particular indicator. However, the set of countries lacking data for one or more of the indicators turns out to be large. The missing data problem is more common among the richest countries. For example, adult literacy rate is missing for Norway, Australia, US, UK, and other European countries (though this information can be found from other data sources, such as the CIA Factbook). Missing data means that the alternative HDI was available for only 80 countries. As shown in Solution figure 4.10, countries with higher HDI tend to have higher alternative HDI. There are, however, exceptions. Columbia, for example, ranked 24 out of the 80 countries in terms of HDI, is ranked 10 in terms of alternative HDI. Swaziland, ranked 51 out of 80 in HDI, is ranked 70 in alternative HDI. Country Alternative Health index Alternative Education index Alternative Income index Alternative HDI excluding countries with missing data Alternative HDI rank HDI rank Brunei Darussalam 0.866 0.702 0.996 0.846 3 1 Saudi Arabia 0.905 0.828 0.943 0.891 1 2 Kuwait 0.936 0.658 1.002 0.851 2 3 Belarus 0.820 0.932 0.763 0.836 4 4 Kazakhstan 0.724 0.804 0.815 0.780 9 5 Malaysia 0.812 0.728 0.832 0.789 8 6 Panama 0.809 0.728 0.796 0.777 14 7 Costa Rica 0.926 0.795 0.747 0.819 6 8 Serbia 0.888 0.677 0.726 0.758 19 9 Cuba 0.914 0.788 0.651 0.777 13 10 Iran (Islamic Republic of) 0.922 0.811 0.770 0.832 5 11 Georgia 0.853 0.763 0.677 0.761 17 12 Sri Lanka 0.807 0.627 0.707 0.710 29 13 Lebanon 0.895 0.759 0.739 0.795 7 14 Mexico 0.848 0.717 0.770 0.776 15 15 Azerbaijan 0.796 0.732 0.770 0.766 16 16 Algeria 0.863 0.669 0.741 0.754 22 17 Armenia 0.794 0.718 0.665 0.724 26 18 Ukraine 0.793 0.914 0.649 0.778 12 19 Thailand 0.777 0.810 0.752 0.779 11 20 Ecuador 0.762 0.700 0.704 0.721 27 21 Mongolia 0.734 0.853 0.702 0.761 18 22 Jamaica 0.869 0.687 0.668 0.736 24 23 Colombia 0.816 0.792 0.732 0.780 10 24 Tunisia 0.884 0.695 0.699 0.754 21 25 Dominican Republic 0.823 0.722 0.732 0.758 20 26 Belize 0.732 0.528 0.650 0.631 40 27 Uzbekistan 0.721 0.690 0.612 0.673 34 28 Moldova (Republic of) 0.814 0.765 0.592 0.717 28 29 Botswana 0.508 0.696 0.753 0.643 37 30 Paraguay 0.823 0.725 0.665 0.735 25 31 Egypt 0.748 0.562 0.697 0.664 35 32 Palestine, State of 0.875 0.785 0.598 0.743 23 33 Viet Nam 0.794 0.734 0.601 0.705 30 34 Philippines 0.629 0.758 0.669 0.683 33 35 El Salvador 0.753 0.689 0.657 0.698 32 36 Kyrgyzstan 0.766 0.703 0.519 0.653 36 37 Morocco 0.861 0.625 0.646 0.703 31 38 Guyana 0.736 0.547 0.639 0.636 39 39 Tajikistan 0.707 0.744 0.492 0.637 38 40 Bhutan 0.608 0.523 0.644 0.589 44 41 Congo 0.615 0.539 0.605 0.585 45 42 Lao People's Democratic Republic 0.566 0.628 0.592 0.595 43 43 Bangladesh 0.694 0.397 0.530 0.527 52 44 Ghana 0.649 0.455 0.551 0.546 49 45 Sao Tome and Principe 0.729 0.369 0.517 0.518 53 46 Cambodia 0.656 0.619 0.518 0.595 42 47 Nepal 0.652 0.547 0.476 0.554 46 48 Myanmar 0.612 0.675 0.589 0.624 41 49 Pakistan 0.603 0.469 0.592 0.551 47 50 Swaziland 0.193 0.555 0.653 0.412 70 51 Angola 0.472 0.390 0.626 0.486 58 52 Tanzania (United Republic of) 0.540 0.591 0.484 0.537 50 53 Cameroon 0.442 0.522 0.508 0.490 57 54 Zimbabwe 0.418 0.576 0.418 0.465 63 55 Mauritania 0.685 0.453 0.538 0.551 48 56 Madagascar 0.501 0.236 0.390 0.359 77 57 Rwanda 0.549 0.549 0.420 0.502 55 58 Comoros 0.596 0.511 0.391 0.492 56 59 Lesotho 0.152 0.523 0.529 0.348 78 60 Senegal 0.714 0.398 0.470 0.511 54 61 Uganda 0.479 0.552 0.425 0.482 59 62 Sudan 0.563 0.474 0.551 0.528 51 63 Togo 0.571 0.472 0.383 0.469 62 64 Benin 0.563 0.343 0.451 0.443 65 65 Malawi 0.485 0.493 0.359 0.441 66 66 Côte d'Ivoire 0.396 0.403 0.522 0.436 67 67 Gambia 0.598 0.434 0.413 0.475 60 68 Ethiopia 0.546 0.462 0.411 0.470 61 69 Mali 0.521 0.264 0.468 0.401 71 70 Congo (Democratic Republic of the) 0.489 0.572 0.289 0.433 69 71 Liberia 0.571 0.329 0.290 0.379 75 72 Eritrea 0.448 0.493 0.408 0.448 64 73 Mozambique 0.318 0.477 0.362 0.380 74 74 Guinea 0.548 0.322 0.356 0.398 72 75 Burundi 0.362 0.591 0.292 0.397 73 76 Burkina Faso 0.549 0.364 0.413 0.435 68 77 Chad 0.382 0.304 0.452 0.374 76 78 Niger 0.542 0.159 0.330 0.305 79 79 Central African Republic 0.334 0.263 0.267 0.286 80 80 Comparing alternative HDI rank and HDI rank. Solution figure 4.10 Comparing alternative HDI rank and HDI rank. The same countries used in Question 5 are used as examples. The classification of Belize and Uzbekistan changes from High human development to Medium human development. Bangladesh, Ghana, and Sao Tome and Principe change from Medium human development countries to Low human development countries under the alternative HDI. Despite using very different indicators for health and education, the rank and classifications of countries under the alternative HDI are similar to those under HDI. HDI is therefore a robust measure of non-material wellbeing. Solution figure 4.11 provides the scatterplot for each country’s GDP per capita rank and HDI rank. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-11.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-11-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-11-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-11-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-04-11.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Scatterplot of GDP per capita rank and HDI rank. '' /> Scatterplot of GDP per capita rank and HDI rank. Solution figure 4.11 Scatterplot of GDP per capita rank and HDI rank. There appears to be a strong correlation, because GDP per capita is a component of HDI. In addition, GDP per capita is strongly related to health and education, the other two components of HDI. We would therefore expect that GDP per capita and HDI would be strongly correlated. There are many possible ways to define ‘high’ GDP per capita; here we define ‘high’ as greater than 11,048 USD, the median of the group, as shown in Solution figure 4.12. HDI Low High GDP per capita Low Afghanistan Zimbabwe Yemen Georgia Sri Lanka Albania High Botswana Iraq South Africa Norway Switzerland United States Comparison of countries according to GDP per capita and HDI. Solution figure 4.12 Comparison of countries according to GDP per capita and HDI. GDP per capita measures the total value of all output in the economy per head of the population. HDI is a geometric mean of three dimension indices: health, education, and the natural log of income per capita. Unlike GDP per capita, the HDI takes into account the possibility that income might have diminishing marginal utility (that is, doubling your income might less than double your wellbeing). GDP per capita, however, assumes that wellbeing increases one-for-one with income, and that income is the only factor that determines wellbeing, which is not the case. There are many possible strengths and weaknesses, including the following: Strengths: The HDI as a measure of wellbeing is more comprehensive than GDP per capita because it accounts for education and health outcomes. The HDI is widely used and recognized. Weaknesses: As mentioned before, schooling years and life expectancy may not be good indicators of knowledge and health, respectively. The three dimensions receive equal weight in HDI. Countries may have different views about which dimension is more important. There are many possible measures, including the following: Unemployment affects people’s non-material wellbeing. Unemployed people often have no income except benefits, and are more likely to feel detached from the world and unhappy. Long-term unemployment could make it more and more difficult for people to be re-employed. The unemployment rate is measured as the percentage of the economically-active population that is out of work but is actively seeking work. For more discussion of these issues see Unit 8 of Economy, Society, and Public Policy on the labour market and unemployment, and Empirical Project 8, which investigates the non-monetary cost of unemployment. Inequality affects people’s satisfaction. It matters whether people get equal opportunities to succeed. People’s happiness may depend more on their relative income compared to that of others rather than the absolute value of income. Inequality measures such as the Gini coefficient and health inequality Gini coefficient can be used alongside HDI. There are many subjective measures of non-material wellbeing, such as gross national happiness and gross national wellbeing. These measures rely on surveys that ask people about their non-material wellbeing directly."
});
index.addDoc({
    id: 27,
    title: "Doing Economics: Empirical Project 5: Measuring inequality: Lorenz curves and Gini coefficients",
    content: "Empirical Project 5 Measuring inequality: Lorenz curves and Gini coefficients Learning objectives In this project you will: draw Lorenz curves (Part 5.1) calculate and interpret the Gini coefficient (Part 5.1) interpret alternative measures of income inequality (Part 5.1) research other dimensions of inequality and how they are measured (Part 5.2). Key concepts Concepts needed for this project: ratio and decile. Concepts introduced in this project: Gini coefficient and Lorenz curve. Introduction CORE projects This empirical project is related to material in: Unit 5 of Economy, Society, and Public Policy Unit 5 and Unit 19 of The Economy. There are many criteria that policymakers can use to assess outcomes of economic interactions, or allocations, in order for them to evaluate which outcome is ‘better’ than the others. One important criterion for assessing an allocation is efficiency, and another is fairness. Outcomes that economists would define as ‘efficient’—those that cannot make one person better off without making someone else worse off—may be undesirable because they are unfair. To read more about how economists use the word ‘efficiency’, see Section 3.4 in Economy, Society, and Public Policy. Lorenz curveA graphical representation of inequality of some quantity such as wealth or income. Individuals are arranged in ascending order by how much of this quantity they have, and the cumulative share of the total is then plotted against the cumulative share of the population. For complete equality of income, for example, it would be a straight line with a slope of one. The extent to which the curve falls below this perfect equality line is a measure of inequality. See also: Gini coefficient. For example, a situation where a small fraction of the population lives in luxury and everybody else struggles to survive could be efficient, but few people would say it is desirable due to the vast inequality between the rich and poor. In this case, policymakers might intervene by implementing a tax system where richer people pay a greater proportion of their income than poorer people (a progressive tax), and some revenue collected in taxes is transferred to the poor. Empirical evidence on people’s views about the fairness of the income distribution and further discussion of the concept of fairness can be found in Sections 3.6 and 3.7 of Economy, Society, and Public Policy. Gini coefficientA measure of inequality of any quantity such as income or wealth, varying from a value of zero (if there is no inequality) to one (if a single individual receives all of it). To assess inequality, economists often use a measure called the Gini coefficient, which is based on the differences between people in incomes, wealth, or some other measure. We will first look at how the Gini coefficient is calculated and compare it with other measures of inequality between the rich and poor, such as the 90/10 ratio. We will also use Lorenz curves to show the entire distribution of income in a country. Then, we will use the Gini coefficient and other measures to look at other dimensions of inequality, such as health-related outcomes and gender participation in education. To learn more about how the Gini coefficient is calculated from differences in people’s endowments, see Section 5.9 of Economy, Society, and Public Policy. (It may help to read this section before starting the project). Working in Excel Working in R Working in Google Sheets"
});
index.addDoc({
    id: 28,
    title: "Doing Economics: Empirical Project 5: Working in Excel",
    content: "Empirical Project 5 Working in Excel Part 5.1 Measuring income inequality Learning objectives for this part draw Lorenz curves calculate and interpret the Gini coefficient interpret alternative measures of income inequality. One way to visualize the income distribution in a population is to draw a Lorenz curve. This curve shows the entire population along the horizontal axis from the poorest to the richest. The height of the curve at any point on the vertical axis indicates the fraction of total income received by the fraction of the population, shown on the horizontal axis. We will start by using income decile data from the Global Consumption and Income Project to draw Lorenz curves and compare changes in the income distribution of a country over time. Note that income here refers to market income, which does not take into account taxes or government transfers (see Section 5.10 of Economy, Society, and Public Policy for further details). To answer the question below: Go to the Globalinc website and download the Excel file containing the data by clicking ‘xlsx’. Save it in an easily accessible location, such as a folder on your Desktop or in your personal folder. Choose two countries that you would like to compare and filter the data so only the values for 1980 and 2014 are visible. You will be using this data as the basis for your Lorenz curves. Copy and paste the filtered data (all columns) into a new tab in your spreadsheet. To draw Lorenz curves, we need to calculate the cumulative share of total income owned by each decile (these will be the vertical axis values). The cumulative income share of a particular decile is the proportion of total income held by that decile and all the deciles below it. For example, if Decile 1 has 1/10 of total income and Decile 2 has 2/10 of total income, the cumulative income share of Decile 2 is 3/10 (or 0.3). In this new tab, make one table (as shown in Figure 5.1) for each country and year (four tables total). Use the country data you have selected to fill in each table. (Remember that each decile represents 10% of the population.) Cumulative share of the population (%) Cumulative share of income (%) 0 0 10 20 30 40 50 60 70 80 90 100 Cumulative share of income owned, for each decile of the population. Figure 5.1 Cumulative share of income owned, for each decile of the population. Excel walk-through 5.1 Creating a table showing cumulative shares <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to create a table showing cumulative shares. '' /> How to create a table showing cumulative shares. Figure 5.2 How to create a table showing cumulative shares. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : We will be using data from Afghanistan and Albania for this example. The data has been copied and pasted into a new tab on the spreadsheet. We will make a cumulative table for Afghanistan in 1980. (The other three tables are made in the same way.) '' /> The data We will be using data from Afghanistan and Albania for this example. The data has been copied and pasted into a new tab on the spreadsheet. We will make a cumulative table for Afghanistan in 1980. (The other three tables are made in the same way.) Figure 5.2a We will be using data from Afghanistan and Albania for this example. The data has been copied and pasted into a new tab on the spreadsheet. We will make a cumulative table for Afghanistan in 1980. (The other three tables are made in the same way.) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the cumulative share of income using the SUM function : To calculate the cumulative share of income, we need to add up all the incomes corresponding to that decile and all smaller deciles, and then divide by the sum of all incomes. '' /> Calculate the cumulative share of income using the SUM function To calculate the cumulative share of income, we need to add up all the incomes corresponding to that decile and all smaller deciles, and then divide by the sum of all incomes. Figure 5.2b To calculate the cumulative share of income, we need to add up all the incomes corresponding to that decile and all smaller deciles, and then divide by the sum of all incomes. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the cumulative share of income using the SUM function : Decile 2 and the remaining deciles are calculated slightly differently from Decile 1, because we have to also include the incomes of lower deciles in the calculation. '' /> Calculate the cumulative share of income using the SUM function Decile 2 and the remaining deciles are calculated slightly differently from Decile 1, because we have to also include the incomes of lower deciles in the calculation. Figure 5.2c Decile 2 and the remaining deciles are calculated slightly differently from Decile 1, because we have to also include the incomes of lower deciles in the calculation. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-02-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the cumulative share of income using the SUM function : You can use this table to plot a Lorenz curve with the first column as the horizontal axis values, and the second column as the vertical axis values. '' /> Calculate the cumulative share of income using the SUM function You can use this table to plot a Lorenz curve with the first column as the horizontal axis values, and the second column as the vertical axis values. Figure 5.2d You can use this table to plot a Lorenz curve with the first column as the horizontal axis values, and the second column as the vertical axis values. Use the tables you have made to draw Lorenz curves for each country in order to visually compare the income distributions over time. Draw a line chart with cumulative share of population on the horizontal axis and cumulative share of income on the vertical axis. Plot one chart per country (each chart should have two lines, one for 1980 and one for 2014). Make sure to include a chart legend, and label your axes and chart appropriately. Follow the steps in Excel walk-through 5.2 to add a straight line representing perfect equality to each chart. (Hint: If income was shared equally across the population, the bottom 10% of people would have 10% of the total income, the bottom 20% would have 20% of the total income, and so on.) Excel walk-through 5.2 Drawing the perfect equality line <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to draw the perfect equality line. '' /> Figure 5.3 How to draw the perfect equality line. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : We will use the Lorenz curve for Afghanistan in 1980 as an example. The values we need to plot the perfect equality line are given in cells C9 to C19 (labelled ‘perfect equality line’). You will notice that these values are the same as those in cells A9 to A19, because the perfect equality line is where the horizontal and vertical axis values are equal to each other. '' /> The data We will use the Lorenz curve for Afghanistan in 1980 as an example. The values we need to plot the perfect equality line are given in cells C9 to C19 (labelled ‘perfect equality line’). You will notice that these values are the same as those in cells A9 to A19, because the perfect equality line is where the horizontal and vertical axis values are equal to each other. Figure 5.3a We will use the Lorenz curve for Afghanistan in 1980 as an example. The values we need to plot the perfect equality line are given in cells C9 to C19 (labelled ‘perfect equality line’). You will notice that these values are the same as those in cells A9 to A19, because the perfect equality line is where the horizontal and vertical axis values are equal to each other. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add the required cells to the line chart : For the perfect equality line to show up on the chart, we need to add it as a separate data series. '' /> Add the required cells to the line chart For the perfect equality line to show up on the chart, we need to add it as a separate data series. Figure 5.3b For the perfect equality line to show up on the chart, we need to add it as a separate data series. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-03-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add the required cells to the line chart : Since the values in cells A9 to A19 and C9 to C19 are the same, it doesn’t matter which range of cells you add to the chart. After step 6, the perfect equality line will appear on your chart. '' /> Add the required cells to the line chart Since the values in cells A9 to A19 and C9 to C19 are the same, it doesn’t matter which range of cells you add to the chart. After step 6, the perfect equality line will appear on your chart. Figure 5.3c Since the values in cells A9 to A19 and C9 to C19 are the same, it doesn’t matter which range of cells you add to the chart. After step 6, the perfect equality line will appear on your chart. Using your Lorenz curves: Compare the distribution of income across time for each country. Compare the distribution of income across countries for each year (1980 and 2014). Suggest some explanations for any similarities and differences you observe. (You may want to research your chosen countries to see if there were any changes in government policy, political events, or other factors that may affect the income distribution.) A rough way to compare income distributions is to use a summary measure such as the Gini coefficient. The Gini coefficient ranges from 0 (complete equality) to 1 (complete inequality). It is calculated by dividing the area between the Lorenz curve and the perfect equality line, by the total area underneath the perfect equality line. Intuitively, the further away the Lorenz curve is from the perfect equality line, the more unequal the income distribution is, and the higher the Gini coefficient will be. Using a Gini coefficient calculator, calculate the Gini coefficient for each of your Lorenz curves. You should have four coefficients in total. Label each Lorenz curve with its corresponding Gini coefficient, and check that the coefficients are consistent with what you see in your charts. (Hint: In the Gini calculator, paste the list of incomes by decile into the box provided, and then add commas between the income values.) Now we will look at other measures of income inequality to see how they can be used with the Gini coefficient to summarize a country’s income distribution. Instead of summarizing the entire income distribution like the Gini coefficient does, we can take the ratio of incomes at two points in the distribution. For example, the 90/10 ratio takes the ratio of the top 10% of incomes (Decile 10) to the lowest 10% of incomes (Decile 1). A 90/10 ratio of five means that the richest 10% of the population earn five times more than the poorest 10%. The higher the ratio, the higher the inequality between these two points in the distribution. Look at the following ratios: 90/10 ratio = the ratio of Decile 10 income to Decile 1 income 90/50 ratio = the ratio of Decile 10 income to Decile 5 income (the median) 50/10 ratio = the ratio of Decile 5 income (the median) to Decile 1 income. For each of these ratios, explain why policymakers might want to compare the two deciles in the income distribution. What kinds of policies or events could affect these ratios? We will now compare these summary measures (ratios and the Gini coefficient) for a larger group of countries, using OECD data. The OECD has annual data for different ratio measures of income inequality for 42 countries around the world, and has an interactive chart function that plots them for you. Go to the OECD website to access the data. You will see a chart similar to Figure 5.4, showing data for 2015. The countries are ranked from smallest to largest Gini coefficient on the horizontal axis, and the vertical axis gives the Gini coefficient. Compare summary measures of inequality for all available countries on the OECD website: Plot the data for the ratio measures by changing the variable selected in the drop-down menu ‘Gini coefficient’. The three ratio measures we looked at previously are called ‘Interdecile P90/P10’, ‘Interdecile P90/P50’, and ‘Interdecile P50/P10’, respectively. (If you click the ‘Compare variables’ option, you can plot more than one variable (except the Gini coefficient) on the same chart.) For each measure, give an intuitive explanation of how it is measured and what it tells us about income inequality. (For example: What do the larger and smaller values of this measure mean? Which parts of the income distribution does this measure use?) Do countries that rank highly on the Gini coefficient also rank highly on the ratio measures, or do the rankings change depending on the measure used? Based on your answers, explain why it is important to look at more than one summary measure of a distribution. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''OECD countries ranked according to their Gini coefficient (2015). '' /> OECD countries ranked according to their Gini coefficient (2015). Figure 5.4 OECD countries ranked according to their Gini coefficient (2015). The Gini coefficient and the ratios we have used are common measures of inequality, but there are other ways to measure income inequality. Go to the Chartbook of Economic Inequality, which contains five measures of income inequality, including the Gini coefficient, for 25 countries around the world. Choose two measures of income inequality that you find interesting (excluding the Gini coefficient). For each measure, give an intuitive explanation of how it is measured and what we can learn about income inequality from it. You may find the page on ‘Inequality measures’ helpful. (For example: What do larger or smaller values of this measure mean? Which parts of the income distribution does this measure use?) On the Chartbook of Economic Inequality main page, charts of these measures are available for all countries shown in green on the map. For two countries of your choice, look at the charts and explain what these measures tell us about inequality in those countries. Part 5.2 Measuring other kinds of inequality Learning objectives for this part research other dimensions of inequality and how they are measured. There are many ways to measure income inequality, but income inequality is only one dimension of inequality within a country. To get a more complete picture of inequality within a country, we need to look at other areas in which there may be inequality in outcomes. We will explore two particular areas, focusing on the measures used and their limitations: health inequality gender inequality in education. First, we will look at how researchers have measured inequality in health-related outcomes. Besides income, health is an important aspect of wellbeing, partly because it determines how long an individual will be alive to enjoy his or her income. If two people had the same annual income throughout their lives, but the one person had a much shorter life than the other, we might say that the distribution of wellbeing is unequal, despite annual incomes being equal. As with income, inequality in life expectancy can be measured using a Gini coefficient. In the study ‘Mortality inequality’, researcher Sam Peltzman (2009) estimated Gini coefficients for life expectancy based on the distribution of total years lived (life-years) across people born in a given year (birth cohort). If everybody born in a given year lived the same number of years, then the total years lived would be divided equally among these people (perfect equality). If a few people lived very long lives but everybody else lived very short lives, then there would be a high degree of inequality (Gini coefficient close to 1). We will now look at mortality inequality Gini coefficients for ten countries around the world. First, download the data: Go to the ‘Health Inequality’ section of the Our World in Data website. Under the heading ‘Mortality Inequality’, click the ‘Data’ button at the bottom of the chart shown. Click the blue button that appears to download the data in csv format. Using the mortality inequality data: Plot all the countries on the same line chart, with Gini coefficient on the vertical axis and year (1952–2002) on the horizontal axis. Make sure to include a legend showing country names and label the axes appropriately. Describe any general patterns in mortality inequality over time, as well as any similarities and differences between countries. Now compare the Gini coefficients in the first year of your line chart (1952) with the last year (2002). For the year 1952, sort the countries according to their mortality inequality Gini coefficient from smallest to largest. Plot a column chart showing these Gini coefficients on the vertical axis, and country on the horizontal axis. Add data labels to display the Gini coefficient for each country. Repeat Question 2(a) for the year 2002. Comparing your charts for 1952 and 2002, have the rankings between countries changed? Suggest some explanations for any observed changes. (You may want to do some additional research, for example, look at the healthcare systems of these countries.) Excel walk-through 5.3 Drawing a column chart with sorted values <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to draw a column chart with sorted values. '' /> Figure 5.5 How to draw a column chart with sorted values. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Sort the data from smallest to largest Gini coefficient : We will use the Gini coefficients for 1952 as an example. The data has been filtered to show values for the year 1952 only. '' /> Sort the data from smallest to largest Gini coefficient We will use the Gini coefficients for 1952 as an example. The data has been filtered to show values for the year 1952 only. Figure 5.5a We will use the Gini coefficients for 1952 as an example. The data has been filtered to show values for the year 1952 only. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Sort the data from smallest to largest Gini coefficient : After step 2, the countries will now be sorted according to their Gini coefficient (from smallest to largest). '' /> Sort the data from smallest to largest Gini coefficient After step 2, the countries will now be sorted according to their Gini coefficient (from smallest to largest). Figure 5.5b After step 2, the countries will now be sorted according to their Gini coefficient (from smallest to largest). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a column chart : Now we will make a column chart with the sorted Gini coefficients. After step 5, the column chart will look like the one shown above. '' /> Draw a column chart Now we will make a column chart with the sorted Gini coefficients. After step 5, the column chart will look like the one shown above. Figure 5.5c Now we will make a column chart with the sorted Gini coefficients. After step 5, the column chart will look like the one shown above. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis labels to country names : Now we will change the horizontal axis labels to country names. '' /> Change the horizontal axis labels to country names Now we will change the horizontal axis labels to country names. Figure 5.5d Now we will change the horizontal axis labels to country names. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis labels to country names : After step 8, the horizontal axis labels are now country names. '' /> Change the horizontal axis labels to country names After step 8, the horizontal axis labels are now country names. Figure 5.5e After step 8, the horizontal axis labels are now country names. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add data labels to the columns : Data labels will make the vertical values easier to see, especially for values that are very close to each other. After step 9, the Gini coefficients will appear in boxes above the columns. '' /> Add data labels to the columns Data labels will make the vertical values easier to see, especially for values that are very close to each other. After step 9, the Gini coefficients will appear in boxes above the columns. Figure 5.5f Data labels will make the vertical values easier to see, especially for values that are very close to each other. After step 9, the Gini coefficients will appear in boxes above the columns. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Round the Gini coefficients to two decimal places : The chart may be too crowded at first because the data labels are not rounded to two decimal places. If we round the Gini coefficient values, the data labels will change accordingly. '' /> Round the Gini coefficients to two decimal places The chart may be too crowded at first because the data labels are not rounded to two decimal places. If we round the Gini coefficient values, the data labels will change accordingly. Figure 5.5g The chart may be too crowded at first because the data labels are not rounded to two decimal places. If we round the Gini coefficient values, the data labels will change accordingly. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-h.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-h-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-h-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-h-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-05-h.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add axis titles and a chart title : After step 16, your chart will look similar in style to that of Figure 5.4. '' /> Add axis titles and a chart title After step 16, your chart will look similar in style to that of Figure 5.4. Figure 5.5h After step 16, your chart will look similar in style to that of Figure 5.4. Note: Questions 3 and 4 can be done independently of each other. Other measures of health inequality, such as those used by the World Health Organization (WHO), are based on access to healthcare, affordability of healthcare, and quality of living conditions. Choose one of the following measures of health inequality to answer Question 3: access to essential medicines basic hospital access composite coverage index. The composite coverage index is a weighted score of coverage for eight different types of healthcare. To download the data for your chosen measure: If you choose to look at either the access to essential medicines or the basic hospital access measure, go to the WHO’s Universal Health Coverage Data Portal, click on the tab ‘Explore UHC Indicators’, and select your chosen measure. A drop-down menu with three buttons will appear: ‘Map’ (or ‘Graph’) shows a visual description of the data, ‘Data’ contains the data files, and ‘Metadata’ contains information about your chosen measure. Click on the ‘Data’ button, then select ‘CSV table’ from the ‘Download complete data set as’ list. If you choose to look at the composite coverage index measure, go to WHO’s Global Health Observatory data repository. The index is given for subgroups of the population, by economic status, education, and place of residence. Choose one of these categories, and download the data by clicking ‘CSV table’ from the ‘Download complete data set as’ list. You can read further information about this index in the WHO’s technical notes. For your chosen measure: Explain how it is constructed and what outcomes it assesses. Create an appropriate chart to summarize the data for all available countries. (You can replicate a chart shown on the website or draw a similar chart.) Explain what your chart shows about health inequality within and between countries, and discuss the limitations of using this measure (for example, measurement issues or other aspects of inequality that this measure ignores). Since an individual’s income and available options in later life partly depend on their level of education, inequality in educational access or attainment can lead to inequality in income and other outcomes. Gender inequality can be measured by the share of women at different levels of attainment. We will focus on the aspect of gender inequality in educational attainment, using data from the Our World in Data website, to make our own comparisons between countries and over time. Choose one of the following measures to answer Question 4: gender gap in primary education (share of enrolled female primary education students) share of women, between 15 and 19 years old, with no education share of women, 15 years and older, with no education. To download the data for your chosen measure: Go to the ‘Educational Mobility and Inequality’ section of the Our World in Data website, and find the chart for your chosen measure. Click the ‘Data’ button at the bottom of the chart, then click the blue button that appears to download the data in csv format. For your chosen measure: Choose ten countries that have data from 1980 to 2010. Plot your chosen countries on the same line chart, with year on the horizontal axis and share on the vertical axis. Make sure to include a legend showing country names and label the axes appropriately. Describe any general patterns in gender inequality in education over time, as well as any similarities and differences between countries. Calculate the change in the value of this measure between 1980 and 2010 for each country chosen. Sort these countries according to this value, from the smallest change to largest change. Now plot a column chart showing the change (1980 to 2010) on the vertical axis, and country on the horizontal axis. Add data labels to display the value for each country. Which country had the largest change? Which country had the smallest change? Suggest some explanations for your observations in Questions 4(b) and (d). (You may want to do some background research on your chosen countries.) Discuss the limitations of using this measure to assess the degree of gender inequality in educational attainment and propose some alternative measures."
});
index.addDoc({
    id: 29,
    title: "Doing Economics: Empirical Project 5: Working in R",
    content: "Empirical Project 5 Working in R Download the code To download the code chunks used in this project, right-click on the download link and select ‘Save Link As…’. You’ll need to save the code download to your working directory, and open it in RStudio. Don’t forget to also download the data into your working directory by following the steps in this project. R-specific learning objectives In addition to the learning objectives for this project, in Part 5.1 you will learn how to use loops to repeat specified tasks for a list of values (Note: this is an extension task so may not apply to all users). Getting started in R For this project you will need the following packages: tidyverse, to help with data manipulation readxl, to import an Excel spreadsheet ineq, to calculate inequality measures reshape2, to rearrange a dataframe. If you need to install any of these packages, run the following code: install.packages(c( ''readxl'', ''tidyverse'', ''ineq'', ''reshape2'')) You can import these libraries now, or when they are used in the R walk-throughs below. library(readxl) library(tidyverse) library(ineq) library(reshape2) Part 5.1 Measuring income inequality Learning objectives for this part draw Lorenz curves calculate and interpret the Gini coefficient interpret alternative measures of income inequality. One way to visualize the income distribution in a population is to draw a Lorenz curve. This curve shows the entire population lined up along the horizontal axis from the poorest to the richest. The height of the curve at any point on the vertical axis indicates the fraction of total income received by the fraction of the population given by that point on the horizontal axis. We will start by using income decile data from the Global Consumption and Income Project to draw Lorenz curves and compare changes in the income distribution of a country over time. Note that income here refers to market income, which does not take into account taxes or government transfers (see Section 5.10 of Economy, Society, and Public Policy for further details). To answer the question below: Go to the Globalinc website and download the Excel file containing the data by clicking ‘xlsx’. Save it in an easily accessible location, such as a folder on your Desktop or in your personal folder. Import the data into R as explained in R walk-through 5.1. R walk-through 5.1 Importing an Excel file (either .xlsx or .xls format) into R As we are importing an Excel file, we use the read_excel function from the readxl package. The file is called GCIPrawdata.xlsx. Before you import the file into R, open the datafile in Excel to understand its structure. You will see that the data is all in one worksheet (which is convenient), and that the headings for the variables are in the third row. Hence we will use the skip = 2 option in the read_excel function to skip the first two rows. library(tidyverse) library(readxl) # Set your working directory to the correct folder. # Insert your file path for 'YOURFILEPATH'. setwd(''YOURFILEPATH'') decile_data <- read_excel(''GCIPrawdata.xlsx'', skip = 2) The data is now in a ‘tibble’ (like a spreadsheet for R). Let’s use the head function to look at the first few rows: head(decile_data) ## # A tibble: 6 x 14 ## Country Year `Decile 1 Income` `Decile 2 Income` `Decile 3 Income` ## <chr> <dbl> <dbl> <dbl> <dbl> ## 1 Afghanistan 1980 206 350 455 ## 2 Afghanistan 1981 212 361 469 ## 3 Afghanistan 1982 221 377 490 ## 4 Afghanistan 1983 238 405 527 ## 5 Afghanistan 1984 249 424 551 ## 6 Afghanistan 1985 256 435 566 ## # ... with 9 more variables: `Decile 4 Income` <dbl>, `Decile 5 ## # Income` <dbl>, `Decile 6 Income` <dbl>, `Decile 7 Income` <dbl>, ## # `Decile 8 Income` <dbl>, `Decile 9 Income` <dbl>, `Decile 10 ## # Income` <dbl>, `Mean Income` <dbl>, Population <dbl> As you can see, each row shows data for a different country-year combination. The first row is for Afghanistan in 1980, and the first value (in the third column) is 206, for the variable Decile 1 Income. This value indicates that the mean annual income of the poorest 10% in Afghanistan was the equivalent of 206 USD (in 1980, adjusted using purchasing power parity). Looking at the next column, you can see that the mean income of the next richest 10% (those in the 11th to 20th percentiles for income) was 350. To see the list of variables, we use the str function. str(decile_data) ## Classes 'tbl_df', 'tbl' and 'data.frame': 4799 obs. of 14 variables: ## $ Country : chr ''Afghanistan'' ''Afghanistan'' ''Afghanistan'' ''Afghanistan'' ... ## $ Year : num 1980 1981 1982 1983 1984 ... ## $ Decile 1 Income : num 206 212 221 238 249 256 268 243 223 202 ... ## $ Decile 2 Income : num 350 361 377 405 424 435 457 414 380 344 ... ## $ Decile 3 Income : num 455 469 490 527 551 566 594 539 493 447 ... ## $ Decile 4 Income : num 556 574 599 644 674 692 726 659 603 547 ... ## $ Decile 5 Income : num 665 686 716 771 806 828 869 788 722 654 ... ## $ Decile 6 Income : num 793 818 854 919 961 ... ## $ Decile 7 Income : num 955 986 1029 1107 1157 ... ## $ Decile 8 Income : num 1187 1225 1278 1376 1438 ... ## $ Decile 9 Income : num 1594 1645 1717 1848 1932 ... ## $ Decile 10 Income: num 3542 3655 3814 4105 4291 ... ## $ Mean Income : num 1030 1063 1109 1194 1248 ... ## $ Population : num 13211412 12996923 12667001 12279095 11912510 ... In addition to the country, year, and the ten income deciles, we have mean income and the population. To draw Lorenz curves, we need to calculate the cumulative share of total income owned by each decile (these will be the vertical axis values). The cumulative income share of a particular decile is the proportion of total income held by that decile and all the deciles below it. For example, if Decile 1 has 1/10 of total income and Decile 2 has 2/10 of total income, the cumulative income share of Decile 2 is 3/10 (or 0.3). Choose two countries. You will be using their data, for 1980 and 2014, as the basis for your Lorenz curves. Use the country data you have selected to calculate the cumulative income share of each decile. (Remember that each decile represents 10% of the population.) R walk-through 5.2 Calculating cumulative shares using the cumsum function Here we have chosen China (a country that recently underwent enormous economic changes) and the US (a developed country). We use the subset function to create a new dataset (called temp) containing only the countries and years we need. # Select the data for the chosen country and years sel_Year <- c(1980, 2014) sel_Country <- c(''United States'', ''China'') temp <- subset( decile_data, (decile_data$Country %in% sel_Country) & (decile_data$Year %in% sel_Year)) temp ## # A tibble: 4 x 14 ## Country Year `Decile 1 Income` `Decile 2 Income` `Decile 3 Incom~ ## <chr> <dbl> <dbl> <dbl> <dbl> ## 1 China 1980 79 113 146 ## 2 China 2014 448 927 1440 ## 3 United States 1980 3392 5820 7855 ## 4 United States 2014 3778 6534 9069 ## # ... with 9 more variables: `Decile 4 Income` <dbl>, `Decile 5 ## # Income` <dbl>, `Decile 6 Income` <dbl>, `Decile 7 Income` <dbl>, ## # `Decile 8 Income` <dbl>, `Decile 9 Income` <dbl>, `Decile 10 ## # Income` <dbl>, `Mean Income` <dbl>, Population <dbl> Before we calculate cumulative income shares, we need to calculate the total income for each country-year combination using the mean income and the population size. print(''Total incomes are:'') ## [1] ''Total incomes are:'' total_income <- temp[, ''Mean Income''] * temp[, ''Population''] total_income ## Mean Income ## 1 2.472624e+11 ## 2 6.609944e+12 ## 3 3.366422e+12 ## 4 6.401280e+12 These numbers are very large, so for our purpose it is easier to assume that there is only one person in each decile, in other words the total income is 10 times the mean income. This simplification works because, by definition, each decile has exactly the same number of people (10% of the population). We will be using the very useful cumsum function (short for ‘cumulative sum’) to calculate the cumulative income. To see what this function does, look at this simple example. test <- c(2, 4, 10, 22) cumsum(test) ## [1] 2 6 16 38 You can see that each number in the sequence is the sum of all the preceding numbers (including itself), for example, we got the third number, 16, by adding 2, 4, and 10. We now apply this function to calculate the cumulative income shares for China (1980) and save them as cum_inc_share_c80. # Pick the deciles (Columns 3 to 12) in Row 1 (China, 1980) decs_c80 <- unlist(temp[1, 3:12]) # The unlist function transforms temp[1, 3:12] from a # tibble to simple vector with data which simplifies the # calculations. # Give the total income, assuming a population of 10 total_inc <- 10 * unlist(temp[1, ''Mean Income'']) cum_inc_share_c80 = cumsum(decs_c80) / total_inc cum_inc_share_c80 ## Decile 1 Income Decile 2 Income Decile 3 Income Decile 4 Income ## 0.03134921 0.07619048 0.13412698 0.20436508 ## Decile 5 Income Decile 6 Income Decile 7 Income Decile 8 Income ## 0.28769841 0.38492063 0.49841270 0.63174603 ## Decile 9 Income Decile 10 Income ## 0.79206349 0.99841270 We repeat the same process for China in 2014 (cum_inc_share_c14) and for the US in 1980 and 2014 (cum_inc_share_us80 and cum_inc_share_us14 respectively). # For China, 2014 # Go to Row 2 (China, 2014) decs_c14 <- unlist(temp[2, 3:12]) # Give the total income, assuming a population of 10 total_inc <- 10 * unlist(temp[2, ''Mean Income'']) cum_inc_share_c14 = cumsum(decs_c14) / total_inc # For the US, 1980 # Select Row 3 (USA, 1980) decs_us80 <- unlist(temp[3, 3:12]) # Give the total income, assuming a population of 10 total_inc <- 10 * unlist(temp[3, ''Mean Income'']) cum_inc_share_us80 = cumsum(decs_us80) / total_inc # For the US, 2014 # Select Row 4 (USA, 2014) decs_us14 <- unlist(temp[4, 3:12]) # Give the total income, assuming a population of 10 total_inc <- 10 * unlist(temp[4, ''Mean Income'']) cum_inc_share_us14 = cumsum(decs_us14) / total_inc Use the cumulative income shares to draw Lorenz curves for each country in order to visually compare the income distributions over time. Draw a line chart with cumulative share of population on the horizontal axis and cumulative share of income on the vertical axis. Make sure to include a chart legend, and label your axes and chart appropriately. Follow the steps in R walk-through 5.3 to add a straight line representing perfect equality to each chart. (Hint: If income was shared equally across the population, the bottom 10% of people would have 10% of the total income, the bottom 20% would have 20% of the total income, and so on.) R walk-through 5.3 Drawing Lorenz curves Let us plot the cumulative income shares for China (1980), which we previously stored in the variable cum_inc_share_c80. The plot function makes the basic chart, with the cumulative income share in blue. We then use the functions abline to add the perfect equality line (in black), and title to add a chart title. plot(cum_inc_share_c80, type = ''l'', col = ''blue'', lwd = 2, ylab = ''Cumulative income share'') # Add the perfect equality line abline(a = 0, b = 0.1, col = ''black'', lwd = 2) title(''Lorenz curve, China, 1980'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-01.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-01-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-01-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-01-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-01.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Lorenz curve, China, 1980. '' /> Lorenz curve, China, 1980. Figure 5.1 Lorenz curve, China, 1980. The blue line is the Lorenz curve. The Gini coefficient is the ratio of the area between the two lines and the total area under the black line. We will calculate the Gini coefficient in R walk-through 5.4. Now we add the other Lorenz curves to the chart using the lines function. We use the col= option to specify a different colour for each line, and the lty option to make the line pattern solid for 2014 data and dashed for 1980 data. Finally, we use the legend function to add a chart legend in the top left corner of the chart. plot(cum_inc_share_c80, type = ''l'', col = ''blue'', lty = 2, lwd = 2, xlab = ''Deciles'', ylab = ''Cumulative income share'') # Add the perfect equality line abline(a = 0, b = 0.1, col = ''black'', lwd = 2) # lty = 1 = dashed line lines(cum_inc_share_c14, col = ''green'', lty = 1, lwd = 2) # lty = 2 = solid line lines(cum_inc_share_us80, col = ''red'', lty = 2, lwd = 2) lines(cum_inc_share_us14, col = ''orange'', lty = 1, lwd = 2) title(''Lorenz curves, China and the US (1980 and 2014)'') legend(''topleft'', lty = 2:1, lwd = 2, cex = 1.2, legend = c(''China, 1980'', ''China, 2014'', ''US, 1980'', ''US, 2014''), col = c(''blue'', ''green'', ''red'', ''orange'')) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-02.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-02-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-02-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-02-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-02.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Lorenz curves, China and the US (1980 and 2014). '' /> Lorenz curves, China and the US (1980 and 2014). Figure 5.2 Lorenz curves, China and the US (1980 and 2014). As the chart shows, the income distribution has changed more clearly for China (from the blue to the green line) than for the US (from the orange to the red line). Using your Lorenz curves: Compare the distribution of income across time for each country. Compare the distribution of income across countries for each year (1980 and 2014). Suggest some explanations for any similarities and differences you observe. (You may want to research your chosen countries to see if there were any changes in government policy, political events, or other factors that may affect the income distribution.) A rough way to compare income distributions is to use a summary measure such as the Gini coefficient. The Gini coefficient ranges from 0 (complete equality) to 1 (complete inequality). It is calculated by dividing the area between the Lorenz curve and the perfect equality line, by the total area underneath the perfect equality line. Intuitively, the further away the Lorenz curve is from the perfect equality line, the more unequal the income distribution is, and the higher the Gini coefficient will be. To calculate the Gini coefficient you can either use a Gini coefficient calculator, or calculate it directly in R as shown in R walk-through 5.4. Calculate the Gini coefficient for each of your Lorenz curves. You should have four coefficients in total. Label each Lorenz curve with its corresponding Gini coefficient, and check that the coefficients are consistent with what you see in your charts. R walk-through 5.4 Calculating Gini coefficients The Gini coefficient is graphically represented by dividing the area between the perfect equality line and the Lorenz curve by the total area under the perfect equality line (see Section 5.9 of Economy, Society, and Public Policy for further details). You could calculate this area manually, by decomposing the area under the Lorenz curve into rectangles and triangles, but as with so many problems, someone else has already figured out how to do that and has provided R users with a package (called ineq) that does this task for you. The function that calculates Gini coefficients from a vector of numbers is called Gini, and we apply it to the income deciles from R walk-through 5.3 (decs_c80, decs_c14, decs_us80, and decs_us14). # Load the ineq library library(ineq) # The decile mean incomes from R walk-through 5.3 are used. g_c80 <- Gini(decs_c80) g_c14 <- Gini(decs_c14) g_us80 <- Gini(decs_us80) g_us14 <- Gini(decs_us14) paste(''Gini coefficients'') ## [1] ''Gini coefficients'' paste(''China - 1980: '', round(g_c80, 2), '', 2014: '', round(g_c14, 2)) ## [1] ''China - 1980: 0.29 , 2014: 0.51'' paste(''United States - 1980: '', round(g_us80, 2), '', 2014: '', round(g_us14, 2)) ## [1] ''United States - 1980: 0.34 , 2014: 0.4'' Now we make the same line chart (simply copy and paste the code from R walk-through 5.3, but use the text function to label curves with their respective Gini coefficients. The two numbers in the text function specify the coordinates (horizontal and vertical) where the text should be written (experiment for yourself to find the best place to put the labels), and we used the round function to show the first three digits of the calculated Gini coefficients. plot(cum_inc_share_c80, type = ''l'', col = ''blue'', lty = 2, lwd = 2, xlab = ''Deciles'', ylab = ''Cumulative income share'') # Add the perfect equality line abline(a = 0, b = 0.1, col = ''black'', lwd = 2) # lty = 1 = dashed line lines(cum_inc_share_c14, col = ''green'', lty = 1, lwd = 2) # lty = 2 = solid line lines(cum_inc_share_us80, col = ''red'', lty = 2, lwd = 2) lines(cum_inc_share_us14, col = ''orange'', lty = 1, lwd = 2) title(''Lorenz curves, China and the US (1980 and 2014)'') legend(''topleft'', lty = 2:1, lwd = 2, cex = 1.2, legend = c(''China, 1980'', ''China, 2014'', ''US, 1980'', ''US, 2014''), col = c(''blue'', ''green'', ''red'', ''orange'')) text(8.5, 0.78, round(g_c80, digits = 3)) text(9.4, 0.6, round(g_c14, digits = 3)) text(5.7, 0.38, round(g_us80, digits = 3)) text(6.4, 0.3, round(g_us14, digits = 3)) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Lorenz curves, China and the US (1980 and 2014). '' /> Lorenz curves, China and the US (1980 and 2014). Figure 5.3 Lorenz curves, China and the US (1980 and 2014). The Gini coefficients for both countries have increased, confirming what we already saw from the Lorenz curves that in both countries the income distribution has become more unequal. Extension R walk-through 5.5 Calculating Gini coefficients for all countries and all years using a loop In this extension walk-through, we show you how to calculate the Gini coefficient for all countries and years in your dataset. This sounds like a tedious task, and indeed if we were to use the same method as before it would be mind-numbing. However, we have a powerful programming language at hand, and this is the time to use it. Here we use a very useful programming tool you may not have come across yet: loops. Loops are used to repeat a specified block of code. There are a few types of loop, and here we will use a ‘for’ loop, meaning that we ask R to apply the same code to each number or item in a specific list (i.e. repeat the code ‘for’ a list of numbers/items). Let’s start with a very simple case: printing the first 10 square numbers. In coding terms, we are printing the values for i^2 for the numbers i=1, ..., 10. for (i in seq(1, 10)){ print(i^2) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 ## [1] 36 ## [1] 49 ## [1] 64 ## [1] 81 ## [1] 100 In the above command, seq(1, 10) creates a vector of numbers from 1 to 10 (1, 2, 3, …, 10). The command for (i in seq(1, 10)) defines the variable i initially as 1, then performs all the commands that are between the curly brackets for each value of i (typically these commands will involve the variable i). Here our command prints the value of i^2 for each value of i. Check that you understand the syntax above by modifying it to print only the first 5 square numbers, or adding 2 to the numbers from 1 to 10 (instead of squaring these numbers). Now we use loops to complete our task. We begin by creating a new variable in our dataset, gini, which we initially set to 0 for all country-year combinations. decile_data$gini <- 0 Now we use a loop to run through all the rows in our dataset (country-year combinations). For each row we will repeat the Gini coefficient calculation from R walk-through 5.4 and save the resulting value in the gini variable we created. # Give us the number of rows in decile_data noc <- nrow(decile_data) for (i in seq(1, noc)){ # Go to Row I to get the decile data decs_i <- unlist(decile_data[i, 3:12]) decile_data$gini[i] <- Gini(decs_i) } With this code, we calculated 4,799 Gini coefficients without having to manually run the same command 4,799 times. We now look at some summary measures for the gini variable. summary(decile_data$gini) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.1791 0.3470 0.4814 0.4617 0.5700 0.7386 The average Gini coefficient is 0.46, the maximum is 0.74, and the minimum 0.18. Let’s look at these extreme cases. First we will look at the extremely equal income distributions (those with a Gini coefficient smaller than 0.20): temp <- subset( decile_data, decile_data$gini < 0.20, select = c(''Country'', ''Year'', ''gini'')) temp ## # A tibble: 17 x 3 ## Country Year gini ## <chr> <dbl> <dbl> ## 1 Bulgaria 1987 0.191 ## 2 Czech Republic 1985 0.195 ## 3 Czech Republic 1986 0.194 ## 4 Czech Republic 1987 0.192 ## 5 Czech Republic 1988 0.191 ## 6 Czech Republic 1989 0.194 ## 7 Czech Republic 1990 0.196 ## 8 Czech Republic 1991 0.199 ## 9 Slovak Republic 1985 0.195 ## 10 Slovak Republic 1986 0.194 ## 11 Slovak Republic 1987 0.193 ## 12 Slovak Republic 1988 0.192 ## 13 Slovak Republic 1989 0.193 ## 14 Slovak Republic 1990 0.194 ## 15 Slovak Republic 1991 0.195 ## 16 Slovak Republic 1992 0.196 ## 17 Slovak Republic 1993 0.179 These correspond to eastern European countries before the fall of communism. Now the most unequal countries (those with a Gini coefficient larger than 0.73): temp <- subset( decile_data, decile_data$gini > 0.73, select = c(''Country'', ''Year'', ''gini'')) temp ## # A tibble: 27 x 3 ## Country Year gini ## <chr> <dbl> <dbl> ## 1 Burkina Faso 1980 0.738 ## 2 Burkina Faso 1981 0.738 ## 3 Burkina Faso 1982 0.738 ## 4 Burkina Faso 1983 0.738 ## 5 Burkina Faso 1984 0.738 ## 6 Burkina Faso 1985 0.738 ## 7 Burkina Faso 1986 0.738 ## 8 Burkina Faso 1987 0.738 ## 9 Burkina Faso 1988 0.738 ## 10 Burkina Faso 1989 0.739 ## # ... with 17 more rows Extension R walk-through 5.6 Plotting time series of Gini coefficients, using ggplot In this extension walk-through, we show you how to make time series plots (time on the horizontal axis, the variable of interest on the vertical axis) with Gini coefficients for a list of countries of your choice. There are many ways to plot data in R, one being the standard plotting function (plot) we used in previous walk-throughs. Another (and perhaps more beautiful) way is to use the ggplot function, which is part of the tidyverse package we loaded earlier. Our dataset is already in a format which the ggplot function can easily use (the ‘long’ format, where each row corresponds to a different country-year combination). First we use the subset function to select a small list of countries and save their data as temp_data. As an example, we have chosen four anglophone countries: the UK, the US, Ireland, and Australia. temp_data <- subset( decile_data, Country %in% c(''United Kingdom'', ''United States'', ''Ireland'', ''Australia'')) Now we plot the data using ggplot. ggplot(temp_data, aes(x = Year, y = gini, color = Country)) + geom_line(size = 1) + theme_bw() + ylab(''Gini'') + # Add a title ggtitle(''Gini coefficients for anglophone countries'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Time series plots of Gini coefficients for anglophone countries. '' /> Time series plots of Gini coefficients for anglophone countries. Figure 5.4 Time series plots of Gini coefficients for anglophone countries. We asked the ggplot function to use the temp_data dataframe, with Year on the horizontal axis (x = ) and gini on the vertical axis (y = ). The color option indicates which variable we use to separate the data (use a different line for each unique item in Country). The first line of code sets up the chart, and the + geom_line(size = 1) then instructs R to draw lines. (See what happens if you replace + geom_line(size = 1) with + geom_point(size = 1).) ggplot assumes that the different lines you want to show are identified through the different values in one variable (here, the Country variable). If your data is formatted differently, for example, if you have one variable for the Gini of each country (‘wide’ format), then in order to use ggplot you will first have to transform the dataset into ‘long’ format. Doing so is beyond the scope of this task, however you can find a worked example online, such as ‘R TSplots’.1 Project 4 also explains how to transform data between ‘wide’ and ‘long’ formats. The ggplot package is extremely powerful, and if you want to produce a variety of different charts, you may want to read more about that package, for example, see a Harvard R tutorial or an R statistics tutorial for great examples that include code. Now we will look at other measures of income inequality and see how they can be used along with the Gini coefficient to summarize a country’s income distribution. Instead of summarizing the entire income distribution like the Gini coefficient does, we can take the ratio of incomes at two points in the distribution. For example, the 90/10 ratio takes the ratio of the top 10% of incomes (Decile 10) to the lowest 10% of incomes (Decile 1). A 90/10 ratio of 5 means that the richest 10% earns 5 times more than the poorest 10%. The higher the ratio, the higher the inequality between these two points in the distribution. Look at the following ratios: 90/10 ratio = the ratio of Decile 10 income to Decile 1 income 90/50 ratio = the ratio of Decile 10 income to Decile 5 income (the median) 50/10 ratio = the ratio of Decile 5 income (the median) to Decile 1 income. For each of these ratios, explain why policymakers might want to compare these two deciles in the income distribution. What kinds of policies or events could affect these ratios? We will now compare these summary measures (ratios and the Gini coefficient) for a larger group of countries, using OECD data. The OECD has annual data for different ratio measures of income inequality for 42 countries around the world, and has an interactive chart function that plots them for you. Go to the OECD website to access the data. You will see a chart similar to Figure 5.5, showing data for 2015. The countries are ranked from smallest to largest Gini coefficient on the horizontal axis, and the vertical axis gives the Gini coefficient. Compare summary measures of inequality for all available countries on the OECD website: Plot the data for the ratio measures by changing the variable selected in the drop-down menu ‘Gini coefficient’. The three ratio measures we looked at previously are called ‘Interdecile P90/P10’, ‘Interdecile P90/P50’, and ‘Interdecile P50/P10’, respectively. (If you click the ‘Compare variables’ option, you can plot more than one variable (except the Gini coefficient) on the same chart.) For each measure, give an intuitive explanation of how it is measured and what it tells us about income inequality. (For example: What do the larger and smaller values of this measure mean? Which parts of the income distribution does this measure use?) Do countries that rank highly on the Gini coefficient also rank highly on the ratio measures, or do the rankings change depending on the measure used? Based on your answers, explain why it is important to look at more than one summary measure of a distribution. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''OECD countries ranked according to their Gini coefficient (2015). '' /> OECD countries ranked according to their Gini coefficient (2015). Figure 5.5 OECD countries ranked according to their Gini coefficient (2015). The Gini coefficient and the ratios we have used are common measures of inequality, but there are other ways to measure income inequality. Go to the Chartbook of Economic Inequality, which contains five measures of income inequality, including the Gini coefficient, for 25 countries around the world. Choose two measures of income inequality that you find interesting (excluding the Gini coefficient). For each measure, give an intuitive explanation of how it is measured and what we can learn about income inequality from it. You may find the page on ‘Inequality measures’ helpful. (For example: What do larger or smaller values of this measure mean? Which parts of the income distribution does this measure use?) On the Chartbook of Economic Inequality main page, charts of these measures are available for all countries shown in green on the map. For two countries of your choice, look at the charts and explain what these measures tell us about inequality in those countries. Part 5.2 Measuring other kinds of inequality Learning objectives for this part research other dimensions of inequality and how they are measured. There are many ways to measure income inequality, but income inequality is only one dimension of inequality within a country. To get a more complete picture of inequality within a country, we need to look at other areas in which there may be inequality in outcomes. We will explore two particular areas: health inequality gender inequality in education. First, we will look at how researchers have measured inequality in health-related outcomes. Besides income, health is an important aspect of wellbeing, partly because it determines how long an individual will be alive to enjoy his or her income. If two people had the same annual income throughout their lives, but one person had a much shorter life than the other, we might say that the distribution of wellbeing is unequal, despite annual incomes being equal. As with income, inequality in life expectancy can be measured using a Gini coefficient. In the study ‘Mortality inequality’, researcher Sam Peltzman (2009) estimated Gini coefficients for life expectancy based on the distribution of total years lived (life-years) across people born in a given year (birth cohort). If everybody born in a given year lived the same number of years, then the total years lived would be divided equally among these people (perfect equality). If a few people lived very long lives but everybody else lived very short lives, then there would be a high degree of inequality (Gini coefficient close to 1). We will now look at mortality inequality Gini coefficients for 10 countries around the world. First, download the data: Go to the ‘Health Inequality’ section of the Our World in Data website. In Section 1.1 (Mortality inequality), click the ‘Data’ button at the bottom of the chart shown. Click the blue button that appears to download the data in csv format. Import the data into R and investigate the structure of the data as explained in R walk-through 5.7. R walk-through 5.7 Importing .csv files into R Before importing, make sure the .csv file is saved in your working directory. After importing (using the read.csv function), use the str function to check that the data was imported correctly. # Open the csv file from the working directory health_in <- read.csv('' inequality-of-life-as-measured-by-mortality-gini-coefficient-1742-2002.csv'') str(health_in) ## 'data.frame': 320 obs. of 4 variables: ## $ Entity : Factor w/ 10 levels ''Brazil'',''England and Wales'',..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Code : Factor w/ 10 levels '''',''BRA'',''DEU'',..: 2 2 2 2 2 2 2 2 2 2 ... ## $ Year : int 1892 1897 1902 1907 1912 1917 1922 1927 1932 1937 ... ## $ X.percent.: num 0.566 0.557 0.547 0.482 0.494 ... The variable Entity is the country and the variable X.percent is the health Gini. Let’s change these variable names (to Country and Health, respectively) to clarify what they actually refer to, which will help when writing code (and if we go back to read this code at a later date). # Country is the first variable. names(health_in)[1] <- ''Country'' # Health Gini is the fourth variable. names(health_in)[4] <- ''HGini'' There is another quirk in the data that you may not have noticed in this initial data inspection: All countries have a short code (Code), except for England and Wales (currently blank '''' in the dataframe). As Code is a factor variable, we use the levels function to change the blanks to ''ENW''. levels(health_in$Code)[levels(health_in$Code) == ''''] <- ''ENW'' Tip The way this code works may seem a little mysterious, and you may find it difficult to remember the code for this step. However, an Internet search for ‘R renaming one factor level’ (recall that Code is a factor variable) will show you many ways to achieve this (including the one shown above). Often you will find answers on stackoverflow.com, where experienced coders provide useful help. Using the mortality inequality data: Plot all the countries on the same line chart, with Gini coefficient on the vertical axis and year (1952–2002 only) on the horizontal axis. Make sure to include a legend showing country names, and label the axes appropriately. Describe any general patterns in mortality inequality over time, as well as any similarities and differences between countries. R walk-through 5.8 Creating line graphs with ggplot As shown in R walk-through 5.7, the data is already formatted so that we can use ggplot directly (in ‘long’ format), in other words we have only one variable for the mortality Gini (HGini), and we can separate the data by country using one variable (Country). Most of the code below is similar to our use of ggplot in previous walk-throughs, though this time we added the option labs to change the vertical axis label (y = ) and the option scale_color_brewer to change the colour palette (to clearly differentiate the lines for each country). # Select all data after 1951 temp_data <- subset(health_in, Year > 1951) ggplot(temp_data, aes(x = Year, y = HGini, color = Country)) + geom_line(size = 1) + labs(y = ''Mortality inequality Gini coefficient'') + # Change the colour palette scale_color_brewer(palette = ''Paired'') + theme_bw() + # Add a title ggtitle(''Mortality inequalities'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Mortality inequality Gini coefficients (1952–2002). '' /> Mortality inequality Gini coefficients (1952–2002). Figure 5.6 Mortality inequality Gini coefficients (1952–2002). Now compare the Gini coefficients in the first year of your line chart (1952) with the last year (2002). For the year 1952, sort the countries according to their mortality inequality Gini coefficient from smallest to largest. Plot a column chart showing these Gini coefficients on the vertical axis, and country on the horizontal axis. Repeat Question 2(a) for the year 2002. Comparing your charts for 1952 and 2002, have the rankings between countries changed? Suggest some explanations for any observed changes. (You may want to do some additional research, for example, look at the healthcare systems of these countries.) R walk-through 5.9 Drawing a column chart with sorted values Plot a column chart for 1952 First we use subset to extract the data for 1952 only, and store it in a temporary dataset (tempdata). # Select all data for 1952 temp_data <- subset(health_in, Year == 1952) # Reorder rows in temp_data by the values of HGini temp_data <- temp_data[order(temp_data$HGini), ] temp_data ## Country Code Year HGini ## 279 Sweden SWE 1952 0.1194045 ## 46 England and Wales ENW 1952 0.1319542 ## 310 United States USA 1952 0.1471329 ## 138 Germany DEU 1952 0.1572112 ## 86 France FRA 1952 0.1605238 ## 228 Spain ESP 1952 0.1985371 ## 184 Japan JPN 1952 0.2021728 ## 206 Russia RUS 1952 0.2237161 ## 161 India IND 1952 0.3978703 ## 13 Brazil BRA 1952 0.4103805 The rows are now ordered according to HGini, in ascending order. Let’s use ggplot again. ggplot(temp_data, aes(x = Code, y = HGini)) + geom_bar(stat = ''identity'', width = .5, fill = ''tomato3'') + theme_bw() + labs(title = ''Mortality Gini coefficients (1952)'', caption = ''source: ourworldindata.org/health-inequality'', y = ''Mortality inequality Gini coefficient'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Mortality Gini coefficients (1952). '' /> Mortality Gini coefficients (1952). Figure 5.7 Mortality Gini coefficients (1952). Unfortunately, the columns are not ordered correctly, because when the horizontal axis variable (here, Code) is a factor, then ggplot uses the ordering of the factor levels, which we can see by using the levels function: levels(temp_data$Code) ## [1] ''ENW'' ''BRA'' ''DEU'' ''ESP'' ''FRA'' ''IND'' ''JPN'' ''RUS'' ''SWE'' ''USA'' A blog post from Data Se provides the following code for ‘R geom_bar change order’, and uses the reorder function to reorder the horizontal axis variable (Code) according to the HGini value. ggplot(temp_data, aes(x = reorder(Code, HGini), y = HGini)) + geom_bar(stat = ''identity'', width = .5, fill = ''tomato3'') + coord_cartesian(ylim = c(0, 0.45)) + theme_bw() + labs(title = ''Mortality Gini coefficients (1952)'', x = ''Country'', caption = ''source: ourworldindata.org/health-inequality'', y = ''Mortality inequality Gini coefficient'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-08.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-08-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-08-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-08-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-08.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Mortality Gini coefficients (1952). '' /> Mortality Gini coefficients (1952). Figure 5.8 Mortality Gini coefficients (1952). Plot a column chart for 2002 We want to compare this ranking with the ranking of 2002. First we extract the relevant data again. # Select all data for 2002 temp_data <- subset(health_in, Year == 2002) ggplot(temp_data, aes(x = reorder(Code, HGini), y = HGini)) + geom_bar(stat = ''identity'', width = .5, ylim = c(0, 0.45), fill = ''tomato3'') + # Adjust vertical axis scale for comparability with 1952 coord_cartesian(ylim = c(0, 0.45)) + theme_bw() + labs(title = ''Mortality Gini coefficients (2002)'', x = ''Country'', caption = ''source: ourworldindata.org/health-inequality'', y = ''Mortality inequality Gini coefficient'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-09.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-09-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-09-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-09-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-09.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Mortality Gini coefficients (2002). '' /> Mortality Gini coefficients (2002). Figure 5.9 Mortality Gini coefficients (2002). It is fairly easy to plot the data for both years in the same chart, by extracting both years into the same temporary dataset. # Select all data for 1952 and 2002 temp_data <- subset(health_in, Year %in% c(''1952'', ''2002'')) temp_data$Year <- factor(temp_data$Year) ggplot(temp_data, aes(x = reorder(Code, HGini), y = HGini, fill = Year)) + geom_bar(position=''dodge'', stat = ''identity'') + theme_bw() + labs( title = ''Mortality Gini coefficients (1952 and 2002)'', x = ''Country'', caption = ''source: ourworldindata.org/health-inequality'', y = ''Mortality inequality Gini coefficient'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-10.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-10-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-10-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-10-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-10.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Mortality Gini coefficients (1952 and 2002). '' /> Mortality Gini coefficients (1952 and 2002). Figure 5.10 Mortality Gini coefficients (1952 and 2002). Now the country ordering is in terms of the average HGini, rather than HGini in 1952 (which might have made comparisons easier). Note: Questions 3 and 4 can be done independently of each other. Other measures of health inequality, such as those used by the World Health Organization (WHO), are based on access to healthcare, affordability of healthcare, and quality of living conditions. Choose one of the following measures of health inequality to answer Question 3: access to essential medicines basic hospital access composite coverage index. The composite coverage index is a weighted score of coverage for eight different types of healthcare. To download the data for your chosen measure: If you choose to look at either the access to essential medicines or the basic hospital access measure, go to the WHO’s Universal Health Coverage Data Portal, click on the tab ‘Explore UHC Indicators’, and select your chosen measure. A drop-down menu with three buttons will appear: ‘Map’ (or ‘Graph’) shows a visual description of the data, ‘Data’ contains the data files, and ‘Metadata’ contains information about your chosen measure. Click on the ‘Data’ button, then select ‘CSV table’ from the ‘Download complete data set as’ list. If you choose to look at the composite coverage index measure, go to WHO’s Global Health Observatory data repository. The index is given for subgroups of the population, by economic status, education, and place of residence. Choose one of these categories, and download the data by clicking ‘CSV table’ from the ‘Download complete data set as’ list. You can read further information about this index in the WHO’s technical notes. For your chosen measure: Explain how it is constructed and what outcomes it assesses. Create an appropriate chart to summarize the data for all available countries. (You can replicate a chart shown on the website or draw a similar chart.) Explain what your chart shows about health inequality within and between countries, and discuss the limitations of using this measure (for example, measurement issues or other aspects of inequality that this measure ignores). R walk-through 5.10 Drawing a column chart with sorted values For this walk-through, we downloaded the ‘access to essential medicines’ data, as explained above. Here we saved it as WHO access to essential medicines.csv. Looking at the spreadsheet in Excel, you can see that the actual data starts in row three, meaning there are two header rows. So let’s skip the first row when uploading it. med_access <- read.csv( ''WHO access to essential medicines.csv'', skip = 1) str(med_access) ## 'data.frame': 38 obs. of 3 variables: ## $ Country : Factor w/ 38 levels ''Afghanistan'',..: 1 2 3 4 5 6 7 8 9 10 ... ## $ X2007.2013 : num 94 42.9 86.7 76.7 72.1 58.3 13.3 90.7 31.3 33.3 ... ## $ X2007.2013.1: num 81.1 43.2 31.9 0 87.1 46.7 15.5 86.7 21.2 100 ... Using the str function to inspect the dataset, you can see that the second and third variables have lost their labels during the import. From the spreadsheet you know that they are: median availability of selected generic medicines (%) – Private median availability of selected generic medicines (%) – Public. Let’s change the names of these variables (to Private_Access and Public_Access respectively) to make working with them easier: names(med_access)[2] <- ''Private_Access'' names(med_access)[3] <- ''Public_Access'' To find details about these variables, click the ‘Metadata’ button on the website to find the following explanation (under ‘Method of measurement’): A standard methodology has been developed by WHO and Health Action International (HAI). Data on the availability of a specific list of medicines are collected in at least four geographic or administrative areas in a sample of medicine dispensing points. Availability is reported as the percentage of medicine outlets where a medicine was found on the day of the survey. Before we produce charts of the data, we shall look at some summary measures of the access variable (med_access). summary(med_access) ## Country Private_Access Public_Access ## Afghanistan : 1 Min. : 2.80 Min. : 0.00 ## Bahamas : 1 1st Qu.: 54.62 1st Qu.: 39.67 ## Bolivia (Plurinational State of): 1 Median : 70.15 Median : 55.95 ## Brazil : 1 Mean : 65.97 Mean : 58.25 ## Burkina Faso : 1 3rd Qu.: 86.70 3rd Qu.: 82.50 ## Burundi : 1 Max. :100.00 Max. :100.00 ## (Other) :32 NA's :2 On average, private sector patients have better access to essential medication. From the summary statistics for the Public_Access variable, you can see that there are two missing observations (NA). Here, we will keep these observations because leaving them in doesn’t affect the following analysis. med_access <- med_access[complete.cases(med_access), ] There are a number of interesting aspects to look at. We shall produce a bar chart comparing the private and public access in countries, ordered according to values of private access (largest to smallest). First, we need to reformat the data into ‘long’ format (so there is a single variable containing all the values we want to plot), then use the ggplot function to make the chart. # Reorder by values of private access (largest to smallest) med_access$Country <- reorder( med_access$Country, med_access$Private_Access) # This is required for the melt function. library(reshape2) # Rearrange the data for ggplot med_access_melt <- melt(med_access) # This creates a dataframe with three columns # Country = Country name # value = % access (Private_Access or Public_Access). # variable = indicates Public_Access or Private_Access. ggplot(med_access_melt, aes(x = Country, y = value, fill = variable)) + geom_bar(position = ''dodge'', stat = ''identity'') + scale_fill_discrete(name = ''Access'', labels = c(''Private sector'', ''Public sector'')) + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + theme_bw() + labs(title = ''Access to essential medication'', x = ''Country'', y = ''Percent of patients with access to essential medication'') + # Flip axis to make country labels readable coord_flip() <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-11.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-11-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-11-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-11-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-11.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Access to essential medication. '' /> Access to essential medication. Figure 5.11 Access to essential medication. Let’s find the extreme values, starting with the two countries where public sector patients have access to all (100%) essential medications (which you can also see in the chart). med_access[med_access$Public_Access == 100, ] ## Country Private_Access Public_Access ## 10 Cook Islands 33.3 100 ## 30 Russian Federation 100.0 100 Let’s see which countries provide 0% access to essential medication for people in the public sector. med_access[med_access$Public_Access == 0, ] ## Country Private_Access Public_Access ## 4 Brazil 76.7 0 Since an individual’s income and available options in later life partly depend on their level of education, inequality in educational access or attainment can lead to inequality in income and other outcomes. Gender inequality can be measured by the share of women at different levels of attainment. We will focus on the aspect of gender inequality in educational attainment, using data from the Our World in Data website, to make our own comparisons between countries and over time. Choose one of the following measures to answer Question 4: gender gap in primary education (share of enrolled female primary education students) share of women, between 15 and 19 years old, with no education share of women, 15 years and older, with no education. To download the data for your chosen measure: Go to the ‘Educational Mobility and Inequality’ section of the Our World in Data website, and find the chart for your chosen measure. Click the ‘Data’ button at the bottom of the chart, then click the blue button that appears to download the data in csv format. For your chosen measure: Choose ten countries that have data from 1980 to 2010. Plot your chosen countries on the same line chart, with year on the horizontal axis and share on the vertical axis. Make sure to include a legend showing country names and label the axes appropriately. Describe any general patterns in gender inequality in education over time, as well as any similarities and differences between countries. Calculate the change in the value of this measure between 1980 and 2010 for each country chosen. Sort these countries according to this value, from the smallest change to largest change. Now plot a column chart showing the change (1980 to 2010) on the vertical axis, and country on the horizontal axis. Add data labels to display the value for each country. Which country had the largest change? Which country had the smallest change? Suggest some explanations for your observations in Questions 4(b) and (d). (You may want to do some background research on your chosen countries.) Discuss the limitations of using this measure to assess the degree of gender inequality in educational attainment and propose some alternative measures. R walk-through 5.11 Using line and bar charts to illustrate changes in time Import data and plot a line chart First we import the data into R and check its structure. # Open the csv file from the working directory data_prim <- read.csv( ''OWID-gender-gap-in-primary-education.csv'') str(data_prim) ## 'data.frame': 8780 obs. of 4 variables: ## $ Entity : Factor w/ 250 levels ''Afghanistan'',..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Code : Factor w/ 207 levels '''',''ABW'',''AFG'',..: 3 3 3 3 3 3 3 3 3 3 ... ## $ Year : int 1970 1971 1972 1973 1974 1975 1976 1977 1978 1980 ... ## $ Primary.education..pupils....female.....female.: num 14.1 13.7 14 14.5 14.4 ... The data is now in the dataframe data_prim. The variable of interest (percentage of female enrolment) has a very long name so we will shorten it to PFE. names(data_prim)[4] <- ''PFE'' As usual, ensure that you understand the definition of the variables you are using. In the Our World in Data website, look at the ‘Sources’ tab underneath the graph for a definition: Percentage of female enrollment is calculated by dividing the total number of female students at a given level of education by the total enrolment at the same level, and multiplying by 100. This definition implies that if the primary-school-age population was 50% male and 50% female and all children were enrolled in school, the percentage of female enrolment would be 50. Before choosing ten countries, we check which countries (Entity) are in the dataset using the unique function. Here we also use the head() function to only show the first few countries. head(unique(data_prim$Entity)) ## [1] Afghanistan Albania Algeria ## [4] Andorra Angola Antigua and Barbuda ## 250 Levels: Afghanistan Albania Algeria Andorra ... Zimbabwe You can find nearly all the countries in the world in this list (plus some sub- and supra-country entities, like OECD countries, which explains why the variable wasn’t initially called ‘Country’). Plot a line chart for a selection of countries We now make a selection of ten countries. (You can of course make a different selection, but ensure that you get the spelling right as R is unforgiving!). temp_data <- subset(data_prim, Entity %in% c( ''Albania'', ''China'', ''France'', ''India'', ''Japan'', ''Switzerland'', ''United Arab Emirates'', ''United Kingdom'', ''Zambia'', ''United States'')) Now we plot the data, similar to what we did earlier. ggplot(temp_data, aes(x = Year, y = PFE, color = Entity)) + # size = 1 sets the line thickness. geom_line(size = 1) + # Remove grey background theme_bw() + # Change the set of colours used scale_colour_brewer(palette = ''Paired'') + scale_colour_discrete(name = ''Country'') + # Set the vertical axis label ylab(''Percentage (%)'') + # Add a title ggtitle(''Female pupils as a percentage of total enrolment in primary education'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-12.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-12-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-12-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-12-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-12.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Female pupils as a percentage of total enrolment in primary education. '' /> Female pupils as a percentage of total enrolment in primary education. Figure 5.12 Female pupils as a percentage of total enrolment in primary education. Plot a column chart with sorted values To calculate the change in the value of this measure between 1980 and 2010 for each country chosen, we have to manipulate the data so that we have one entry (row) for each entity (or country), but two different variables for the percentage of female enrolment PFE (one for each year). # Select all data for 1980 temp_data_80 <- subset(temp_data, Year == ''1980'') # Rename variable to include year names(temp_data_80)[4] <- ''PFE_80'' # Select all data for 2010 temp_data_10 <- subset(temp_data, Year == ''2010'') # Rename variable to include year names(temp_data_10)[4] <- ''PFE_10'' temp_data2 <- merge( temp_data_80, temp_data_10, by = c(''Entity'')) Have a look at temp_data2, which now contains two variables for every country, PFE_80 and PFE_10. It also has multiple variables for Year (Year.x and Year.y) and Code (Code.x and Code.y), but that is a minor issue and you could delete one of them. Now we can calculate the difference. temp_data2$dPFE <- temp_data2$PFE_10 - temp_data2$PFE_80 You could plot a separate chart for each year and check the order, but here we show how to create one chart with the data from both years. ggplot(temp_data2, aes( x = reorder(Code.x, dPFE), y = dPFE)) + geom_bar(stat = ''identity'', fill = ''tomato3'') + labs(title = ''Change (%) in female pupils’ share of total enrolment in primary education'', x = ''Country'', y = ''Percentage change (%)'', caption = ''source: https://ourworldindata.org/educational-mobility-inequality'') + theme_bw() <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-13.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-13-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-13-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-13-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-05-13.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change in percentage of female enrolment in primary school from 1980 to 2010. '' /> Change in percentage of female enrolment in primary school from 1980 to 2010. Figure 5.13 Change in percentage of female enrolment in primary school from 1980 to 2010. It is apparent that some countries saw very little or no change (the countries that already had very high PFE). The countries with initially low female participation have significantly improved. University of Manchester’s Econometric Computing Learning Resource (ECLR). 2018. ‘R TSplots’. Updated 26 July 2016. ↩"
});
index.addDoc({
    id: 30,
    title: "Doing Economics: Empirical Project 5: Working in Google Sheets",
    content: "Empirical Project 5 Working in Google Sheets Part 5.1 Measuring income inequality Learning objectives for this part draw Lorenz curves calculate and interpret the Gini coefficient interpret alternative measures of income inequality. One way to visualize the income distribution in a population is to draw a Lorenz curve. This curve shows the entire population along the horizontal axis from the poorest to the richest. The height of the curve at any point on the vertical axis indicates the fraction of total income received by the fraction of the population, shown on the horizontal axis. We will start by using income decile data from the Global Consumption and Income Project to draw Lorenz curves and compare changes in the income distribution of a country over time. Note that income here refers to market income, which does not take into account taxes or government transfers (see Section 5.10 of Economy, Society, and Public Policy for further details). To answer the question below: Go to the Globalinc website and download the Excel file containing the data by clicking ‘xlsx’. Save it in an easily accessible location, such as a folder on your Desktop or in your personal folder. Choose two countries that you would like to compare and filter the data so only the values for 1980 and 2014 are visible. You will be using this data as the basis for your Lorenz curves. Copy and paste the filtered data (all columns) into a new tab in your spreadsheet. To draw Lorenz curves, we need to calculate the cumulative share of total income owned by each decile (these will be the vertical axis values). The cumulative income share of a particular decile is the proportion of total income held by that decile and all the deciles below it. For example, if Decile 1 has 1/10 of total income and Decile 2 has 2/10 of total income, the cumulative income share of Decile 2 is 3/10 (or 0.3). In this new tab, make one table (as shown in Figure 5.1) for each country and year (four tables total). Use the country data you have selected to fill in each table. (Remember that each decile represents 10% of the population.) Cumulative share of the population (%) Cumulative share of income (%) 0 0 10 20 30 40 50 60 70 80 90 100 Cumulative share of income owned, for each decile of the population. Figure 5.1 Cumulative share of income owned, for each decile of the population. Google Sheets walk-through 5.1 Creating a table showing cumulative shares <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to create a table showing cumulative shares. '' /> How to create a table showing cumulative shares. Figure 5.2 How to create a table showing cumulative shares. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : We will be using data from Afghanistan and Albania for this example. The data has been copied and pasted into a new tab on the spreadsheet. We will make a cumulative table for Afghanistan in 1980. (The other three tables are made in the same way.) '' /> The data We will be using data from Afghanistan and Albania for this example. The data has been copied and pasted into a new tab on the spreadsheet. We will make a cumulative table for Afghanistan in 1980. (The other three tables are made in the same way.) Figure 5.2a We will be using data from Afghanistan and Albania for this example. The data has been copied and pasted into a new tab on the spreadsheet. We will make a cumulative table for Afghanistan in 1980. (The other three tables are made in the same way.) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the cumulative share of income using the SUM function : To calculate the cumulative share of income, we need to add up all the incomes corresponding to that decile and all smaller deciles, and then divide by the sum of all incomes. The SUM function adds up all cells in the selection. This value tells us that the bottom 10% of the people own 2% of the total income in the population. '' /> Calculate the cumulative share of income using the SUM function To calculate the cumulative share of income, we need to add up all the incomes corresponding to that decile and all smaller deciles, and then divide by the sum of all incomes. The SUM function adds up all cells in the selection. This value tells us that the bottom 10% of the people own 2% of the total income in the population. Figure 5.2b To calculate the cumulative share of income, we need to add up all the incomes corresponding to that decile and all smaller deciles, and then divide by the sum of all incomes. The SUM function adds up all cells in the selection. This value tells us that the bottom 10% of the people own 2% of the total income in the population. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the cumulative share of income using the SUM function : Decile 2 and the remaining deciles are calculated slightly differently from Decile 1, because we have to also include the incomes of lower deciles in the calculation. '' /> Calculate the cumulative share of income using the SUM function Decile 2 and the remaining deciles are calculated slightly differently from Decile 1, because we have to also include the incomes of lower deciles in the calculation. Figure 5.2c Decile 2 and the remaining deciles are calculated slightly differently from Decile 1, because we have to also include the incomes of lower deciles in the calculation. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-02-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the cumulative share of income using the SUM function : You can use this table to plot a Lorenz curve with the first column as the horizontal axis values, and the second column as the vertical axis values. '' /> Calculate the cumulative share of income using the SUM function You can use this table to plot a Lorenz curve with the first column as the horizontal axis values, and the second column as the vertical axis values. Figure 5.2d You can use this table to plot a Lorenz curve with the first column as the horizontal axis values, and the second column as the vertical axis values. Use the tables you have made to draw Lorenz curves for each country in order to visually compare the income distributions over time. Draw a line chart with cumulative share of population on the horizontal axis and cumulative share of income on the vertical axis. Plot one chart per country (each chart should have two lines, one for 1980 and one for 2014). Make sure to include a chart legend, and label your axes and chart appropriately. Follow the steps in Google Sheets walk-through 5.2 to add a straight line representing perfect equality to each chart. (Hint: If income was shared equally across the population, the bottom 10% of people would have 10% of the total income, the bottom 20% would have 20% of the total income, and so on.) Google Sheets walk-through 5.2 Drawing the perfect equality line <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to draw the perfect equality line. '' /> Figure 5.3 How to draw the perfect equality line. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : We will use the Lorenz curve for Afghanistan in 1980 as an example. The values we need to plot the perfect equality line are given in Cells C9 to C19 (labelled ‘perfect equality line’). You will notice that these values are the same as those in Cells A9 to A19, because the perfect equality line is where the horizontal and vertical axis values are equal to each other. '' /> The data We will use the Lorenz curve for Afghanistan in 1980 as an example. The values we need to plot the perfect equality line are given in Cells C9 to C19 (labelled ‘perfect equality line’). You will notice that these values are the same as those in Cells A9 to A19, because the perfect equality line is where the horizontal and vertical axis values are equal to each other. Figure 5.3a We will use the Lorenz curve for Afghanistan in 1980 as an example. The values we need to plot the perfect equality line are given in Cells C9 to C19 (labelled ‘perfect equality line’). You will notice that these values are the same as those in Cells A9 to A19, because the perfect equality line is where the horizontal and vertical axis values are equal to each other. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add the required cells to the line chart : For the perfect equality line to show up on the chart, we need to add it as a separate data series. '' /> Add the required cells to the line chart For the perfect equality line to show up on the chart, we need to add it as a separate data series. Figure 5.3b For the perfect equality line to show up on the chart, we need to add it as a separate data series. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-03-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis labels : After step 3, there will be one label on the horizontal axis for each decile. '' /> Change the horizontal axis labels After step 3, there will be one label on the horizontal axis for each decile. Figure 5.3c After step 3, there will be one label on the horizontal axis for each decile. Using your Lorenz curves: Compare the distribution of income across time for each country. Compare the distribution of income across countries for each year (1980 and 2014). Suggest some explanations for any similarities and differences you observe. (You may want to research your chosen countries to see if there were any changes in government policy, political events, or other factors that may affect the income distribution.) A rough way to compare income distributions is to use a summary measure such as the Gini coefficient. The Gini coefficient ranges from 0 (complete equality) to 1 (complete inequality). It is calculated by dividing the area between the Lorenz curve and the perfect equality line, by the total area underneath the perfect equality line. Intuitively, the further away the Lorenz curve is from the perfect equality line, the more unequal the income distribution is, and the higher the Gini coefficient will be. Using a Gini coefficient calculator, calculate the Gini coefficient for each of your Lorenz curves. You should have four coefficients in total. Label each Lorenz curve with its corresponding Gini coefficient, and check that the coefficients are consistent with what you see in your charts. (Hint: In the Gini calculator, paste the list of incomes by decile into the box provided, and then add commas between the income values.) Now we will look at other measures of income inequality to see how they can be used with the Gini coefficient to summarize a country’s income distribution. Instead of summarizing the entire income distribution like the Gini coefficient does, we can take the ratio of incomes at two points in the distribution. For example, the 90/10 ratio takes the ratio of the top 10% of incomes (Decile 10) to the lowest 10% of incomes (Decile 1). A 90/10 ratio of five means that the richest 10% of the population earn five times more than the poorest 10%. The higher the ratio, the higher the inequality between these two points in the distribution. Look at the following ratios: 90/10 ratio = the ratio of Decile 10 income to Decile 1 income 90/50 ratio = the ratio of Decile 10 income to Decile 5 income (the median) 50/10 ratio = the ratio of Decile 5 income (the median) to Decile 1 income. For each of these ratios, explain why policymakers might want to compare the two deciles in the income distribution. What kinds of policies or events could affect these ratios? We will now compare these summary measures (ratios and the Gini coefficient) for a larger group of countries, using OECD data. The OECD has annual data for different ratio measures of income inequality for 42 countries around the world, and has an interactive chart function that plots them for you. Go to the OECD website to access the data. You will see a chart similar to Figure 5.4, showing data for 2015. The countries are ranked from smallest to largest Gini coefficient on the horizontal axis, and the vertical axis gives the Gini coefficient. Compare summary measures of inequality for all available countries on the OECD website: Plot the data for the ratio measures by changing the variable selected in the drop-down menu ‘Gini coefficient’. The three ratio measures we looked at previously are called ‘Interdecile P90/P10’, ‘Interdecile P90/P50’, and ‘Interdecile P50/P10’, respectively. (If you click the ‘Compare variables’ option, you can plot more than one variable (except the Gini coefficient) on the same chart.) For each measure, give an intuitive explanation of how it is measured and what it tells us about income inequality. (For example: What do the larger and smaller values of this measure mean? Which parts of the income distribution does this measure use?) Do countries that rank highly on the Gini coefficient also rank highly on the ratio measures, or do the rankings change depending on the measure used? Based on your answers, explain why it is important to look at more than one summary measure of a distribution. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-05-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''OECD countries ranked according to their Gini coefficient (2015). '' /> OECD countries ranked according to their Gini coefficient (2015). Figure 5.4 OECD countries ranked according to their Gini coefficient (2015). The Gini coefficient and the ratios we have used are common measures of inequality, but there are other ways to measure income inequality. Go to the Chartbook of Economic Inequality, which contains five measures of income inequality, including the Gini coefficient, for 25 countries around the world. Choose two measures of income inequality that you find interesting (excluding the Gini coefficient). For each measure, give an intuitive explanation of how it is measured and what we can learn about income inequality from it. You may find the page on ‘Inequality measures’ helpful. (For example: What do larger or smaller values of this measure mean? Which parts of the income distribution does this measure use?) On the Chartbook of Economic Inequality main page, charts of these measures are available for all countries shown in green on the map. For two countries of your choice, look at the charts and explain what these measures tell us about inequality in those countries. Part 5.2 Measuring other kinds of inequality Learning objectives for this part research other dimensions of inequality and how they are measured. There are many ways to measure income inequality, but income inequality is only one dimension of inequality within a country. To get a more complete picture of inequality within a country, we need to look at other areas in which there may be inequality in outcomes. We will explore two particular areas, focusing on the measures used and their limitations: health inequality gender inequality in education. First, we will look at how researchers have measured inequality in health-related outcomes. Besides income, health is an important aspect of wellbeing, partly because it determines how long an individual will be alive to enjoy his or her income. If two people had the same annual income throughout their lives, but the one person had a much shorter life than the other, we might say that the distribution of wellbeing is unequal, despite annual incomes being equal. As with income, inequality in life expectancy can be measured using a Gini coefficient. In the study ‘Mortality inequality’, researcher Sam Peltzman (2009) estimated Gini coefficients for life expectancy based on the distribution of total years lived (life-years) across people born in a given year (birth cohort). If everybody born in a given year lived the same number of years, then the total years lived would be divided equally among these people (perfect equality). If a few people lived very long lives but everybody else lived very short lives, then there would be a high degree of inequality (Gini coefficient close to 1). We will now look at mortality inequality Gini coefficients for ten countries around the world. First, download the data: Go to the ‘Health Inequality’ section of the Our World in Data website. Under the heading ‘Mortality Inequality’, click the ‘Data’ button at the bottom of the chart shown. Click the blue button that appears to download the data in csv format. Using the mortality inequality data: Plot all the countries on the same line chart, with Gini coefficient on the vertical axis and year (1952–2002) on the horizontal axis. Make sure to include a legend showing country names and label the axes appropriately. Describe any general patterns in mortality inequality over time, as well as any similarities and differences between countries. Now compare the Gini coefficients in the first year of your line chart (1952) with the last year (2002). For the year 1952, sort the countries according to their mortality inequality Gini coefficient from smallest to largest. Plot a column chart showing these Gini coefficients on the vertical axis, and country on the horizontal axis. Add data labels to display the Gini coefficient for each country. Repeat Question 2(a) for the year 2002. Comparing your charts for 1952 and 2002, have the rankings between countries changed? Suggest some explanations for any observed changes. (You may want to do some additional research, for example, look at the healthcare systems of these countries.) Google Sheets walk-through 5.3 Drawing a column chart with sorted values <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to draw a column chart with sorted values. '' /> Figure 5.5 How to draw a column chart with sorted values. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Sort the data from smallest to largest Gini coefficient : We will use the Gini coefficients for 1952 as an example. The data has been filtered to show values for the year 1952 only. '' /> Sort the data from smallest to largest Gini coefficient We will use the Gini coefficients for 1952 as an example. The data has been filtered to show values for the year 1952 only. Figure 5.5a We will use the Gini coefficients for 1952 as an example. The data has been filtered to show values for the year 1952 only. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Sort the data from smallest to largest Gini coefficient : We will sort the data according to the values in Column D. '' /> Sort the data from smallest to largest Gini coefficient We will sort the data according to the values in Column D. Figure 5.5b We will sort the data according to the values in Column D. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Sort the data from smallest to largest Gini coefficient : After step 4, the countries will now be sorted according to their Gini coefficient (from smallest to largest). '' /> Sort the data from smallest to largest Gini coefficient After step 4, the countries will now be sorted according to their Gini coefficient (from smallest to largest). Figure 5.5c After step 4, the countries will now be sorted according to their Gini coefficient (from smallest to largest). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create a new table containing sorted data : We will use this table to create a column chart. '' /> Create a new table containing sorted data We will use this table to create a column chart. Figure 5.5d We will use this table to create a column chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Round the Gini coefficients to two decimal places : We will label the chart columns with the corresponding Gini coefficients, so we will first round the data to two decimal places to avoid making the chart look too crowded. '' /> Round the Gini coefficients to two decimal places We will label the chart columns with the corresponding Gini coefficients, so we will first round the data to two decimal places to avoid making the chart look too crowded. Figure 5.5e We will label the chart columns with the corresponding Gini coefficients, so we will first round the data to two decimal places to avoid making the chart look too crowded. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Draw a column chart : Now we will make a column chart with the sorted and rounded Gini coefficients. '' /> Draw a column chart Now we will make a column chart with the sorted and rounded Gini coefficients. Figure 5.5f Now we will make a column chart with the sorted and rounded Gini coefficients. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-05-05-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add data labels to the columns : Data labels will make the vertical values easier to see, especially for values that are very close to each other. After step 9, the Gini coefficients will appear in boxes above the columns. '' /> Add data labels to the columns Data labels will make the vertical values easier to see, especially for values that are very close to each other. After step 9, the Gini coefficients will appear in boxes above the columns. Figure 5.5g Data labels will make the vertical values easier to see, especially for values that are very close to each other. After step 9, the Gini coefficients will appear in boxes above the columns. Note: Questions 3 and 4 can be done independently of each other. Other measures of health inequality, such as those used by the World Health Organization (WHO), are based on access to healthcare, affordability of healthcare, and quality of living conditions. Choose one of the following measures of health inequality to answer Question 3: access to essential medicines basic hospital access composite coverage index. The composite coverage index is a weighted score of coverage for eight different types of healthcare. To download the data for your chosen measure: If you choose to look at either the access to essential medicines or the basic hospital access measure, go to the WHO’s Universal Health Coverage Data Portal, click on the tab ‘Explore UHC Indicators’, and select your chosen measure. A drop-down menu with three buttons will appear: ‘Map’ (or ‘Graph’) shows a visual description of the data, ‘Data’ contains the data files, and ‘Metadata’ contains information about your chosen measure. Click on the ‘Data’ button, then select ‘CSV table’ from the ‘Download complete data set as’ list. If you choose to look at the composite coverage index measure, go to WHO’s Global Health Observatory data repository. The index is given for subgroups of the population, by economic status, education, and place of residence. Choose one of these categories, and download the data by clicking ‘CSV table’ from the ‘Download complete data set as’ list. You can read further information about this index in the WHO’s technical notes. For your chosen measure: Explain how it is constructed and what outcomes it assesses. Create an appropriate chart to summarize the data for all available countries. (You can replicate a chart shown on the website or draw a similar chart.) Explain what your chart shows about health inequality within and between countries, and discuss the limitations of using this measure (for example, measurement issues or other aspects of inequality that this measure ignores). Since an individual’s income and available options in later life partly depend on their level of education, inequality in educational access or attainment can lead to inequality in income and other outcomes. Gender inequality can be measured by the share of women at different levels of attainment. We will focus on the aspect of gender inequality in educational attainment, using data from the Our World in Data website, to make our own comparisons between countries and over time. Choose one of the following measures to answer Question 4: gender gap in primary education (share of enrolled female primary education students) share of women, between 15 and 19 years old, with no education share of women, 15 years and older, with no education. To download the data for your chosen measure: Go to the ‘Educational Mobility and Inequality’ section of the Our World in Data website, and find the chart for your chosen measure. Click the ‘Data’ button at the bottom of the chart, then click the blue button that appears to download the data in csv format. For your chosen measure: Choose ten countries that have data from 1980 to 2010. Plot your chosen countries on the same line chart, with year on the horizontal axis and share on the vertical axis. Make sure to include a legend showing country names and label the axes appropriately. Describe any general patterns in gender inequality in education over time, as well as any similarities and differences between countries. Calculate the change in the value of this measure between 1980 and 2010 for each country chosen. Sort these countries according to this value, from the smallest change to largest change. Now plot a column chart showing the change (1980 to 2010) on the vertical axis, and country on the horizontal axis. Add data labels to display the value for each country. Which country had the largest change? Which country had the smallest change? Suggest some explanations for your observations in Questions 4(b) and (d). (You may want to do some background research on your chosen countries.) Discuss the limitations of using this measure to assess the degree of gender inequality in educational attainment and propose some alternative measures."
});
index.addDoc({
    id: 31,
    title: "Doing Economics: Empirical Project 5 Solutions",
    content: "Empirical Project 5 Solutions These are not model answers. They are provided to help students, including those doing the project outside a formal class, to check their progress while working through the questions using the Excel, R, or Google Sheets walk-throughs. There are also brief notes for the more interpretive questions. Students taking courses using Doing Economics should follow the guidance of their instructors. Part 5.1 Measuring income inequality China and the US are used as examples. China, 1980 Cumulative share of the population (%) Cumulative share of income (%) 0 0.00 10 3.14 20 7.63 30 13.43 40 20.47 50 28.82 60 38.55 70 49.92 80 63.28 90 79.33 100 100.00 Table showing cumulative income shares for China (1980). Solution figure 5.1 Table showing cumulative income shares for China (1980). China, 2014 Cumulative share of the population (%) Cumulative share of income (%) 0 0.00 10 0.92 20 2.84 30 5.81 40 9.95 50 15.44 60 22.55 70 31.75 80 43.95 90 61.43 100 100.00 Table showing cumulative income shares for China (2014). Solution figure 5.2 Table showing cumulative income shares for China (2014). United States, 1980 Cumulative share of the population (%) Cumulative share of income (%) 0 0.00 10 2.29 20 6.22 30 11.52 40 18.08 50 25.89 60 35.04 70 45.73 80 58.44 90 74.39 100 100.00 Table showing cumulative income shares for the US (1980). Solution figure 5.3 Table showing cumulative income shares for the US (1980). United States, 2014 Cumulative share of the population (%) Cumulative share of income (%) 0 0.00 10 1.88 20 5.14 30 9.66 40 15.41 50 22.45 60 30.92 70 41.09 80 53.58 90 69.90 100 100.00 Table showing cumulative income shares for the US (2014). Solution figure 5.4 Table showing cumulative income shares for the US (2014). Solution figures 5.5 and 5.6 show the Lorenz curves for China and the US. The perfect equality line applies to the next question’s solution. Solution figures 5.5 and 5.6 show the Lorenz curves for China and the US, with the perfect equality line. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Lorenz curves for China. '' /> Lorenz curves for China. Solution figure 5.5 Lorenz curves for China. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Lorenz curves for the US. '' /> Lorenz curves for the US. Solution figure 5.6 Lorenz curves for the US. The area between the perfect equality line and the Lorenz curve reflects inequality. Inequality in both countries widened between 1980 and 2014. The change in China is far larger than that in the US. Although income distribution is more equal in China than in the US in 1980, it is less equal in China than in the US in 2014. China had a mostly planned economy in 1980, which prioritized equality. Since 1978, China has undertaken waves of reforms to marketize the economy and improve efficiency. The rapid growth has come at the cost of equality. By introducing market reforms, opportunities emerged for private gain through entrepreneurial activities. Although rapid growth and high inequality are negatively correlated both in high income countries and in a group of ‘catching up’ countries, as discussed in Section 19.11 of The Economy, rapid growth in China has come at the cost of rising inequality. Inequality in the US is higher than in most developed countries. Many people attribute the higher inequality to policies favouring the rich. Worsening inequality in the US can be explained by a range of factors, including tax policies that favour the rich, education policies that dampen the opportunities for intergenerational mobility (see Section 19.2 of The Economy), the skill-biased technological change that raises the incomes of workers with skills complementary to ICT and reduces that of workers with skills substitutable by ICT, and the decline of labour unions. Solution figures 5.7 and 5.8 show the Lorenz curves for China and the US with Gini coefficients labelled. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Lorenz curves for China, with labelled Gini coefficients. '' /> Lorenz curves for China, with labelled Gini coefficients. Solution figure 5.7 Lorenz curves for China, with labelled Gini coefficients. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-08.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-08-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-08-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-08-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-08.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Lorenz curves for the US, with labelled Gini coefficients. '' /> Lorenz curves for the US, with labelled Gini coefficients. Solution figure 5.8 Lorenz curves for the US, with labelled Gini coefficients. These ratios all help give policymakers an idea of the distribution of income in the economy and where income is concentrated. Policymakers may use the information to decide on policies favouring certain income deciles of the population. The 90/10 ratio compares the two extremes of the income distribution and tells policymakers about the difference between the richest and the poorest. Policymakers can use the information to decide how much income to redistribute to the poorest. The 90/50 ratio tells policymakers about how the middle class is doing relative to the richest. The ratio can also be used to determine the distribution of tax burden among the relatively rich population. The 50/10 ratio reveals the distribution of income among the relatively poor population. Policymakers can use the information to determine the amount of income to be redistributed to each group, and to determine who is in relative poverty (many governments define the poverty line relative to the median income). See Section 19.8 of The Economy to see how governments can affect income inequality. Students will plot the data for the ratio measures by changing the variable selected for the Gini coefficient. The interdecile ratios are calculated as the ratios between incomes of various deciles of income distribution. The 90/10 ratio, for example, is the ratio of the income of the 9th decile to the income of the 1st decile. Larger values mean the income from one decile of the distribution is higher relative to the income from another decile. Countries that rank highly on the Gini coefficient also generally rank highly on ratio measures. There are, however, some exceptions. Slovenia, for example, while being the most equal country in terms of the Gini coefficient in 2015, was only the 5th most equal country in terms of the 90/10 ratio. The potential differences in rankings of different measures mean it is important to look at more than one measure. The Gini coefficient is an overall measure of a distribution that may mask extreme inequalities between certain groups of the population. Measures chosen here are the share of income going to the top 1%, and the share of the population living in relative poverty. Share of income going to the top 1%: This measure looks at the high end of the income distribution (the right tail). Larger values indicate that the very rich have a larger share of the income, and that there is therefore more inequality between the very rich and the rest of society. However, this is a narrower measure of inequality than the Gini coefficient because it only tells us about how the very rich are doing. Share of the population living in relative poverty: This measure is defined as the share of individuals who live in a household with 60% of the disposable income of the median household. A larger value indicates that a larger proportion of individuals are living in relative poverty. Part 5.2 Measuring other kinds of inequality Solution figure 5.9 shows the mortality inequality Gini coefficients for the ten countries. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-09.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-09-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-09-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-09-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-09.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Mortality inequality Gini coefficients (1952–2002). '' /> Mortality inequality Gini coefficients (1952–2002). Solution figure 5.9 Mortality inequality Gini coefficients (1952–2002). Mortality inequality has been falling over time in all countries except Russia. Developing countries tend to have greater mortality inequality than developed countries. Industrialized, richer countries seem to have materialized most of the available improvement (somewhere at a mor­tality Gini of 0.1) since the 1960s. Exceptions to this are India and Brazil, which are both still on a significant downward trend and still not close to a mortality Gini value of 0.1. The only country in this set of countries where some of the gains are being reversed is Russia, although the latest upward movement is fairly modest, and one may interpret this as Russia having settled on a higher mortality Gini of about 0.15. Solution figure 5.10 shows Gini coefficients by country for 1952. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-10.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-10-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-10-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-10-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-10.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Countries ranked according to mortality inequality Gini coefficients in 1952. '' /> Countries ranked according to mortality inequality Gini coefficients in 1952. Solution figure 5.10 Countries ranked according to mortality inequality Gini coefficients in 1952. Solution figure 5.11 shows Gini coefficients by country for 2002. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-11.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-11-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-11-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-11-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-11.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Countries ranked according to mortality inequality Gini coefficients in 2002. '' /> Countries ranked according to mortality inequality Gini coefficients in 2002. Solution figure 5.11 Countries ranked according to mortality inequality Gini coefficients in 2002. The rankings are different in 1952 and 2002. Japan, for example, moved up five places in the ranking to become the second most equal country in 2002. The rapid economic development in Japan has led to rising life expectancy. Living to old age is now the norm in Japan rather than a privilege enjoyed only by the rich. The rising proportion of elderly voters has contributed to policies aimed at improving elderly care, which have reduced the variation in life expectancy. The United States, on the other hand, dropped four places to become a relatively less equal nation in the group. The high costs of healthcare may prevent poor people from accessing treatment, especially if uninsured. It is more likely for disadvantaged groups in society such as minorities or part-time workers to lack insurance coverage. This example looks at access to essential medicines. The median availability of selected generic medicines (in percentage terms) is a measure of the access to treatment. Data on availability, defined as the percentage of medicine outlets where a medicine was found on a given day, are collected through surveys in multiple regions for each country. Solution figures 5.12 and 5.13 provide two charts summarizing the data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-12.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-12-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-12-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-12-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-12.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Median availability of selected generic medicines in the private sector (2007–2013). '' /> Median availability of selected generic medicines in the private sector (2007–2013). Solution figure 5.12 Median availability of selected generic medicines in the private sector (2007–2013). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-13.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-13-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-13-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-13-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-13.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Median availability of selected generic medicines in the public sector (2007–2013). '' /> Median availability of selected generic medicines in the public sector (2007–2013). Solution figure 5.13 Median availability of selected generic medicines in the public sector (2007–2013). There are large disparities in health inequality across countries. For example, availability in the Russian Federation is 100%, whereas in China it is about 15%. The availability of medicines within a country can differ depending on whether an outlet belongs to the public or the private sector. In some countries, such as Brazil, private sector availability of medicines is far higher than that in the public sector. The reverse is true for other countries such as Sao Tome and Principe. Note that a higher availability of medicines in the private sector does not necessarily mean greater access for the entire population, since the private sector is only open to individuals with the ability to pay. This disparity means that richer individuals can access a wider range of medical treatments. The data has some limitations. The basket of medicines differs across countries. The data reflects availability on the day of data collection, which may not be a representative day. Outlets could stockpile medicines in expectation of the arrival of the data collection team. Availability does not account for the dosage and strengths of the products. Solution figure 5.14 looks at the gender gap in primary education. Note: It is difficult to find ten countries without any missing data point between 1980 and 2010. Countries with full data may not be as interesting as others. The lines below connect all available data points. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-14.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-14-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-14-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-14-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-14.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Female pupils as a percentage of total enrolment in primary education (1980–2010). '' /> Female pupils as a percentage of total enrolment in primary education (1980–2010). Solution figure 5.14 Female pupils as a percentage of total enrolment in primary education (1980–2010). For most countries in the selected group, the share of female pupils in primary education fluctuated around levels just below 50% throughout the period. China and India were the most unequal countries in 1980. India had the greatest improvement in equality over the period, and by 2010 the female share reached nearly 48%. Note the inverse U-shape for China, which could be due to the increasing gender imbalance in the school-age population (around 112 males per 100 females in 2010). Solution figure 5.15 shows the percentage change in the measure between 1980 and 2010. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-15.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-15-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-15-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-15-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-05-15.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change (%) in female pupils’ share of total enrolment in primary education (1980–2010). '' /> Change (%) in female pupils’ share of total enrolment in primary education (1980–2010). Solution figure 5.15 Change (%) in female pupils’ share of total enrolment in primary education (1980–2010). India had the largest change, whereas France had the smallest change. India had the lowest share of enrolled female primary education students in the group in 1980. Rapid development and changing beliefs have contributed to the efforts to reduce gender education inequalities. Universal primary education and promotion of gender equality are among the eight goals in the Millennium Development Goals (MDGs) to which India committed to achieve by 2015 since 2000. France, as a developed country, had relatively high equality from the beginning of the period and hence had experienced relatively little change over the period (due to less scope for improvement). From Question 4(c), it is apparent that countries which already had a very high percentage of female enrolment (PFE) saw no change. Those countries with initially low female participation have significantly improved. The data demonstrates that the past few decades have seen a significant improvement in access to education for girls. If you repeated the above analysis for all countries, you would see similar results. The measure depends on the gender composition of the population. If there are more male than female children of primary schooling age in a country, then the share of females enrolled must be less than 50%. The ratio of female to male in enrolment rate, which provides a population-adjusted measure of gender parity, can be used instead. Remember that all we can see here is enrolment in primary education. It is possible that males could receive more education overall (secondary and higher levels). In fact, if you go back to the ‘Educational Mobility and Inequality’ section of the Our World in Data website, you will see that in many regions females still receive a significantly smaller amount of education overall."
});
index.addDoc({
    id: 32,
    title: "Doing Economics: Empirical Project 6: Measuring management practices",
    content: "Empirical Project 6 Measuring management practices Learning objectives In this project you will: explain how survey data is collected, and describe measures that can increase the reliability and validity of survey data (Part 6.1) use column charts and box and whisker plots to compare distributions (Part 6.1) calculate conditional means for one or more conditions, and compare them on a bar chart (Parts 6.1, 6.2, and 6.3) construct confidence intervals and use them to assess differences between groups (Parts 6.2 and 6.3) evaluate the usefulness and limitations of survey data for determining causality (Part 6.3). Key concepts Concepts needed for this project: mean, conditional mean, median, percentile, maximum, minimum, causation, and natural log transformation. Concepts introduced in this project: box and whisker plots, and confidence intervals. Introduction CORE projects This empirical project is related to material in: Unit 6 of Economy, Society, and Public Policy Unit 6 of The Economy. Firms play an important part in most modern economies, by coordinating production to produce goods and services. Within firms, there is usually a top-down decision-making structure in which owners decide on long-term strategies, and managers direct the activities of their employees to implement these strategies. Read more about how decisions are made in firms, and the relationships between owners, managers, and employees in Sections 6.2, 6.3 and 6.4 of Economy, Society, and Public Policy. We might expect firms where employees and production processes are better managed to be more productive than poorly managed firms. To make these comparisons between firms, we first need to define what ‘good’ management is, and then find a way to quantify it—neither of which are straightforward tasks. Researchers Bloom, Genakos, Sadun, and Van Reenen (2012) took on this challenge, devising a way to accurately evaluate and score management practices across industries and countries. They then surveyed numerous organizations in various industries and countries to collect information about the quality of managerial practices within each firm. Using these survey responses, they constructed a measure of how well a firm was managed, ranging from 1 (‘worst’ management practice) to 5 (‘best’ management practice). This measure allowed them to compare management practices across industries and countries—something which would be difficult to do with other data collection methods (such as experiments). An example, for firms in manufacturing industry, is shown in Figure 6.1 below. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-01.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-01-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-01-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-01-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-01.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Management practices in manufacturing firms around the world. '' /> Management practices in manufacturing firms around the world. Figure 6.1 Management practices in manufacturing firms around the world. We will be using the data that Bloom et al. collected to make comparisons between countries, industries, and types of firms, and consider possible explanations for the patterns we observe. Working in Excel Working in R Working in Google Sheets"
});
index.addDoc({
    id: 33,
    title: "Doing Economics: Empirical Project 6: Working in Excel",
    content: "Empirical Project 6 Working in Excel Part 6.1 Looking for patterns in the survey data Learning objectives for this part explain how survey data is collected, and describe measures that can increase the reliability and validity of survey data use column charts and box and whisker plots to compare distributions calculate conditional means for one or more conditions, and compare them on a bar chart use line charts to describe the behaviour of real-world variables over time. First download the data used in the paper to understand how this information was collected. The dataset is publicly available and free of charge, but you will need to create a user account in order to access it. Go to the World Management Survey data registration page. On the middle right-hand side of the page, click the ‘Register’ button. Fill in the form with the required details, then click ‘Register’. An account activation link will be sent to the email you provided. Click on it to activate your account. Now go to the World Management Survey data download page. In the subsection ‘Download the public WMS data now’, click the ‘Download Now’ button. In the ‘Login’ section, enter your account’s email and password, then click ‘Login’. Under the heading ‘Manufacturing: 2004–2010 combined survey data (AMP)’, click the ‘Download’ button. Unzip the files in the downloaded zip folder into your working directory (the folder you will be working from). You may also find it helpful to download the Bloom et al. paper ‘Management practices across firms and countries’. To learn about how Bloom et al. (2012) conducted their survey, read the sections ‘How Can Management Practices Be Measured?’ and ‘Validating the Management Data’ (pages 5–9) of their paper. Briefly describe how the interviews with managers were conducted, and explain some methods the researchers used to improve the reliability and validity of their data. (There are a few technical terms that you may not understand, but these are not necessary for answering this question.) Three aspects of management practices were evaluated: monitoring, targets, and incentives. Do you think that these are the best criteria for assessing management practices? What (if any) important aspects of management are not included in this assessment? (You may also find it helpful to refer to the ‘Contingent Management’ section on pages 23–25 of the paper.) Now we will create some charts to summarize the data and make comparisons across countries, industries (manufacturing, healthcare, retail, and education), and firm characteristics. In ‘Manufacturing: 2004–2010 combined survey data (AMP)’, open the Excel document ‘AMP_graph_manufacturing.csv’. Use this data on manufacturing firms to do the following: In a new tab, create a table like Figure 6.2a, showing the average management scores for all the firms in each of the twenty countries, and fill it in with the required values. The variables for the overall score and three individual criteria are ‘management’, ‘monitor’, ‘target’, and ‘people’. You may find it helpful to use Excel’s PivotTable option—see Excel walk-through 3.1 if you need guidance. For each criterion, rank countries from highest to lowest, then create and fill in a table like Figure 6.2b (see Excel walk-through 4.4 for help on how to assign ranks). Do countries with a high overall rank also tend to rank highly on individual criteria? Country Overall management (mean) Monitoring management (mean) Targets management (mean) Incentives management (mean) Mean of management scores. Figure 6.2a Mean of management scores. Country Overall management (rank) Monitoring management (rank) Targets management (rank) Incentives management (rank) Rank according to management scores. Figure 6.2b Rank according to management scores. Create a bar chart showing the average overall management score (the variable ‘management’) for each country, ordered from highest to lowest. (Hint: You will need to sort your data from highest to lowest so it appears correctly in the chart.) Your chart should look similar to Figure 6.1. Compare your chart with Figure 1 in Bloom et al. (2012). Can you explain why your chart is slightly different? (Hint: See the note at the bottom of Figure 1.) To look at how management quality varies within countries, instead of just looking at the mean we can use column charts to visualize the entire distribution of scores (as in Empirical Project 1). To compare distributions, we have to use the same horizontal axis, so we will first need to make a frequency table for each distribution to be used. Also, since each country has a different number of observations, we will use percentages instead of frequencies as the vertical axis variable. For three countries of your choice and for the US, carry out the following: Using the overall management score (variable ‘management’), create and fill in a frequency table similar to Figure 6.3 below for the US, and separately for each chosen country. The values in the first column should range from 1 to 5, in intervals of 0.2. (Hint: To count observations for a specific country only, you will need to use the IF function and FREQUENCY function together, as shown in Excel walk-through 6.1). Range of management score Frequency Percentage of firms (%) 1.00 1.20 … 4.80 5.00 Frequency table for overall management score. Figure 6.3 Frequency table for overall management score. Plot a column chart for each country to show the distribution of management scores, with the percentage of firms on the vertical axis and the range of management scores on the horizontal axis. On each country’s chart, plot the distribution of the US on top of that country’s distribution, as shown in Excel walk-through 6.2. Describe any visual similarities and differences between the distributions of your chosen countries and that of the US. (Hint: For example look at where the distribution is centred, the percentages of observations on the left tail or the right tail of the distribution, and how spread out the scores are.) Excel walk-through 6.1 Using Excel’s IF function Follow the walk-through in the CORE video, or in Figure 6.4, to find out how to use Excel’s IF function. How to use Excel’s IF function <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to use Excel’s IF function within another function. '' /> How to use Excel’s IF function within another function. Figure 6.4 How to use Excel’s IF function within another function. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : In this example, we will make a frequency table for the US data in Columns A to C. It’s a good idea to put all the tables in a separate place from the data. '' /> The data In this example, we will make a frequency table for the US data in Columns A to C. It’s a good idea to put all the tables in a separate place from the data. Figure 6.4a In this example, we will make a frequency table for the US data in Columns A to C. It’s a good idea to put all the tables in a separate place from the data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculating frequencies for a particular country : We want to calculate the frequency but only for firms in the US. To do this, add an IF condition within the FREQUENCY function to tell Excel which data to use. '' /> Calculating frequencies for a particular country We want to calculate the frequency but only for firms in the US. To do this, add an IF condition within the FREQUENCY function to tell Excel which data to use. Figure 6.4b We want to calculate the frequency but only for firms in the US. To do this, add an IF condition within the FREQUENCY function to tell Excel which data to use. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculating frequencies for a particular country : By putting the IF function inside the FREQUENCY function, Excel will only use the data that satisfies the condition we specified (firms in the US). '' /> Calculating frequencies for a particular country By putting the IF function inside the FREQUENCY function, Excel will only use the data that satisfies the condition we specified (firms in the US). Figure 6.4c By putting the IF function inside the FREQUENCY function, Excel will only use the data that satisfies the condition we specified (firms in the US). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculating frequencies for a particular country : The formula that is completed in step 4 is: ‘(=FREQUENCY(IF(AMP_graph_manufacturing!N:N=”United States”,AMP_graph_manufacturing!A:A),Sheet2!A2:A22))’. After step 4, all the frequency values will be filled in. '' /> Calculating frequencies for a particular country The formula that is completed in step 4 is: ‘(=FREQUENCY(IF(AMP_graph_manufacturing!N:N=”United States”,AMP_graph_manufacturing!A:A),Sheet2!A2:A22))’. After step 4, all the frequency values will be filled in. Figure 6.4d The formula that is completed in step 4 is: ‘(=FREQUENCY(IF(AMP_graph_manufacturing!N:N=”United States”,AMP_graph_manufacturing!A:A),Sheet2!A2:A22))’. After step 4, all the frequency values will be filled in. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-04-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Using frequencies to calculate percentages : The $ symbol before the row numbers in the formula tells Excel to keep these row numbers the same when copying the formula to other cells. We used it here because we are dividing the frequency value by the total number of observations (cells B2 to B22). '' /> Using frequencies to calculate percentages The $ symbol before the row numbers in the formula tells Excel to keep these row numbers the same when copying the formula to other cells. We used it here because we are dividing the frequency value by the total number of observations (cells B2 to B22). Figure 6.4e The $ symbol before the row numbers in the formula tells Excel to keep these row numbers the same when copying the formula to other cells. We used it here because we are dividing the frequency value by the total number of observations (cells B2 to B22). Excel walk-through 6.2 Overlaying one column chart over another <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to overlay one column chart over another. '' /> How to overlay one column chart over another. Figure 6.5 How to overlay one column chart over another. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : In this example, we use data for the US (Columns A to C) and Chile (Columns F to H), and plot a column chart of the percentages in Columns C and H (see the steps in Figure 6.4 for how to calculate these). '' /> The data In this example, we use data for the US (Columns A to C) and Chile (Columns F to H), and plot a column chart of the percentages in Columns C and H (see the steps in Figure 6.4 for how to calculate these). Figure 6.5a In this example, we use data for the US (Columns A to C) and Chile (Columns F to H), and plot a column chart of the percentages in Columns C and H (see the steps in Figure 6.4 for how to calculate these). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Plot a column chart : After step 3, the column chart will look like the one shown above. '' /> Plot a column chart After step 3, the column chart will look like the one shown above. Figure 6.5b After step 3, the column chart will look like the one shown above. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the appearance of the columns : First, we will remove the gaps between the columns, and make the columns overlap (rather than being plotted side-by-side). After step 7, there will be a vertical axis on the left and right side of the chart. '' /> Change the appearance of the columns First, we will remove the gaps between the columns, and make the columns overlap (rather than being plotted side-by-side). After step 7, there will be a vertical axis on the left and right side of the chart. Figure 6.5c First, we will remove the gaps between the columns, and make the columns overlap (rather than being plotted side-by-side). After step 7, there will be a vertical axis on the left and right side of the chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the vertical axis values : In order to compare the distributions, make sure the left and right vertical axis have the same labels. '' /> Change the vertical axis values In order to compare the distributions, make sure the left and right vertical axis have the same labels. Figure 6.5d In order to compare the distributions, make sure the left and right vertical axis have the same labels. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis values and series names : Now, change the horizontal axis values to match the data in our frequency tables. '' /> Change the horizontal axis values and series names Now, change the horizontal axis values to match the data in our frequency tables. Figure 6.5e Now, change the horizontal axis values to match the data in our frequency tables. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis values and series names : After step 12, the horizontal axis value for the selected columns will change. '' /> Change the horizontal axis values and series names After step 12, the horizontal axis value for the selected columns will change. Figure 6.5f After step 12, the horizontal axis value for the selected columns will change. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis values and series names : Now, we will change the series names to country names. '' /> Change the horizontal axis values and series names Now, we will change the series names to country names. Figure 6.5g Now, we will change the series names to country names. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-h.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-h-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-h-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-h-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-h.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis values and series names : After step 14, the legend entry for that country will now be the country name. '' /> Change the horizontal axis values and series names After step 14, the legend entry for that country will now be the country name. Figure 6.5h After step 14, the legend entry for that country will now be the country name. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-i.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-i-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-i-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-i-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-i.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis values and series names. : Since we have two vertical axes on our chart (one for each data series), we need to change the horizontal axis labels for both axes before Excel will update the chart. '' /> Change the horizontal axis values and series names. Since we have two vertical axes on our chart (one for each data series), we need to change the horizontal axis labels for both axes before Excel will update the chart. Figure 6.5i Since we have two vertical axes on our chart (one for each data series), we need to change the horizontal axis labels for both axes before Excel will update the chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-j.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-j-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-j-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-j-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-05-j.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the horizontal axis values and series names. : Finally, change the shading of the columns so we can see the distributions of both countries. After step 19, the distributions of both countries should be clearly visible. '' /> Change the horizontal axis values and series names. Finally, change the shading of the columns so we can see the distributions of both countries. After step 19, the distributions of both countries should be clearly visible. Figure 6.5j Finally, change the shading of the columns so we can see the distributions of both countries. After step 19, the distributions of both countries should be clearly visible. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Example of two overlapping distributions on the same column chart. '' /> Example of two overlapping distributions on the same column chart. Figure 6.6 Example of two overlapping distributions on the same column chart. box and whisker plotA graphic display of the range and quartiles of a distribution, where the first and third quartile form the ‘box’ and the maximum and minimum values form the ‘whiskers’. Another way to visualize distributions is a box and whisker plot, which shows some parts of a distribution rather than the whole distribution. We can use box and whisker plots to compare particular aspects of distributions more easily than when looking at the entire distribution. As shown in Figure 6.7, the ‘box’ consists of the first quartile (value corresponding to the bottom 25 per cent, or 25th percentile, of all values), the median, and the third quartile (75th percentile). The ‘whiskers’ are the minimum and maximum values. (In Excel, the ‘whiskers’ may not be the actual maximum or minimum, since any values larger than 1.5 times the width of the box are considered outliers and are shown as separate points.) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Example of a box and whisker plot. (Note: In Excel, the mean value is shown by X. In general, the median may not be in the centre of the box, and can differ greatly from the mean. Using the data shown in Figure 6.7 for a variable from the dataset, the mean and median are very similar.) '' /> Example of a box and whisker plot. (Note: In Excel, the mean value is shown by X. In general, the median may not be in the centre of the box, and can differ greatly from the mean. Using the data shown in Figure 6.7 for a variable from the dataset, the mean and median are very similar.) Figure 6.7 Example of a box and whisker plot. (Note: In Excel, the mean value is shown by X. In general, the median may not be in the centre of the box, and can differ greatly from the mean. Using the data shown in Figure 6.7 for a variable from the dataset, the mean and median are very similar.) Using the same countries you chose in Question 3: Make a box and whisker plot for each country and the US, showing the distribution of management scores. You can either make a separate chart for each country or show all countries in the same plot. To check that your plots make sense, compare your box and whisker plots to the distributions from Question 3. Use your box and whisker plots to add to your comparisons from Question 3(c). Excel walk-through 6.3 Drawing box and whisker plots Follow the walk-through in the CORE video, or in Figure 6.8, to find out how to draw a box and whisker plot in Excel. How to draw a box and whisker plot <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to create box and whisker plots. '' /> How to create box and whisker plots. Figure 6.8 How to create box and whisker plots. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : In this example, we will use data for the US (Column A) and Chile (Column B). To create a box and whisker plot of more than one variable, each variable needs to be in a separate column. You will need to filter, then copy and paste the required data into a new tab in Excel. '' /> The data In this example, we will use data for the US (Column A) and Chile (Column B). To create a box and whisker plot of more than one variable, each variable needs to be in a separate column. You will need to filter, then copy and paste the required data into a new tab in Excel. Figure 6.8a In this example, we will use data for the US (Column A) and Chile (Column B). To create a box and whisker plot of more than one variable, each variable needs to be in a separate column. You will need to filter, then copy and paste the required data into a new tab in Excel. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create box and whisker plots : After step 3, your box and whisker plot will look like the one above. The plots are ordered according to the columns in Excel, so the first plot corresponds to Column A, the second plot corresponds to Column B, and so on. '' /> Create box and whisker plots After step 3, your box and whisker plot will look like the one above. The plots are ordered according to the columns in Excel, so the first plot corresponds to Column A, the second plot corresponds to Column B, and so on. Figure 6.8b After step 3, your box and whisker plot will look like the one above. The plots are ordered according to the columns in Excel, so the first plot corresponds to Column A, the second plot corresponds to Column B, and so on. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add a chart legend : First, we will add a legend to indicate to which country each plot corresponds. '' /> Add a chart legend First, we will add a legend to indicate to which country each plot corresponds. Figure 6.8c First, we will add a legend to indicate to which country each plot corresponds. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the legend entries : Now, we will change the labels in the legend. If your legend is already correctly labelled, skip Steps 6–9. '' /> Change the legend entries Now, we will change the labels in the legend. If your legend is already correctly labelled, skip Steps 6–9. Figure 6.8d Now, we will change the labels in the legend. If your legend is already correctly labelled, skip Steps 6–9. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the legend entries : After changing the name of the data series, the label in the legend will change. '' /> Change the legend entries After changing the name of the data series, the label in the legend will change. Figure 6.8e After changing the name of the data series, the label in the legend will change. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the legend entries : After step 9, your data should now be correctly labelled in the chart legend. '' /> Change the legend entries After step 9, your data should now be correctly labelled in the chart legend. Figure 6.8f After step 9, your data should now be correctly labelled in the chart legend. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-08-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Remove the outliers (optional) : Excel shows any observations that are greater than 1.5 times the box width (in absolute value) separately as outliers. If your chart is too cluttered with outliers, you can remove them to make your chart more readable. '' /> Remove the outliers (optional) Excel shows any observations that are greater than 1.5 times the box width (in absolute value) separately as outliers. If your chart is too cluttered with outliers, you can remove them to make your chart more readable. Figure 6.8g Excel shows any observations that are greater than 1.5 times the box width (in absolute value) separately as outliers. If your chart is too cluttered with outliers, you can remove them to make your chart more readable. From the manufacturing data, firms in the US seem to be managed better (on average) than firms in other countries. To investigate whether this is the case in other sectors, we will use data gathered on hospitals and schools. Using the data for hospitals and schools (AMP_graph_public.csv): Create a table for hospitals and schools, showing the mean management score and criteria score (monitoring, targets, incentives) for each country, as in Figure 6.2a. (Hint: You may find it helpful to use Excel’s PivotTable option—see Excel walk-through 3.1.) Make separate bar charts for hospitals and schools showing the mean overall management score for each country, sorted from highest to lowest, as in Figure 6.1. Are the country rankings similar to those in manufacturing? Using your average criteria scores from Question 5(a), suggest some explanations for the observed rankings in either hospitals or schools. (You may find it helpful to research healthcare or educational policies and reforms in those countries to support your explanations.) Part 6.2 Do management practices differ between countries? Learning objectives for this part calculate conditional means for one or more conditions, and compare them on a bar chart construct confidence intervals and use them to assess differences between groups. Using the management survey data collected by Bloom et al. (2012), we can compare average management scores across countries and industries. When we find differences between groups in the survey, we are interested in what that tells us about the true differences in management practices between the countries. confidence intervalA range of values that is centred around the sample value, and is defined so that there is a specified probability (usually 95%) that it contains the ‘true value’ of interest. In Empirical Project 2, we used p-values to assess differences between groups. A p-value tells us the probability that a difference we observe in the data could have arisen by chance. If the p-value is small, we conclude that the data gives us evidence of a real difference between the groups. Now we will use another method that helps us to allow for random variation when we interpret data, called a confidence interval. When we work with data we usually have only a small sample from the entire population of interest. For example, the World Management Survey collects information from a selection of all the firms in a particular country. If we calculate the average management score for the sample, we have an estimate of the average management score across all firms in the country (the ‘true value’) but it may not be a very accurate estimate—especially if the sample is small and management scores vary a lot between firms. A 95% confidence interval is a range of possible values within which the true value might lie. It is estimated from the mean and standard deviation of the data. We cannot be certain that the true value lies in the range (we might have the bad luck to pick an atypical sample) but we can say that there is a 95% per cent probability that it does so. For example, suppose that the average score in the data is 3.5, and we calculate that the 95% confidence is [3.1, 3.9]. Then we say that there is a 95% chance that the true score is between 3.1 and 3.9. As the name suggests, confidence intervals tell us how much confidence we can place in our estimates, or in other words, how precisely the sample mean is estimated. The confidence interval gives us a margin of error for our estimate of the true value. If the data varies a lot, the 95% confidence interval may be quite wide. If we have plenty of data, and the standard deviation is low, the estimate will be more precise and the 95% interval will be narrow. Rule of thumb for comparing means When comparing two distributions, if neither mean is in the 95% confidence interval for the other mean, the p-value for the difference in means is less than 5%. This rule of thumb is handy when looking at charts. If two 95% confidence intervals don’t overlap, we can say immediately that the difference between the means for the two groups is unlikely to have arisen by chance. For a more definite conclusion, we can calculate the actual p-value (see Empirical Project 2) or construct a confidence interval for the difference in means. (This method involves more mathematics so we will discuss that in Empirical Project 8.) It is possible to calculate a confidence interval for any probability: however wide the 95% confidence interval, a 99% confidence interval would be wider, and an 80% one would be narrower. 95% is a common choice: it gives us quite a high degree of confidence, and to go higher tends to lead to very wide intervals. We will use 95% confidence intervals throughout this project. To sum up: A confidence interval is a range of values centred around the sample mean value and is defined so that there is a specified probability (usually 95%) that it contains the true value of interest. We will now build on the results from the Bloom et al. (2012) paper by using 95% confidence intervals to make comparisons between the mean overall management score for different countries and types of firms. The confidence interval for the population mean (mean management score for that country) is centred around the sample mean. To determine the width of the interval, we use the standard deviation and number of firms. First look at manufacturing firms in different countries. Using the manufacturing data (AMP_graph_manufacturing.csv) for three countries of your choice and for the US: Create a summary table for the overall management score as shown in Figure 6.9, with one row for each country. (Hint: Use Excel’s PivotTable option.) Country Mean Standard deviation Number of firms Summary table for manufacturing firms. Figure 6.9 Summary table for manufacturing firms. Use Excel’s CONFIDENCE.T function to determine the width of the 95% confidence interval (this is the distance from the mean to one end of the interval). See Excel walk-through 6.4 for help on how to do this. You should get a different number for each country. Plot a column chart showing the mean management score and add the confidence intervals to your chart. Use the width of the confidence intervals to describe how precisely each mean was estimated. Using your chart from Question 1(c) and this rule of thumb, what can you say about the differences between the US management score, and the scores of other countries? How would your results change if you use a different specified probability (for example, 99%)? Excel walk-through 6.4 Creating confidence intervals and adding them to a chart <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to create confidence intervals and add them to a chart. '' /> How to create confidence intervals and add them to a chart. Figure 6.10 How to create confidence intervals and add them to a chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the width of the confidence interval : In this example we will use data for the US and Chile (shown in Columns A and B). To calculate the width of the 95% confidence interval (distance from the mean to one end of the interval), we first need to calculate the standard deviation and number of observations for each country. The 0.05 in the CONFIDENCE.T function is 1 − 0.95 (our specified probability). '' /> Calculate the width of the confidence interval In this example we will use data for the US and Chile (shown in Columns A and B). To calculate the width of the 95% confidence interval (distance from the mean to one end of the interval), we first need to calculate the standard deviation and number of observations for each country. The 0.05 in the CONFIDENCE.T function is 1 − 0.95 (our specified probability). Figure 6.10a In this example we will use data for the US and Chile (shown in Columns A and B). To calculate the width of the 95% confidence interval (distance from the mean to one end of the interval), we first need to calculate the standard deviation and number of observations for each country. The 0.05 in the CONFIDENCE.T function is 1 − 0.95 (our specified probability). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Plot a bar chart : After completing step 4, your bar chart will look similar to the one above. '' /> Plot a bar chart After completing step 4, your bar chart will look similar to the one above. Figure 6.10b After completing step 4, your bar chart will look similar to the one above. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add confidence intervals to the chart : The ‘error bars’ option in Excel plots confidence intervals. We will use the calculated width values from step 1 to determine the size of the error bars. '' /> Add confidence intervals to the chart The ‘error bars’ option in Excel plots confidence intervals. We will use the calculated width values from step 1 to determine the size of the error bars. Figure 6.10c The ‘error bars’ option in Excel plots confidence intervals. We will use the calculated width values from step 1 to determine the size of the error bars. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add confidence intervals to the chart : The positive error value determines the right end of the interval (the value above the mean). The negative value determines the left end of the interval (the value below the mean). Confidence intervals are symmetric, so we use the same values for both. '' /> Add confidence intervals to the chart The positive error value determines the right end of the interval (the value above the mean). The negative value determines the left end of the interval (the value below the mean). Confidence intervals are symmetric, so we use the same values for both. Figure 6.10d The positive error value determines the right end of the interval (the value above the mean). The negative value determines the left end of the interval (the value below the mean). Confidence intervals are symmetric, so we use the same values for both. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-10-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The finished chart : After completing step 9, your chart will look similar to the one above. You can also add horizontal and vertical axis titles and a chart title. '' /> The finished chart After completing step 9, your chart will look similar to the one above. You can also add horizontal and vertical axis titles and a chart title. Figure 6.10e After completing step 9, your chart will look similar to the one above. You can also add horizontal and vertical axis titles and a chart title. Using the data for hospitals or schools (AMP_graph_public.csv), using all available countries: Create a summary table like Figure 6.9 for the overall management score, with one row for each country. (Hint: Use Excel’s PivotTable option.) Add a column containing the widths of the confidence intervals for the country means. Plot a column chart, showing the confidence intervals. Compare the management practices in the US with those in other countries. Are there any countries for which you can be confident that management practices are either better, or worse, on average than in the US? Explain your answer. Look at the width of your confidence intervals and the corresponding standard deviation and number of observations for each one. Explain whether or not the relationship between them is what you would expect. Part 6.3 What factors affect the quality of management? Learning objectives for this part calculate conditional means for one or more conditions, and compare them on a bar chart construct confidence intervals and use them to assess differences between groups evaluate the usefulness and limitations of survey data for determining causality. Besides documenting and comparing management practices across industries and countries, another purpose of the World Management Survey was to investigate factors that affect management quality. One possible factor affecting differences in management is firm ownership. To look at the data for this factor in the healthcare and education sectors, we will focus on broad groups (public vs privately-owned firms), and for manufacturing firms we will focus on different kinds of private ownership. Using the data for hospitals and schools (AMP_graph_public.csv): Create a pivot table for hospitals and schools, showing the average management score, standard deviation (StdDev), and number of observations, with ‘country’ as the row variable, and ‘ownership’ (public or private) and ‘ind’ as the column variables. Use your pivot table from Question 1(a) to calculate the confidence interval widths for management in public and private hospitals. Then do the same for schools. Plot a bar chart (one for hospitals and another for schools) showing the means from Question 1(a) and the confidence intervals from Question 1(b). Describe the differences between public and private firms within countries and compare management scores for the same firm type across countries. (For example, is one type of firm generally better managed than the other? Are there similar patterns for hospitals and schools? If you have done Question 5 in Part 6.1, you may want to discuss whether the rankings change after conditioning on ownership type. Besides ownership type, management practices may vary depending on firm size, though it is difficult to predict what the relationship between these variables might be. Larger firms have more employees and could be more difficult to manage well, but may also attract more experienced managers. We will look at the conditional means for manufacturing firms, depending on whether they are above or below the median number of employees (calculated from the data), and see if there is a clear relationship. Using the data for manufacturing firms (AMP_graph_manufacturing.csv): In a new column in the original spreadsheet, use Excel’s IF function (see Excel walk-through 6.1) to create a variable that equals ‘Smaller’ if a firm has less than the median number of employees (330) and ‘Larger’ otherwise. In natural log terms, ‘Smaller’ corresponds to log employment of less than 5.80. For two countries of your choice and the US, create a pivot table showing the mean overall management score, standard deviation, and number of observations, with ‘country’ and ‘ownership’ as the row variables, and firm size (from Question 2(a)) as the column variable. (Note: When there is only one observation in a group, there is no standard deviation.) Use your pivot table from Question 2(b) to calculate the confidence interval width for each firm size and ownership type. Plot a column chart for each country, showing the means from Question 2(b) and the confidence intervals from Question 2(c). Describe any patterns you observe across ownership types and firm size in each country. Excel walk-through 6.5 Using Excel’s IF function <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to use Excel’s IF function. '' /> How to use Excel’s IF function. Figure 6.11 How to use Excel’s IF function. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the manufacturing data looks like. We will create a new variable in Column T, according to the log employment values in Column E. We have labelled our new column ‘size’ (cell T1). '' /> The data This is what the manufacturing data looks like. We will create a new variable in Column T, according to the log employment values in Column E. We have labelled our new column ‘size’ (cell T1). Figure 6.11a This is what the manufacturing data looks like. We will create a new variable in Column T, according to the log employment values in Column E. We have labelled our new column ‘size’ (cell T1). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-11-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create a new variable : After completing step 2, you will have a variable for firm size. We used the IF function to fill the cells in Column T with the word ‘Smaller’ if log employment is smaller than 5.8, otherwise the cell is filled with the word ‘Larger’. '' /> Create a new variable After completing step 2, you will have a variable for firm size. We used the IF function to fill the cells in Column T with the word ‘Smaller’ if log employment is smaller than 5.8, otherwise the cell is filled with the word ‘Larger’. Figure 6.11b After completing step 2, you will have a variable for firm size. We used the IF function to fill the cells in Column T with the word ‘Smaller’ if log employment is smaller than 5.8, otherwise the cell is filled with the word ‘Larger’. So far we have looked at associations between firm characteristics and management practices, but have not made any causal statements. We will now discuss the difficulties with making causal statements using this data and examine how we might determine the direction of causation. For each of the following variables, explain how it could affect management practices, and then explain how management practices could affect it: education level of managers (percentage with a college degree) number of competitors firm size (number of employees). One way to establish the direction of causation is through a randomized field experiment. Read the discussion on pages 22–23 of the Bloom et al. paper (the section ‘Experimental Evidence on Management Quality and Firm Performance’) about one such experiment that was conducted in Indian textile factories. Briefly describe the idea behind a randomized field experiment, and explain, with reference to the results of the experiment in India, whether we can use it to determine the direction of causation between management practice and firm performance. The paper ‘Does Management Matter? Evidence from India’ provides more details about the experiment (pages 9–10 are particularly useful). Figure 12 in the paper shows productivity in treatment and control firms over time, with 95% confidence intervals. Use the information in the chart to describe the effect of the treatment on firm productivity."
});
index.addDoc({
    id: 34,
    title: "Doing Economics: Empirical Project 6: Working in R",
    content: "Empirical Project 6 Working in R Download the code To download the code chunks used in this project, right-click on the download link and select ‘Save Link As…’. You’ll need to save the code download to your working directory, and open it in RStudio. Don’t forget to also download the data into your working directory by following the steps in this project. Getting started in R For this project you will need the following packages: tidyverse, to help with data manipulation ggthemes, to change the look of charts easily. We will also use the ggplot2 package to produce graphs, but that does come as part of the tidyverse package. If you need to install either of these packages, run the following code: install.packages(c(''tidyverse'',''ggthemes'')) You can import the libraries now, or when they are used in the R walk-throughs below. library(tidyverse) library(ggthemes) Part 6.1 Looking for patterns in the survey data Learning objectives for this part explain how survey data is collected, and describe measures that can increase the reliability and validity of survey data use column charts and box and whisker plots to compare distributions calculate conditional means for one or more conditions, and compare them on a bar chart use line charts to describe the behaviour of real-world variables over time. First download the data used in the paper to understand how this information was collected. The data is publicly available and free of charge, but you will need to create a user account in order to access it. Go to the World Management Survey data registration page. On the middle right-hand side of the page, click the ‘Register’ button. Fill in the form with the required details, then click ‘Register’. An account activation link will be sent to the email you provided. Click on it to activate your account. Now go to the World Management Survey data download page. In the subsection ‘Download the public WMS data now’, click the ‘Download Now’ button. In the ‘Login’ section, enter your account’s email and password, then click ‘Login’. Under the heading ‘Manufacturing: 2004–2010 combined survey data (AMP)’, click the ‘Download’ button. Unzip the files in the downloaded zip folder into your working directory (the folder you will be working from). You may also find it helpful to download the Bloom et al. paper ‘Management practices across firms and countries’. To learn about how Bloom et al. (2012) conducted their survey, read the sections ‘How Can Management Practices Be Measured?’ and ‘Validating the Management Data’ (pages 5–9) of their paper. Briefly describe how the interviews with managers were conducted, and explain some methods the researchers used to improve the reliability and validity of their data. (There are a few technical terms that you may not understand, but these are not necessary for answering this question.) Three aspects of management practices were evaluated: monitoring, targets, and incentives. Do you think that these are the best criteria for assessing management practices? What (if any) important aspects of management are not included in this assessment? (You may also find it helpful to refer to the ‘Contingent Management’ section on pages 23–25 of the paper.) Now we will create some charts to summarize the data and make comparisons across countries, industries (manufacturing, healthcare, retail, and education), and firm characteristics. In ‘Manufacturing: 2004–2010 combined survey data (AMP)’, open the file ‘AMP_graph_manufacturing.csv’. Use this data on manufacturing firms to do the following: Create a table like Figure 6.2a, showing the average management scores for all the firms in each of the twenty countries, and fill it in with the required values. The variables for the overall score and three individual criteria are ‘management’, ‘monitor’, ‘target’, and ‘people’. You may find it helpful to refer to R walk-through 3.3 if you need guidance. For each criterion, rank countries from highest to lowest, then create and fill in a table like Figure 6.2b (see R walk-through 4.8 for help on how to assign ranks). Do countries with a high overall rank also tend to rank highly on individual criteria? Country Overall management (mean) Monitoring management (mean) Targets management (mean) Incentives management (mean) Mean of management scores. Figure 6.2a Mean of management scores. Country Overall management (rank) Monitoring management (rank) Targets management (rank) Incentives management (rank) Rank according to management scores. Figure 6.2b Rank according to management scores. Create a bar chart showing the average overall management score (the variable management) for each country, ordered from highest to lowest. (Hint: You will need to sort your data from highest to lowest so it appears correctly in the chart.) Your chart should look similar to Figure 6.1. Compare your chart with Figure 1 in Bloom et al. (2012). Can you explain why your chart is slightly different? (Hint: See the note at the bottom of Figure 1.) R walk-through 6.1 Importing data into R and creating tables and charts Before uploading an Excel or csv file into R, first open the file in a spreadsheet software (like Excel) to understand how the file is structured. From looking at the file we learn that: the variable names are in the first row (no need to use the skip option) missing values are represented by empty cells (hence we will use the option na.strings = '''') the last variable is in Column S, with short variable descriptions in Column U: it is easier to import everything first and remove the unnecessary data afterwards. We will call our imported data man_data. # Set your working directory to the correct folder. # Insert your file path for 'YOURFILEPATH'. setwd(''YOURFILEPATH'') man_data <- read.csv(''AMP_graph_manufacturing.csv'', na.strings = '''') str(man_data) ## 'data.frame': 9207 obs. of 21 variables: ## $ management : num 3.5 3.17 3 2.41 4.44 ... ## $ monitor : num 3.6 3.8 2.8 2.75 4.6 4.8 4.6 4.8 4.8 3.8 ... ## $ target : num 3.6 2.6 3.6 2.4 4.4 4.4 4.6 4.2 4.8 3 ... ## $ people : num 3.5 2.5 3 2.67 4.33 ... ## $ lemp_firm : num 5.99 6.4 7.6 8.04 5.24 ... ## $ export : num NA NA 70 NA NA NA NA NA NA NA ... ## $ competition : int NA 2 4 NA NA NA NA NA NA NA ... ## $ ownership : Factor w/ 9 levels ''Dispersed Shareholders'',..: NA 1 1 NA NA NA NA NA NA NA ... ## $ mne_country : Factor w/ 77 levels ''Argentina'',''Australia'',..: NA NA 72 NA NA NA NA NA NA NA ... ## $ mne_f : int 0 0 0 0 0 0 0 0 0 0 ... ## $ mne_d : int 0 0 1 0 0 0 0 0 0 0 ... ## $ degree_m : int NA 100 100 NA NA NA NA NA NA NA ... ## $ degree_nm : int NA 75 5 NA NA NA NA NA NA NA ... ## $ country : Factor w/ 20 levels ''Argentina'',''Australia'',..: 20 20 20 20 20 20 20 20 20 20 ... ## $ competition2004 : int 3 NA NA NA 3 3 3 3 3 3 ... ## $ year : int 2004 2006 2006 2002 2004 2004 2004 2004 2004 2004 ... ## $ sic : int 382 382 382 308 281 281 366 366 357 382 ... ## $ lb_employindex : int 0 0 0 NA 0 0 0 0 0 0 ... ## $ pppgdp : num 11868 13399 14119 10642 11868 ... ## $ X : logi NA NA NA NA NA NA ... ## $ storage..display.....value: Factor w/ 22 levels ''----------------------------------------------------------------------------------------------------------------------'',..: 21 1 11 15 20 17 10 8 3 16 ... You can see that Column U was imported as two variables, X (which only contains ‘NAs’) and storage..display.....value (which contains variable information). We will store the variable information in a new vector (man_varinfo) and remove both variables from the man_data dataset. # Keep the variable information man_varinfo <- unlist(man_data$ storage..display.....value[1:23]) # Delete last two variables man_data <- man_data[, !(names(man_data) %in% c(''X'', ''storage..display.....value''))] Let’s look at the variable information. man_varinfo ## [1] variable name type format label variable label ## [2] ---------------------------------------------------------------------------------------------------------------------- ## [3] management float %9.0g * Average of all management questions ## [4] monitor float %9.0g Average of perf1 to perf5 ## [5] target float %9.0g Average of perf6 to perf10 ## [6] people float %9.0g Average of talent1 to talent6 ## [7] lemp_firm float %9.0g Log of 'No. of firm employees as declared in interview' ## [8] export double %10.0g * % of production exported ## [9] competition byte %12.0g * No. of competitors ## [10] ownership str33 %33s * Who owns the firm? ## [11] mne_country str19 %19s * Country of origin of multinational (best guess) ## [12] mne_f byte %9.0g = 1 if foreign MNE ## [13] mne_d byte %9.0g = 1 if domestic MNE ## [14] degree_m byte %8.0g * % of managers with a college degree ## [15] degree_nm float %8.0g * % of non-managers with a college degree ## [16] country str19 %19s Country in which plant is located ## [17] competition2004 byte %9.0g 1=No competitors, 2=A few competitors, 3=Many competitors ## [18] year int %9.0g * SENSITIVE: Accts: Year of Accounts Data ## [19] sic int %8.0g * Three digit US SIC 1987 code (999 is missing) ## [20] lb_employindex byte %10.0g * WB: Rigidity of employment index (0-100) ## [21] pppgdp float %9.0g * IMF: GDP based on PPP valuation of cty GDP (Current international $ - ## [22] Billions) ## [23] <NA> ## 22 Levels: ---------------------------------------------------------------------------------------------------------------------- ... A few of the variables that have been imported as numbers are actually categorical (‘factor’) variables ( mne_f , mne_d, and competition2004). The reason R thought they were numerical variables was because each category was represented by a number (instead of text). We use the factor function to tell R how to treat these variables, and the labels option to define what each number in those variables represents. # Indicates what to call 0 and 1 entries man_data$mne_f <- factor(man_data$mne_f, labels = c(''no MNE_f'', ''MNE_f'')) man_data$mne_d <- factor(man_data$mne_d, labels = c(''no MNE_d'', ''MNE_d'')) # Indicates what to call 1, 2, and 3 entries man_data$competition2004 <- factor( man_data$competition2004, labels = c(''No competitors'', ''A few competitors'', ''Many competitors'')) When you create new labels, check that the labels have been attached to the correct entries (the labels should be ordered from the lowest to highest entry). To create the tables, we use piping operators (%>%) from the tidyverse package, which allow us to run a series of commands on the same data all at once. For more information on how piping works, refer to a short introduction on using piping operators.1 First, we will group data by country (group_by), then calculate the required summary statistics for each of these groups (summarize), then order the countries according to their overall score (highest to lowest) (arrange). When summarizing the data, in addition to the mean values for the different categories and the overall score, we add a variable recording how many observations we have for each country (obs). library(tidyverse) table_mean <- man_data %>% group_by(country) %>% summarize(obs = length(management), m_overall = mean(management), m_monitor = mean(monitor), m_target = mean(target), m_incentives = mean(people)) %>% arrange(desc(m_overall)) table_mean ## # A tibble: 20 x 6 ## country obs m_overall m_monitor m_target m_incentives ## <fct> <int> <dbl> <dbl> <dbl> <dbl> ## 1 United States 1225 3.35 3.58 3.26 3.25 ## 2 Germany 646 3.23 3.57 3.22 2.98 ## 3 Japan 176 3.23 3.50 3.34 2.92 ## 4 Sweden 388 3.21 3.64 3.19 2.83 ## 5 Canada 385 3.17 3.55 3.07 2.94 ## 6 UK 1242 3.03 NA 2.98 2.86 ## 7 France 613 3.03 3.43 2.97 2.74 ## 8 Italy 289 3.03 3.26 3.10 2.76 ## 9 Australia 392 3.02 3.29 3.02 2.74 ## 10 New Zealand 106 2.93 3.18 2.96 2.63 ## 11 Mexico 189 2.92 3.29 2.88 2.71 ## 12 Poland 351 2.90 3.12 2.94 2.83 ## 13 Republic of Ireland 106 2.89 3.14 2.81 2.79 ## 14 Portugal 247 2.87 3.27 2.83 2.59 ## 15 Chile 317 2.83 3.14 2.72 2.67 ## 16 Argentina 249 2.76 3.08 2.68 2.56 ## 17 Greece 251 2.73 2.97 2.66 2.58 ## 18 China 746 2.71 2.90 2.63 2.69 ## 19 Brazil 569 2.71 3.06 2.69 2.55 ## 20 India 720 2.67 2.91 2.66 2.63 You can see that m_monitor for the UK is recorded as NA, because there is a NA entry for the monitor variable. The mean function, by default, will not produce a mean value if any observations are missing. Doing so allows you to investigate if there is a data issue. Here, this missing observation isn’t really an issue for our analysis⁠—in the code above, we simply add the option na.rm = TRUE in the mean function to calculate the mean, ignoring the missing observation(s). table_mean <- man_data %>% group_by(country) %>% summarize(obs = length(management), m_overall = mean(management), m_monitor = mean(monitor, na.rm = TRUE), m_target = mean(target), m_incentives = mean(people)) %>% arrange(desc(m_overall)) table_mean ## # A tibble: 20 x 6 ## country obs m_overall m_monitor m_target m_incentives ## <fct> <int> <dbl> <dbl> <dbl> <dbl> ## 1 United States 1225 3.35 3.58 3.26 3.25 ## 2 Germany 646 3.23 3.57 3.22 2.98 ## 3 Japan 176 3.23 3.50 3.34 2.92 ## 4 Sweden 388 3.21 3.64 3.19 2.83 ## 5 Canada 385 3.17 3.55 3.07 2.94 ## 6 UK 1242 3.03 3.34 2.98 2.86 ## 7 France 613 3.03 3.43 2.97 2.74 ## 8 Italy 289 3.03 3.26 3.10 2.76 ## 9 Australia 392 3.02 3.29 3.02 2.74 ## 10 New Zealand 106 2.93 3.18 2.96 2.63 ## 11 Mexico 189 2.92 3.29 2.88 2.71 ## 12 Poland 351 2.90 3.12 2.94 2.83 ## 13 Republic of Ireland 106 2.89 3.14 2.81 2.79 ## 14 Portugal 247 2.87 3.27 2.83 2.59 ## 15 Chile 317 2.83 3.14 2.72 2.67 ## 16 Argentina 249 2.76 3.08 2.68 2.56 ## 17 Greece 251 2.73 2.97 2.66 2.58 ## 18 China 746 2.71 2.90 2.63 2.69 ## 19 Brazil 569 2.71 3.06 2.69 2.55 ## 20 India 720 2.67 2.91 2.66 2.63 Let’s make the table showing the ranks. We use the mutate function, which adds variables calculated from existing variables. table_rank <- table_mean %>% mutate(r_overall = rank(desc(m_overall)), r_monitor = rank(desc(m_monitor)), r_target = rank(desc(m_target)), r_incentives = rank(desc(m_incentives))) # Select the country variable (Column 1) and the columns # with rank information (7 to 10) table_rank[c(1, 7:10)] ## # A tibble: 20 x 5 ## country r_overall r_monitor r_target r_incentives ## <fct> <dbl> <dbl> <dbl> <dbl> ## 1 United States 1 2 2 1 ## 2 Germany 2 3 3 2 ## 3 Japan 3 5 1 4 ## 4 Sweden 4 1 4 6 ## 5 Canada 5 4 6 3 ## 6 UK 6 7 8 5 ## 7 France 7 6 9 11 ## 8 Italy 8 11 5 9 ## 9 Australia 9 8 7 10 ## 10 New Zealand 10 12 10 16 ## 11 Mexico 11 9 12 12 ## 12 Poland 12 15 11 7 ## 13 Republic of Ireland 13 14 14 8 ## 14 Portugal 14 10 13 17 ## 15 Chile 15 13 15 14 ## 16 Argentina 16 16 17 19 ## 17 Greece 17 18 19 18 ## 18 China 18 20 20 13 ## 19 Brazil 19 17 16 20 ## 20 India 20 19 18 15 Now we use the ggplot set of functions (part of the tidyverse package uploaded earlier) to create a bar chart using the m_overall value in table_mean. To present countries in order of their management score, we specified x = reorder(country, m_overall, mean) (using x = country would have ordered the countries alphabetically, which is R’s default option). To switch the horizontal and vertical axis (as in Figure 6.1), we used the coord_flip option. ggplot(table_mean, aes(x = reorder(country, m_overall, mean), y = m_overall)) + geom_bar(stat = ''identity'', position = ''identity'') + xlab('''') + ylab(''Average management practice score'') + coord_flip() + theme_bw() <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Management practices in manufacturing firms around the world. '' /> Management practices in manufacturing firms around the world. Figure 6.3 Management practices in manufacturing firms around the world. If you want to switch the order of the bars, use rev(table_mean$m_overall) and rev(table_mean$country) to reverse the order of the values. To look at how management quality varies within countries, instead of just looking at the mean we can use column charts to visualize the entire distribution of scores (as in Empirical Project 1). To compare distributions, we have to use the same horizontal axis, so we will first need to make a frequency table for each distribution to be used. Also, since each country has a different number of observations, we will use percentages instead of frequencies as the vertical axis variable. For three countries of your choice and for the US, carry out the following: Using the overall management score (variable ‘management’), create and fill in a frequency table similar to Figure 6.4 below for the US, and separately for each chosen country. The values in the first column should range from 1 to 5, in intervals of 0.2. Range of management score Frequency Percentage of firms (%) 1.00 1.20 … 4.80 5.00 Frequency table for overall management score. Figure 6.4 Frequency table for overall management score. Plot a column chart for each country to show the distribution of management scores, with the percentage of firms on the vertical axis and the range of management scores on the horizontal axis. On each country’s chart, plot the distribution of the US on top of that country’s distribution, as shown in R walk-through 6.2. Describe any visual similarities and differences between the distributions of your chosen countries and that of the US. (Hint: For example, look at where the distribution is centred, the percentage of observations on the left tail or the right tail of the distribution, and how spread out the scores are.) R walk-through 6.2 Obtaining frequency counts and plotting overlapping histograms To get frequency counts, use the cut function. This function will count the number of observations that fall within the intervals specified in the breaks option (here we specified intervals of 0.2). We store this information in the vector temp_counts and use the table function to display the table of frequencies. temp_counts <- cut(man_data$management [man_data$country == ''Chile''], breaks=seq(0, 5, 0.2)) table(temp_counts) ## temp_counts ## (0,0.2] (0.2,0.4] (0.4,0.6] (0.6,0.8] (0.8,1] (1,1.2] (1.2,1.4] ## 0 0 0 0 0 0 1 ## (1.4,1.6] (1.6,1.8] (1.8,2] (2,2.2] (2.2,2.4] (2.4,2.6] (2.6,2.8] ## 3 6 24 15 30 25 49 ## (2.8,3] (3,3.2] (3.2,3.4] (3.4,3.6] (3.6,3.8] (3.8,4] (4,4.2] ## 52 28 27 21 20 9 6 ## (4.2,4.4] (4.4,4.6] (4.6,4.8] (4.8,5] ## 1 0 0 0 To create the required chart, we will use geom_histogram, part of the ggplot set of functions. Let’s first collect one pair of countries, Chile and the US. If you wanted to produce a histogram for the overall (management) rating, use the following code. g1 <- ggplot(subset(man_data, country == ''Chile''), aes(management)) + geom_histogram(breaks = seq(0, 5, 0.2)) + xlab(''Management score'') + ylab(''Frequency Count'') print(g1) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Distribution of management scores, Chile. '' /> Distribution of management scores, Chile. Figure 6.5 Distribution of management scores, Chile. Tip: Using ggplot, it is straightforward to add a second country to the chart. The way to learn is usually to search the Internet (here we searched for ‘r ggplot multiple histograms’). To plot two countries on the same chart, we added the following options: ..density.. gives a chart with a total area of 1, which ensures the histograms are plotted using the same scale (the result is a graph with an area of 1). For proportions, we need to scale by multiplying by the bin width (0.2 in this case). fill = country plots one histogram for each country breaks = seq(0, 5, 0.2) sets the breakpoints (intervals) at 0.2, starting from 0 and ending at 5 alpha = .5 makes the bars semi-transparent (so we can see both histograms at once) position = ''identity'' overlays the two histograms scale_fill_discrete(name = ''Country'') gives a legend title. g1 <- ggplot(subset(man_data, country %in% c(''Chile'', ''United States'')), aes(x = management, y = 0.2 * ..density.., fill = country)) + geom_histogram(breaks = seq(0, 5, 0.2), alpha = .5, position = ''identity'') + xlab(''Management score'') + ylab(''Density'') + ggtitle(''Histogram for management score'') + scale_fill_discrete(name = ''Country'') + theme_bw() print(g1) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Comparing the distribution of management scores for the US and Chile. '' /> Comparing the distribution of management scores for the US and Chile. Figure 6.6 Comparing the distribution of management scores for the US and Chile. box and whisker plotA graphic display of the range and quartiles of a distribution, where the first and third quartile form the ‘box’ and the maximum and minimum values form the ‘whiskers’. Another way to visualize distributions is a box and whisker plot, which shows some parts of a distribution rather than the whole distribution. We can use box and whisker plots to compare particular aspects of distributions more easily than when looking at the entire distribution. As shown in Figure 6.7, the ‘box’ consists of the first quartile (value corresponding to the bottom 25 per cent, or 25th percentile, of all values), the median, and the third quartile (75th percentile). The ‘whiskers’ are the minimum and maximum values. (In R, the ‘whiskers’ may not be the actual maximum or minimum, since any values larger than 1.5 times the width of the box are considered outliers and are shown as separate points.) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Example of a box and whisker plot. (Note: The mean is not shown in R’s default chart setting, but is denoted here by X. In general, the median may not be in the centre of the box, and can differ greatly from the mean. Using the data shown in Figure 6.7 for a variable from the dataset, the mean and median are very similar.) '' /> Example of a box and whisker plot. (Note: The mean is not shown in R’s default chart setting, but is denoted here by X. In general, the median may not be in the centre of the box, and can differ greatly from the mean. Using the data shown in Figure 6.7 for a variable from the dataset, the mean and median are very similar.) Figure 6.7 Example of a box and whisker plot. (Note: The mean is not shown in R’s default chart setting, but is denoted here by X. In general, the median may not be in the centre of the box, and can differ greatly from the mean. Using the data shown in Figure 6.7 for a variable from the dataset, the mean and median are very similar.) Using the same countries you chose in Question 3: Make a box and whisker plot for each country and the US, showing the distribution of management scores. You can either make a separate chart for each country or show all countries in the same plot. To check that your plots make sense, compare your box and whisker plots to the distributions from Question 3. Use your box and whisker plots to add to your comparisons from Question 3(c). R walk-through 6.3 Creating box and whisker plots We use exactly the same structure as for the overlapping histograms, this time plotting countries on the horizontal axis (x = country) and management scores on the vertical axis (y = management). A useful feature of ggplot is that using more or less the same structure, you can create a variety of graphs. In this example, we include a few more countries, as this can be done without overcrowding the figure. To change the look of our chart, we use the option theme_solarized(). library(ggthemes) g2 <- ggplot(subset(man_data, country %in% c(''Chile'', ''United States'', ''Brazil'', ''Germany'', ''UK'')), aes(x = country, y = management)) + geom_boxplot() + ylab(''Management score'') + ggtitle(''Box and whisker plots for management score'') + theme_solarized() print(g2) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-08.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-08-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-08-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-08-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-08.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Box and whisker plots for a selection of countries. '' /> Box and whisker plots for a selection of countries. Figure 6.8 Box and whisker plots for a selection of countries. From the manufacturing data, firms in the US seem to be managed better (on average) than firms in other countries. To investigate whether this is the case in other sectors, we will use data gathered on hospitals and schools. Using the data for hospitals and schools (AMP_graph_public.csv): Create a table for hospitals and schools, showing the mean management score and criteria score (monitoring, targets, incentives) for each country, as in Figure 6.2a. Make separate bar charts for hospitals and schools showing the mean overall management score for each country, sorted from highest to lowest, as in Figure 6.1. Are the country rankings similar to those in manufacturing? Using your average criteria scores from Question 5(a), suggest some explanations for the observed rankings in either hospitals or schools. (You may find it helpful to research healthcare or educational policies and reforms in those countries to support your explanations.) Part 6.2 Do management practices differ between countries? Learning objectives for this part calculate conditional means for one or more conditions, and compare them on a bar chart construct confidence intervals and use them to assess differences between groups. Using the management survey data collected by Bloom et al. (2012), we can compare average management scores across countries and industries. When we find differences between groups in the survey, we are interested in what that tells us about the true differences in management practices between the countries. confidence intervalA range of values that is centred around the sample value, and is defined so that there is a specified probability (usually 95%) that it contains the ‘true value’ of interest. In Empirical Project 2, we used p-values to assess differences between groups. A p-value tells us the probability that a difference we observe in the data could have arisen by chance. If the p-value is small, we conclude that the data gives us evidence of a real difference between the groups. Now we will use another method that helps us to allow for random variation when we interpret data, called a confidence interval. When we work with data we usually have only a small sample from the entire population of interest. For example, the World Management Survey collects information from a selection of all the firms in a particular country. If we calculate the average management score for the sample, we have an estimate of the average management score across all firms in the country (the ‘true value’) but it may not be a very accurate estimate⁠—especially if the sample is small and management scores vary a lot between firms. A 95% confidence interval is a range of possible values within which the true value might lie. It is estimated from the mean and standard deviation of the data. We cannot be certain that the true value lies in the range (we might have the bad luck to pick an atypical sample) but we can say that there is a 95% per cent probability that it does so. For example, suppose that the average score in the data is 3.5, and we calculate that the 95% confidence is [3.1, 3.9]. Then we say that there is a 95% chance that the true score is between 3.1 and 3.9. As the name suggests, confidence intervals tell us how much confidence we can place in our estimates, or in other words how precisely the sample mean is estimated. The confidence interval gives us a margin of error for our estimate of the true value. If the data varies a lot, the 95% confidence interval may be quite wide. If we have plenty of data, and the standard deviation is low, the estimate will be more precise and the 95% interval will be narrow. It is possible to calculate a confidence interval for any probability: however wide the 95% confidence interval, a 99% confidence interval would be wider, and an 80% one would be narrower. 95% is a common choice: it gives us quite a high degree of confidence, and to go higher tends to lead to very wide intervals. We will use 95% confidence intervals throughout this project. To sum up: A confidence interval is a range of values centred around the sample value and is defined so that there is a specified probability (usually 95%) that it contains the true value of interest. Rule of thumb for comparing means When comparing two distributions, if neither mean is in the 95% confidence interval for the other mean, the p-value for the difference in means is less than 5%. This rule of thumb is handy when looking at charts. If two 95% confidence intervals don’t overlap, we can say immediately that the difference between the means for the two groups is unlikely to have arisen by chance. For a more definite conclusion, we can calculate the actual p-value (see Empirical Project 2) or construct a confidence interval for the difference in means. (This method involves more mathematics so we will discuss that in Empirical Project 8.) We will now build on the results from the Bloom et al. (2012) paper by using 95% confidence intervals to make comparisons between the mean overall management score for different countries and types of firms. The confidence interval for the population mean (mean management score for that country) is centred around the sample mean. To determine the width of the interval, we use the standard deviation and number of firms. First look at manufacturing firms in different countries. Using the manufacturing data (AMP_graph_manufacturing.csv) for three countries of your choice and for the US: Create a summary table for the overall management score as shown in Figure 6.9 below, with one row for each country. Country Mean Standard deviation Number of firms Summary table for manufacturing firms. Figure 6.9 Summary table for manufacturing firms. Determine the width of the 95% confidence interval (this is the distance from the mean to one end of the interval). See R walk-through 6.4 for help on how to do this. You should get a different number for each country. Plot a bar chart showing the mean management score and add the confidence intervals to your chart. Use the width of the confidence intervals to describe how precisely each mean was estimated. Using your chart from Question 1(c) and the rule of thumb, what can you say about the differences between the US management score, and the scores of other countries? How would your results change if you use a different specified probability (for example, 99%)? R walk-through 6.4 Calculating confidence intervals and adding them to a chart As in R walk-through 6.1, we use piping operators (%>%) from the tidyverse package. First, we take man_data and extract the countries we need (filter). Then, we group the data by country (group_by), calculate the required summary measures (summarize), and arrange the data according to the values of mean_m (rev sorts from highest to lowest). We save the final result in table_stats. table_stats <- man_data %>% filter(country %in% c(''Chile'', ''United States'', ''Brazil'', ''Germany'', ''UK'')) %>% group_by(country) %>% summarize(obs = length(management), mean_m = mean(management), sd_m = sd(management, na.rm = TRUE)) %>% arrange(rev(mean_m)) table_stats ## # A tibble: 5 x 4 ## country obs mean_m sd_m ## <fct> <int> <dbl> <dbl> ## 1 United States 1225 3.35 0.643 ## 2 UK 1242 3.03 0.679 ## 3 Chile 317 2.83 0.599 ## 4 Germany 646 3.23 0.569 ## 5 Brazil 569 2.71 0.685 To get the confidence intervals, we use the t.test function, which calculates them automatically (along with a lot of other information). The confidence interval is stored as conf.int[1:2]. # tUS contains a lot of information; $conf.int[1:2] is the # confidence interval. tUS <- t.test(subset(man_data, country == ''United States'', select = management)) tUS$conf.int[1:2] ## [1] 3.312379 3.384448 standard errorA measure of the degree to which the sample mean deviates from the population mean. It is calculated by dividing the standard deviation of the sample by the square root of the number of observations. We want to add these interval values to table_stats. In order for R to plot the confidence intervals, instead of the actual values, we need to store the interval values as the amount to add/subtract from the mean value. The easiest way is to calculate the standard error for the sample mean and multiply this by 1.96 (m_err = 1.96 times sqrt{s^2/(n − 1)}), where 1.96 is the factor required to get a 95% confidence interval (assuming a normal distribution). The confidence interval is then [mean_m − m_err, mean_m + m_err]. table_stats <- man_data %>% filter(country %in% c(''Chile'', ''United States'', ''Brazil'', ''Germany'', ''UK'')) %>% group_by(country) %>% summarize(obs = length(management), mean_m = mean(management), sd_m = sd(management, na.rm = TRUE), m_err = 1.96 * sqrt(sd_m^2 / (obs - 1))) %>% arrange(rev(mean_m)) table_stats ## # A tibble: 5 x 5 ## country obs mean_m sd_m m_err ## <fct> <int> <dbl> <dbl> <dbl> ## 1 United States 1225 3.35 0.643 0.0360 ## 2 UK 1242 3.03 0.679 0.0378 ## 3 Chile 317 2.83 0.599 0.0660 ## 4 Germany 646 3.23 0.569 0.0439 ## 5 Brazil 569 2.71 0.685 0.0563 Now we can use this information to make a bar chart: ggplot(table_stats, aes(y = mean_m, x = country)) + geom_bar(position = position_dodge(), # Use black outlines and add thinner bar outlines stat = ''identity'', colour = ''black'', size = .3) + ylab(''Mean management score'') + xlab('''') + geom_errorbar(aes(ymin = mean_m - m_err, ymax = mean_m + m_err), # Thinner lines for confidence intervals size = .6, width = .5, position = position_dodge(.9)) + coord_cartesian(ylim = c(2, 4)) + theme_bw() + theme(axis.text.x = element_text(size = rel(1.5)), axis.text.y = element_text(size = rel(1.3))) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-10.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-10-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-10-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-10-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-06-10.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Bar chart of mean management score in manufacturing firms for a selection of countries, with 95% confidence intervals. '' /> Bar chart of mean management score in manufacturing firms for a selection of countries, with 95% confidence intervals. Figure 6.10 Bar chart of mean management score in manufacturing firms for a selection of countries, with 95% confidence intervals. Using the data for hospitals or schools (AMP_graph_public.csv), using all available countries: Create a summary table like Figure 6.9 for the overall management score, with one row for each country. Add a column containing the widths of the confidence intervals for the country means. Plot a column chart, showing the confidence intervals. Compare the management practices in the US with those in other countries. Are there any countries for which you can be confident that management practices are either better, or worse, on average than in the US? Explain your answer. Look at the width of your confidence intervals and the corresponding standard deviation and number of observations for each one. Explain whether or not the relationship between them is what you would expect. Part 6.3 What factors affect the quality of management? Learning objectives for this part calculate conditional means for one or more conditions, and compare them on a bar chart construct confidence intervals and use them to assess differences between groups evaluate the usefulness and limitations of survey data for determining causality. Besides documenting and comparing management practices across industries and countries, another purpose of the World Management Survey was to investigate factors that affect management quality. One possible factor affecting differences in management is firm ownership. To look at the data for this factor in the healthcare and education sectors, we will focus on broad groups (public vs privately-owned firms), and for manufacturing firms we will focus on different kinds of private ownership. Using the data for hospitals and schools (AMP_graph_public.csv): Create a table for hospitals and schools, showing the average management score, standard deviation (StdDev), and number of observations, with country as the row variable, and ownership (public or private) and ind as the column variables. Use your table from Question 1(a) to calculate the confidence interval widths for management in public and private hospitals. Then do the same for schools. Plot a bar chart (one for hospitals and another for schools) showing the means and the confidence intervals for the management score. Describe the differences between public and private firms within countries and compare management scores for the same firm type across countries. (For example, is one type of firm generally better managed than the other? Are there similar patterns for hospitals and schools? If you have done Question 5 in Part 6.1, you may want to discuss whether the rankings change after conditioning on ownership type. Besides ownership type, management practices may vary depending on firm size, though it is difficult to predict what the relationship between these variables might be. Larger firms have more employees and could be more difficult to manage well, but may also attract more experienced managers. We will look at the conditional means for manufacturing firms, depending on whether they are above or below the median number of employees (calculated from the data), and see if there is a clear relationship. Using the data for manufacturing firms (AMP_graph_manufacturing.csv): Create a new variable that equals ‘Smaller’ if a firm has less than the median number of employees (330), and ‘Larger’ otherwise. In natural log terms, ‘Smaller’ corresponds to log employment of less than 5.80. For two countries of your choice and the US, create a table showing the mean overall management score, standard deviation, and number of observations, with country and ownership as the row variables, and firm size as the column variable. (Note: When there is only one observation in a group, there is no standard deviation.) Use your table to calculate the confidence interval width for each firm size and ownership type. Plot a bar chart for each country, showing the means and the confidence intervals. Describe any patterns you observe across ownership types and firm size in each country. R walk-through 6.5 Calculating and adding conditional summary statistics and confidence intervals to a chart We will use many techniques encountered previously, but first we have to create a new variable that indicates whether a firm is larger or smaller (size). A firm with lemp_firm > 5.8 is considered larger. We use the factor function to do this. man_data$size <- factor(man_data$lemp_firm > 5.8, labels = c(''small'', ''large'')) We choose Canada, Brazil, and the United States. Again, we use the piping technique to make the table. In the group_by command, we group the variables by size and ownership (as we did in R walk-through 6.1). table_stats2 <- man_data %>% filter(country %in% c(''Canada'', ''United States'', ''Brazil'')) %>% group_by(country, ownership, size) %>% summarize(obs = length(management), mean_m = mean(management, na.rm = TRUE), sd_m = sd(management, na.rm = TRUE)) table_stats2 ## # A tibble: 53 x 6 ## # Groups: country, ownership [?] ## country ownership size obs mean_m sd_m ## <fct> <fct> <fct> <int> <dbl> <dbl> ## 1 Brazil Dispersed Shareholders small 28 3.06 0.667 ## 2 Brazil Dispersed Shareholders large 45 3.48 0.731 ## 3 Brazil Family owned, external CEO small 8 2.82 0.725 ## 4 Brazil Family owned, external CEO large 10 2.99 0.688 ## 5 Brazil Family owned, family CEO small 80 2.50 0.668 ## 6 Brazil Family owned, family CEO large 41 2.70 0.645 ## 7 Brazil Founder small 124 2.35 0.524 ## 8 Brazil Founder large 72 2.66 0.591 ## 9 Brazil Government small 1 4 NaN ## 10 Brazil Government large 2 2.44 1.18 ## # ... with 43 more rows Now we use the variable size as a column variable, so that we can see the summary statistics in two blocks of columns (separately for larger and smaller firms). This is not a standard or straightforward procedure, but an Internet search (for ‘tidyverse spread multiple columns’) gives the following solution. table_stats2_mc <- table_stats2 %>% gather(variable, value, -(country:size)) %>% unite(temp, size, variable) %>% spread(temp, value) print(table_stats2_mc) ## # A tibble: 28 x 8 ## # Groups: country, ownership [28] ## country ownership large_mean_m large_obs large_sd_m small_mean_m ## <fct> <fct> <dbl> <dbl> <dbl> <dbl> ## 1 Brazil Dispersed Share~ 3.48 45 0.731 3.06 ## 2 Brazil Family owned, e~ 2.99 10 0.688 2.82 ## 3 Brazil Family owned, f~ 2.70 41 0.645 2.50 ## 4 Brazil Founder 2.66 72 0.591 2.35 ## 5 Brazil Government 2.44 2 1.18 4 ## 6 Brazil Managers 2.51 7 0.631 2.64 ## 7 Brazil Other 3.01 29 0.541 2.57 ## 8 Brazil Private Equity NA NA NA 3.23 ## 9 Brazil Private Individ~ 2.94 42 0.523 2.69 ## 10 Canada Dispersed Share~ 3.52 53 0.582 3.43 ## # ... with 18 more rows, and 2 more variables: small_obs <dbl>, ## # small_sd_m <dbl> To understand the logic of this command, go through it step by step: first apply gather (which compiles the values of multiple columns (country and size in this case) into a single column), then unite (which pastes multiple columns into one), and then spread (takes two columns and spreads them into multiple columns). So far we have looked at associations between firm characteristics and management practices, but have not made any causal statements. We will now discuss the difficulties with making causal statements using this data and examine how we might determine the direction of causation. For each of the following variables, explain how it could affect management practices, and then explain how management practices could affect it: education level of managers (percentage with a college degree) number of competitors firm size (number of employees). One way to establish the direction of causation is through a randomized field experiment. Read the discussion on pages 22–23 of the Bloom et al. paper (the section ‘Experimental Evidence on Management Quality and Firm Performance’) about one such experiment that was conducted in Indian textile factories. Briefly describe the idea behind a randomized field experiment, and explain, with reference to the results of the experiment in India, whether we can use it to determine the direction of causation between management practice and firm performance. The paper ‘Does Management Matter? Evidence from India’ provides more details about the experiment (pages 9–10 are particularly useful). Figure 12 in the paper shows productivity in treatment and control firms over time, with 95% confidence intervals. Use the information in the chart to describe the effect of the treatment on firm productivity. University of Manchester’s Econometric Computing Learning Resource (ECLR). 2018. ‘R AnalysisTidy’. Updated 9 January 2018. ↩"
});
index.addDoc({
    id: 35,
    title: "Doing Economics: Empirical Project 6: Working in Google Sheets",
    content: "Empirical Project 6 Working in Google Sheets Part 6.1 Looking for patterns in the survey data Learning objectives for this part explain how survey data is collected, and describe measures that can increase the reliability and validity of survey data use column charts and box and whisker plots to compare distributions calculate conditional means for one or more conditions, and compare them on a bar chart use line charts to describe the behaviour of real-world variables over time. First download the data used in the paper to understand how this information was collected. The dataset is publicly available and free of charge, but you will need to create a user account in order to access it. Go to the World Management Survey data registration page. On the middle right-hand side of the page, click the ‘Register’ button. Fill in the form with the required details, then click ‘Register’. An account activation link will be sent to the email you provided. Click on it to activate your account. Now go to the World Management Survey data download page. In the subsection ‘Download the public WMS data now’, click the ‘Download Now’ button. In the ‘Login’ section, enter your account’s email and password, then click ‘Login’. Under the heading ‘Manufacturing: 2004–2010 combined survey data (AMP)’, click the ‘Download’ button. Unzip the files in the downloaded zip folder into your working directory (the folder you will be working from). You may also find it helpful to download the Bloom et al. paper ‘Management practices across firms and countries’. To learn about how Bloom et al. (2012) conducted their survey, read the sections ‘How Can Management Practices Be Measured?’ and ‘Validating the Management Data’ (pages 5–9) of their paper. Briefly describe how the interviews with managers were conducted, and explain some methods the researchers used to improve the reliability and validity of their data. (There are a few technical terms that you may not understand, but these are not necessary for answering this question.) Three aspects of management practices were evaluated: monitoring, targets, and incentives. Do you think that these are the best criteria for assessing management practices? What (if any) important aspects of management are not included in this assessment? (You may also find it helpful to refer to the ‘Contingent Management’ section on pages 23–25 of the paper.) Now we will create some charts to summarize the data and make comparisons across countries, industries (manufacturing, healthcare, retail, and education), and firm characteristics. In ‘Manufacturing: 2004–2010 combined survey data (AMP)’, open the Excel document ‘AMP_graph_manufacturing.csv’. Use this data on manufacturing firms to do the following: In a new tab, create a table like Figure 6.2a, showing the average management scores for all the firms in each of the twenty countries, and fill it in with the required values. The variables for the overall score and three individual criteria are ‘management’, ‘monitor’, ‘target’, and ‘people’. You may find it helpful to use Google Sheets’ PivotTable option—see Google Sheets walk-through 3.1 if you need guidance. For each criterion, rank countries from highest to lowest, then create and fill in a table like Figure 6.2b (see Google Sheets walk-through 4.4 for help on how to assign ranks). Do countries with a high overall rank also tend to rank highly on individual criteria? Country Overall management (mean) Monitoring management (mean) Targets management (mean) Incentives management (mean) Mean of management scores. Figure 6.2a Mean of management scores. Country Overall management (rank) Monitoring management (rank) Targets management (rank) Incentives management (rank) Rank according to management scores. Figure 6.2b Rank according to management scores. Create a bar chart showing the average overall management score (the variable ‘management’) for each country, ordered from highest to lowest. (Hint: You will need to sort your data from highest to lowest so it appears correctly in the chart.) Your chart should look similar to Figure 6.1. Compare your chart with Figure 1 in Bloom et al. (2012). Can you explain why your chart is slightly different? (Hint: See the note at the bottom of Figure 1.) To look at how management quality varies within countries, instead of just looking at the mean we can use column charts to visualize the entire distribution of scores (as in Empirical Project 1). To compare distributions, we have to use the same horizontal axis, so we will first need to make a frequency table for each distribution to be used. Also, since each country has a different number of observations, we will use percentages instead of frequencies as the vertical axis variable. For three countries of your choice and for the US, carry out the following: Using the overall management score (variable ‘management’), create and fill in a frequency table similar to Figure 6.3 below for the US, and separately for each chosen country. The values in the first column should range from 1 to 5, in intervals of 0.2. (Hint: To count observations for a specific country only, you will need to use the IF function and FREQUENCY function together, as shown in Google Sheets walk-through 6.1). Range of management score Frequency Percentage of firms (%) 1.00 1.20 … 4.80 5.00 Frequency table for overall management score. Figure 6.3 Frequency table for overall management score. Plot a column chart for each country to show the distribution of management scores, with the percentage of firms on the vertical axis and the range of management scores on the horizontal axis. On each country’s chart, plot the distribution of the US on top of that country’s distribution, as shown in Google Sheets walk-through 6.2. Describe any visual similarities and differences between the distributions of your chosen countries and that of the US. (Hint: For example look at where the distribution is centred, the percentages of observations on the left tail or the right tail of the distribution, and how spread out the scores are.) Google Sheets walk-through 6.1 Using the IF function <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to use Google Sheets’ IF function within another function. '' /> How to use Google Sheets’ IF function within another function. Figure 6.4 How to use Google Sheets’ IF function within another function. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : In this example, we will make a frequency table for the US data in Columns A to C. It’s a good idea to put all the tables in a separate place from the data. '' /> The data In this example, we will make a frequency table for the US data in Columns A to C. It’s a good idea to put all the tables in a separate place from the data. Figure 6.4a In this example, we will make a frequency table for the US data in Columns A to C. It’s a good idea to put all the tables in a separate place from the data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add a new column to the original spreadsheet : We will create a new column that shows data for the US only, and blank cells for other countries. '' /> Add a new column to the original spreadsheet We will create a new column that shows data for the US only, and blank cells for other countries. Figure 6.4b We will create a new column that shows data for the US only, and blank cells for other countries. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use the IF function to display relevant data values : The IF function will display the value in Column A if the data satisfies the condition we specified (Column N is the “United States”) and leave the cells blank if the condition is not satisfied. '' /> Use the IF function to display relevant data values The IF function will display the value in Column A if the data satisfies the condition we specified (Column N is the “United States”) and leave the cells blank if the condition is not satisfied. Figure 6.4c The IF function will display the value in Column A if the data satisfies the condition we specified (Column N is the “United States”) and leave the cells blank if the condition is not satisfied. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Apply the formula to the remaining cells : After step 5, the data is ready to put in a frequency table. '' /> Apply the formula to the remaining cells After step 5, the data is ready to put in a frequency table. Figure 6.4d After step 5, the data is ready to put in a frequency table. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculating frequencies for a particular country : Now we will apply the FREQUENCY function to the new column we created with the IF function. '' /> Calculating frequencies for a particular country Now we will apply the FREQUENCY function to the new column we created with the IF function. Figure 6.4e Now we will apply the FREQUENCY function to the new column we created with the IF function. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculating frequencies for a particular country : By using the column we created, the FREQUENCY function will only count data for the country we are interested in. '' /> Calculating frequencies for a particular country By using the column we created, the FREQUENCY function will only count data for the country we are interested in. Figure 6.4f By using the column we created, the FREQUENCY function will only count data for the country we are interested in. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculating frequencies for a particular country : After step 8, all the frequency values will be filled in. '' /> Calculating frequencies for a particular country After step 8, all the frequency values will be filled in. Figure 6.4g After step 8, all the frequency values will be filled in. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-h.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-h-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-h-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-h-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-04-h.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Using frequencies to calculate percentages : The $ symbol in the formula tells Google Sheets to keep these row or column numbers the same when copying the formula to other cells. We used it here because we are dividing the frequency value by the total number of observations (Cells B2 to B23). '' /> Using frequencies to calculate percentages The $ symbol in the formula tells Google Sheets to keep these row or column numbers the same when copying the formula to other cells. We used it here because we are dividing the frequency value by the total number of observations (Cells B2 to B23). Figure 6.4h The $ symbol in the formula tells Google Sheets to keep these row or column numbers the same when copying the formula to other cells. We used it here because we are dividing the frequency value by the total number of observations (Cells B2 to B23). Google Sheets walk-through 6.2 Overlaying one column chart over another <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to overlay one column chart over another. '' /> How to overlay one column chart over another. Figure 6.5 How to overlay one column chart over another. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : In this example, we use data for the US (Columns A to C) and Chile (Columns F to H), and plot a column chart of the percentages in Columns C and H (see the steps in walk-through 6.1 for how to calculate these). '' /> The data In this example, we use data for the US (Columns A to C) and Chile (Columns F to H), and plot a column chart of the percentages in Columns C and H (see the steps in walk-through 6.1 for how to calculate these). Figure 6.5a In this example, we use data for the US (Columns A to C) and Chile (Columns F to H), and plot a column chart of the percentages in Columns C and H (see the steps in walk-through 6.1 for how to calculate these). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Plot a column chart : We will create a new table showing only the data we want to include in the chart (Columns C and H). Then we select this table and create a column chart. '' /> Plot a column chart We will create a new table showing only the data we want to include in the chart (Columns C and H). Then we select this table and create a column chart. Figure 6.5b We will create a new table showing only the data we want to include in the chart (Columns C and H). Then we select this table and create a column chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Plot a column chart : After step 3, the column chart will look like the one shown above. '' /> Plot a column chart After step 3, the column chart will look like the one shown above. Figure 6.5c After step 3, the column chart will look like the one shown above. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add axis titles and change the legend position : Give the axes and the chart appropriate titles, and move the legend to the bottom of the chart. '' /> Add axis titles and change the legend position Give the axes and the chart appropriate titles, and move the legend to the bottom of the chart. Figure 6.5d Give the axes and the chart appropriate titles, and move the legend to the bottom of the chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-05-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Change the appearance of the chart columns : You can change the shading and other visual aspects of the columns so that the distributions of both countries are clearly visible. '' /> Change the appearance of the chart columns You can change the shading and other visual aspects of the columns so that the distributions of both countries are clearly visible. Figure 6.5e You can change the shading and other visual aspects of the columns so that the distributions of both countries are clearly visible. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Example of two overlapping distributions on the same column chart. '' /> Example of two overlapping distributions on the same column chart. Figure 6.6 Example of two overlapping distributions on the same column chart. box and whisker plotA graphic display of the range and quartiles of a distribution, where the first and third quartile form the ‘box’ and the maximum and minimum values form the ‘whiskers’. Another way to visualize distributions is a box and whisker plot, which shows some parts of a distribution rather than the whole distribution. We can use box and whisker plots to compare particular aspects of distributions more easily than when looking at the entire distribution. As shown in Figure 6.7, the ‘box’ consists of the first quartile (value corresponding to the bottom 25 per cent, or 25th percentile, of all values), the median, and the third quartile (75th percentile). The ‘whiskers’ are the minimum and maximum values. (In Google Sheets, the ‘whiskers’ may not be the actual maximum or minimum, since any values larger than 1.5 times the width of the box are considered outliers and are shown as separate points.) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-06-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Example of a box and whisker plot. (Note: In Google Sheets, the mean value is sometimes denoted by X. In general, the median may not be in the centre of the box, and can differ greatly from the mean. Using the data shown in Figure 6.7 for a variable from the dataset, the mean and median are very similar.) '' /> Example of a box and whisker plot. (Note: In Google Sheets, the mean value is sometimes denoted by X. In general, the median may not be in the centre of the box, and can differ greatly from the mean. Using the data shown in Figure 6.7 for a variable from the dataset, the mean and median are very similar.) Figure 6.7 Example of a box and whisker plot. (Note: In Google Sheets, the mean value is sometimes denoted by X. In general, the median may not be in the centre of the box, and can differ greatly from the mean. Using the data shown in Figure 6.7 for a variable from the dataset, the mean and median are very similar.) Using the same countries you chose in Question 3: Make a box and whisker plot for each country and the US, showing the distribution of management scores. You can either make a separate chart for each country or show all countries in the same plot. To check that your plots make sense, compare your box and whisker plots to the distributions from Question 3. Use your box and whisker plots to add to your comparisons from Question 3(b). Google Sheets walk-through 6.3 Drawing box and whisker plots <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to create box and whisker plots. '' /> How to create box and whisker plots. Figure 6.8 How to create box and whisker plots. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : In this example, we will use data for the US (Column A) and Chile (Column B). To create a box and whisker plot of more than one variable, each variable needs to be in a separate column. We will filter, then copy and paste the required data into a new tab in Google Sheets. '' /> The data In this example, we will use data for the US (Column A) and Chile (Column B). To create a box and whisker plot of more than one variable, each variable needs to be in a separate column. We will filter, then copy and paste the required data into a new tab in Google Sheets. Figure 6.8a In this example, we will use data for the US (Column A) and Chile (Column B). To create a box and whisker plot of more than one variable, each variable needs to be in a separate column. We will filter, then copy and paste the required data into a new tab in Google Sheets. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the data : We previously used the IF function to create new columns (O and P) that only contain data for the US and Chile, respectively (walk-through 6.1 explains how to use the IF function). We can then use the FILTER function to select cells in those columns that contain values (i.e. are not empty). '' /> Filter the data We previously used the IF function to create new columns (O and P) that only contain data for the US and Chile, respectively (walk-through 6.1 explains how to use the IF function). We can then use the FILTER function to select cells in those columns that contain values (i.e. are not empty). Figure 6.8b We previously used the IF function to create new columns (O and P) that only contain data for the US and Chile, respectively (walk-through 6.1 explains how to use the IF function). We can then use the FILTER function to select cells in those columns that contain values (i.e. are not empty). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the data : After step 5, we have the data needed to make the box and whisker plots. '' /> Filter the data After step 5, we have the data needed to make the box and whisker plots. Figure 6.8c After step 5, we have the data needed to make the box and whisker plots. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the values needed for the box and whisker plots : To make a box and whisker plot, we need to make a summary table showing the minimum and maximum values, as well as the 1st and 3rd quartiles. The minimum and maximum values are calculated using the MIN and MAX functions (see walk-through 2.5). Note that your table should look like the one shown, with countries as the row variable. '' /> Calculate the values needed for the box and whisker plots To make a box and whisker plot, we need to make a summary table showing the minimum and maximum values, as well as the 1st and 3rd quartiles. The minimum and maximum values are calculated using the MIN and MAX functions (see walk-through 2.5). Note that your table should look like the one shown, with countries as the row variable. Figure 6.8d To make a box and whisker plot, we need to make a summary table showing the minimum and maximum values, as well as the 1st and 3rd quartiles. The minimum and maximum values are calculated using the MIN and MAX functions (see walk-through 2.5). Note that your table should look like the one shown, with countries as the row variable. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the values needed for the box and whisker plots : The QUARTILE function finds the value corresponding to the specified quartile. '' /> Calculate the values needed for the box and whisker plots The QUARTILE function finds the value corresponding to the specified quartile. Figure 6.8e The QUARTILE function finds the value corresponding to the specified quartile. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create box and whisker plots : We can now use the summary table to make the box and whisker plot. '' /> Create box and whisker plots We can now use the summary table to make the box and whisker plot. Figure 6.8f We can now use the summary table to make the box and whisker plot. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-g.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-g-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-g-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-g-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-08-g.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Format the box and whisker plot : After step 11, your box and whisker plot should look like the one shown above. '' /> Format the box and whisker plot After step 11, your box and whisker plot should look like the one shown above. Figure 6.8g After step 11, your box and whisker plot should look like the one shown above. From the manufacturing data, firms in the US seem to be managed better (on average) than firms in other countries. To investigate whether this is the case in other sectors, we will use data gathered on hospitals and schools. Using the data for hospitals and schools (AMP_graph_public.csv): Create a table for hospitals and schools, showing the mean manage­ment score and criteria score (monitoring, targets, incentives) for each country, as in Figure 6.2a. (Hint: You may find it helpful to use Google Sheets’ PivotTable option—see Google Sheets walk-through 3.1.) Make separate bar charts for hospitals and schools showing the mean over­all management score for each coun­try, sorted from highest to lowest, as in Figure 6.1. Are the country rankings similar to those in manufacturing? Using your average criteria scores from Question 5(a), suggest some explanations for the observed rankings in either hospitals or schools. (You may find it helpful to research healthcare or educational policies and reforms in those countries to support your explanations.) Part 6.2 Do management practices differ between countries? Learning objectives for this part calculate conditional means for one or more conditions, and compare them on a bar chart construct confidence intervals and use them to assess differences between groups. Using the management survey data collected by Bloom et al. (2012), we can compare average management scores across countries and industries. When we find differences between groups in the survey, we are interested in what that tells us about the true differences in management practices between the countries. confidence intervalA range of values that is centred around the sample value, and is defined so that there is a specified probability (usually 95%) that it contains the ‘true value’ of interest. In Empirical Project 2, we used p-values to assess differences between groups. A p-value tells us the probability that a difference we observe in the data could have arisen by chance. If the p-value is small, we conclude that the data gives us evidence of a real difference between the groups. Now we will use another method that helps us to allow for random variation when we interpret data, called a confidence interval. When we work with data we usually have only a small sample from the entire population of interest. For example, the World Management Survey collects information from a selection of all the firms in a particular country. If we calculate the average management score for the sample, we have an estimate of the average management score across all firms in the country (the ‘true value’) but it may not be a very accurate⁠ estimate—especially if the sample is small and management scores vary a lot between firms. A 95% confidence interval is a range of possible values within which the true value might lie. It is estimated from the mean and standard deviation of the data. We cannot be certain that the true value lies in the range (we might have the bad luck to pick an atypical sample) but we can say that there is a 95% per cent probability that it does so. For example, suppose that the average score in the data is 3.5, and we calculate that the 95% confidence is [3.1, 3.9]. Then we say that there is a 95% chance that the true score is between 3.1 and 3.9. As the name suggests, confidence intervals tell us how much confidence we can place in our estimates, or in other words how precisely the sample mean is estimated. The confidence interval gives us a margin of error for our estimate of the true value. If the data varies a lot, the 95% confidence interval may be quite wide. If we have plenty of data, and the standard deviation is low, the estimate will be more precise and the 95% interval will be narrow. It is possible to calculate a confidence interval for any probability: however wide the 95% confidence interval, a 99% confidence interval would be wider, and an 80% one would be narrower. 95% is a common choice: it gives us quite a high degree of confidence, and to go higher tends to lead to very wide intervals. We will use 95% confidence intervals throughout this project. To sum up: A confidence interval is a range of values centred around the sample mean value and is defined so that there is a specified probability (usually 95%) that it contains the true value of interest. Rule of thumb for comparing means When comparing two distributions, if neither mean is in the 95% confidence interval for the other mean, the p-value for the difference in means is less than 5%. This rule of thumb is handy when looking at charts. If two 95% confidence intervals don’t overlap, we can say immediately that the difference between the means for the two groups is unlikely to have arisen by chance. For a more definite conclusion, we can calculate the actual p-value (see Empirical Project 2) or construct a confidence interval for the difference in means. (This method involves more mathematics so we will discuss that in Empirical Project 8.) We will now build on the results from the Bloom et al. (2012) paper by using 95% confidence intervals to make comparisons between the mean overall management score for different countries and types of firms. The confidence interval for the population mean (mean management score for that country) is centred around the sample mean. To determine the width of the interval, we use the standard deviation and number of firms. First look at manufacturing firms in different countries. Using the manufacturing data (AMP_graph_manufacturing.csv) for three countries of your choice and for the US: Create a summary table for the overall management score as shown in Figure 6.9, with one row for each country. (Hint: Use Google Sheets’ Pivot Table option.) Country Mean Standard deviation Number of firms Summary table for manufacturing firms. Figure 6.9 Summary table for manufacturing firms. Use Google Sheets’ CONFIDENCE.T function to determine the width of the 95% confidence interval (this is the distance from the mean to one end of the interval). See Google Sheets walk-through 6.4 for help on how to do this. You should get a different number for each country. Plot a column chart showing the mean management score and add the confidence intervals to your chart. Use the width of the confidence intervals to describe how precisely each mean was estimated. Using your chart from Question 1(c) and this rule of thumb, what can you say about the differences between the US management score, and the scores of other countries? How would your results change if you use a different specified probability (for example, 99%)? Google Sheets walk-through 6.4 Creating confidence intervals and adding them to a chart <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to create confidence intervals and add them to a chart. '' /> How to create confidence intervals and add them to a chart. Figure 6.10 How to create confidence intervals and add them to a chart. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the width of the confidence interval : In this example we will use data for the US and Chile (shown in Columns A and B). To calculate the width of the 95% confidence interval (distance from the mean to one end of the interval), we first need to calculate the standard deviation and number of observations for each country. The 0.05 in the CONFIDENCE function is 1 − 0.95 (our specified probability). '' /> Calculate the width of the confidence interval In this example we will use data for the US and Chile (shown in Columns A and B). To calculate the width of the 95% confidence interval (distance from the mean to one end of the interval), we first need to calculate the standard deviation and number of observations for each country. The 0.05 in the CONFIDENCE function is 1 − 0.95 (our specified probability). Figure 6.10a In this example we will use data for the US and Chile (shown in Columns A and B). To calculate the width of the 95% confidence interval (distance from the mean to one end of the interval), we first need to calculate the standard deviation and number of observations for each country. The 0.05 in the CONFIDENCE function is 1 − 0.95 (our specified probability). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Plot a column chart : We will first plot a column chart showing the mean values only, then add in the confidence intervals. '' /> Plot a column chart We will first plot a column chart showing the mean values only, then add in the confidence intervals. Figure 6.10b We will first plot a column chart showing the mean values only, then add in the confidence intervals. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Plot a column chart : After step 4, your column chart will look like the one shown above. '' /> Plot a column chart After step 4, your column chart will look like the one shown above. Figure 6.10c After step 4, your column chart will look like the one shown above. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add error bars to the chart : The ‘error bars’ option in Google Sheets plots confidence intervals. We will use the calculated width values from step 1 to determine the size of the error bars. '' /> Add error bars to the chart The ‘error bars’ option in Google Sheets plots confidence intervals. We will use the calculated width values from step 1 to determine the size of the error bars. Figure 6.10d The ‘error bars’ option in Google Sheets plots confidence intervals. We will use the calculated width values from step 1 to determine the size of the error bars. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Add error bars to the chart : After step 6, both columns will have confidence intervals, as shown above. '' /> Add error bars to the chart After step 6, both columns will have confidence intervals, as shown above. Figure 6.10e After step 6, both columns will have confidence intervals, as shown above. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-10-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Resize the column chart : If the confidence intervals are too narrow to be seen clearly, you can make the chart larger. '' /> Resize the column chart If the confidence intervals are too narrow to be seen clearly, you can make the chart larger. Figure 6.10f If the confidence intervals are too narrow to be seen clearly, you can make the chart larger. Using the data for hospitals or schools (AMP_graph_public.csv), using all available countries: Create a summary table like Figure 6.9 for the overall management score, with one row for each country. (Hint: Use Google Sheets’ PivotTable option.) Add a column containing the widths of the confidence intervals for the country means. Plot a column chart, showing the confidence intervals. Compare the management practices in the US with those in other countries. Are there any countries for which you can be confident that management practices are either better, or worse, on average than in the US? Explain your answer. Look at the width of your confidence intervals and the corresponding standard deviation and number of observations for each one. Explain whether or not the relationship between them is what you would expect. Part 6.3 What factors affect the quality of management? Learning objectives for this part calculate conditional means for one or more conditions, and compare them on a bar chart construct confidence intervals and use them to assess differences between groups evaluate the usefulness and limitations of survey data for determining causality. Besides documenting and comparing management practices across industries and countries, another purpose of the World Management Survey was to investigate factors that affect management quality. One possible factor affecting differences in management is firm ownership. To look at the data for this factor in the healthcare and education sectors, we will focus on broad groups (public vs privately-owned firms), and for manufacturing firms we will focus on different kinds of private ownership. Using the data for hospitals and schools (AMP_graph_public.csv): Create a pivot table for hospitals and schools, showing the average management score, standard deviation (StdDev), and number of observations, with ‘country’ as the row variable, and ‘ownership’ (public or private) and ‘ind’ as the column variables. Use your pivot table from Question 1(a) to calculate the confidence interval widths for management in public and private hospitals. Then do the same for schools. Plot a bar chart (one for hospitals and another for schools) showing the means from Question 1(a) and the confidence intervals from Question 1(b). Describe the differences between public and private firms within countries and compare management scores for the same firm type across countries. (For example, is one type of firm generally better managed than the other? Are there similar patterns for hospitals and schools? If you have done Question 5 in Part 6.1, you may want to discuss whether the rankings change after conditioning on ownership type. Besides ownership type, management practices may vary depending on firm size, though it is difficult to predict what the relationship between these variables might be. Larger firms have more employees and could be more difficult to manage well, but may also attract more experienced managers. We will look at the conditional means for manufacturing firms, depending on whether they are above or below the median number of employees (calculated from the data), and see if there is a clear relationship. Using the data for manufacturing firms (AMP_graph_manufacturing.csv): In a new column in the original spreadsheet, use Google Sheets’ IF function (see Google Sheets walk-through 6.1) to create a variable that equals ‘Smaller’ if a firm has less than the median number of employees (330) and ‘Larger’ otherwise. In natural log terms, ‘Smaller’ corresponds to log employment of less than 5.80. For two countries of your choice and the US, create a pivot table showing the mean overall management score, standard deviation, and number of observations, with ‘country’ and ‘ownership’ as the row variables, and firm size (from Question 2(a)) as the column variable. (Note: When there is only one observation in a group, there is no standard deviation.) Use your pivot table from Question 2(b) to calculate the confidence interval width for each firm size and ownership type. Plot a column chart for each country, showing the means from Question 2(b) and the confidence intervals from Question 2(c). Describe any patterns you observe across ownership types and firm size in each country. Google Sheets walk-through 6.5 Using the IF function <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to use the IF function. '' /> How to use the IF function. Figure 6.11 How to use the IF function. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the manufacturing data looks like. We will create a new variable in Column T, according to the log employment values in Column E. We have labelled our new column ‘size’ (Cell T1). '' /> The data This is what the manufacturing data looks like. We will create a new variable in Column T, according to the log employment values in Column E. We have labelled our new column ‘size’ (Cell T1). Figure 6.11a This is what the manufacturing data looks like. We will create a new variable in Column T, according to the log employment values in Column E. We have labelled our new column ‘size’ (Cell T1). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-06-11-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Create a new variable : After completing step 2, you will have a variable for firm size. We used the IF function to fill the cells in Column T with the word ‘Smaller’ if log employment is smaller than 5.8, otherwise the cell is filled with the word ‘Larger’. '' /> Create a new variable After completing step 2, you will have a variable for firm size. We used the IF function to fill the cells in Column T with the word ‘Smaller’ if log employment is smaller than 5.8, otherwise the cell is filled with the word ‘Larger’. Figure 6.11b After completing step 2, you will have a variable for firm size. We used the IF function to fill the cells in Column T with the word ‘Smaller’ if log employment is smaller than 5.8, otherwise the cell is filled with the word ‘Larger’. So far we have looked at associations between firm characteristics and management practices, but have not made any causal statements. We will now discuss the difficulties with making causal statements using this data and examine how we might determine the direction of causation. For each of the following variables, explain how it could affect management practices, and then explain how management practices could affect it: education level of managers (percentage with a college degree) number of competitors firm size (number of employees). One way to establish the direction of causation is through a randomized field experiment. Read the discussion on pages 22–23 of the Bloom et al. paper (the section ‘Experimental Evidence on Management Quality and Firm Performance’) about one such experiment that was conducted in Indian textile factories. Briefly describe the idea behind a randomized field experiment, and explain, with reference to the results of the experiment in India, whether we can use it to determine the direction of causation between management practice and firm performance. The paper ‘Does Management Matter? Evidence from India’ provides more details about the experiment (pages 9–10 are particularly useful). Figure 12 in the paper shows productivity in treatment and control firms over time, with 95% confidence intervals. Use the information in the chart to describe the effect of the treatment on firm productivity."
});
index.addDoc({
    id: 36,
    title: "Doing Economics: Empirical Project 6 Solutions",
    content: "Empirical Project 6 Solutions These are not model answers. They are provided to help students, including those doing the project outside a formal class, to check their progress while working through the questions using the Excel, R, or Google Sheets walk-throughs. There are also brief notes for the more interpretive questions. Students taking courses using Doing Economics should follow the guidance of their instructors. Part 6.1 Looking for patterns in the survey data Interviews were conducted by teams of students from the countries surveyed who had business experience and training. The managers interviewed were middle managers, since they were familiar with day-to-day operations as well as the management practices of the firm (for example hiring and firing decisions, performance reviews). Some measures taken to improve reliability and validity of the data: Interviews were conducted in the manager’s native language, which would minimize misunderstandings. Managers were not told they were being assessed on management practices, only that they were being interviewed on management, so they would be more likely to give honest responses. Managers were also not shown the assessment criteria, so they could not have altered their responses in the hopes of getting a higher score. The firms selected were medium-sized firms that were rarely given press coverage, so interviewers were unlikely to have prior knowledge about firms that could bias their scoring. To check for consistency between interviewers, each firm was scored independently by two people. While these three aspects of management practices are not comprehensive, they are relatively easy to measure, and the resulting definition of ‘good’ and ‘bad’ practice applies to firms across industries and countries. It is therefore possible to make cross-country and/or cross-industry comparisons. There are many possible aspects of management that are not included, such as: leadership qualities that managers possess (although Bloom et al. (2012) admit that this is difficult to quantify) management practices that support innovation strategic decisions such as pricing or takeover decisions (these affect firm performance and survival, but whether a decision was ‘good’ or ‘bad’ depends on the context). Solution figures 6.1 and 6.2 provide the tables with countries listed in alphabetical order. Countries tend to have similar ranks across individual criteria, and ranks for individual criteria are generally similar to overall management rank. We can therefore say that countries with a higher overall rank are better managed across all aspects. Country Overall management (mean) Monitoring management (mean) Targets management (mean) Incentives management (mean) Argentina 2.76 3.08 2.68 2.56 Australia 3.02 3.29 3.02 2.74 Brazil 2.71 3.06 2.69 2.55 Canada 3.17 3.55 3.07 2.94 Chile 2.83 3.14 2.72 2.67 China 2.71 2.90 2.63 2.69 France 3.03 3.43 2.97 2.74 Germany 3.23 3.57 3.22 2.98 Greece 2.73 2.97 2.66 2.58 India 2.67 2.91 2.66 2.63 Italy 3.03 3.26 3.10 2.76 Japan 3.23 3.50 3.34 2.92 Mexico 2.92 3.29 2.88 2.71 New Zealand 2.93 3.18 2.96 2.63 Poland 2.90 3.12 2.94 2.83 Portugal 2.87 3.27 2.83 2.59 Republic of Ireland 2.89 3.14 2.81 2.79 Sweden 3.21 3.64 3.19 2.83 UK 3.03 3.34 2.98 2.86 United States 3.35 3.58 3.26 3.25 Mean of management scores. Solution figure 6.1 Mean of management scores. Country Overall management (rank) Monitoring management (rank) Targets management (rank) Incentives management (rank) Argentina 16 16 17 19 Australia 9 8 7 10 Brazil 19 17 16 20 Canada 5 4 6 3 Chile 15 13 15 14 China 18 20 20 13 France 7 6 9 11 Germany 2 3 3 2 Greece 17 18 19 18 India 20 19 18 15 Italy 8 11 5 9 Japan 3 5 1 4 Mexico 11 9 12 12 New Zealand 10 12 10 16 Poland 12 15 11 7 Portugal 14 10 13 17 Republic of Ireland 13 14 14 8 Sweden 4 1 4 6 UK 6 7 8 5 United States 1 2 2 1 Rank according to management scores. Solution figure 6.2 Rank according to management scores. Solution figure 6.3 shows the average overall management score for each country. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Management practices in manufacturing firms around the world. '' /> Management practices in manufacturing firms around the world. Solution figure 6.3 Management practices in manufacturing firms around the world. The ranking of countries in Figure 1 of Bloom et al. (2012) is slightly different, indicating that the mean values are not the same as in Solution figure 6.1. This difference is due to the number of observations used: Figure 1 uses 9,079 observations, whereas the data we have has 9,207 observations. (Note: It is likely that Bloom et al. (2012) were using an earlier version of this survey data to create their Figure 1. The earlier version had not been updated to include data from the latest round of interviews.) Chile is used as an example in Solution figure 6.4. US Frequency Proportion of firms (%) Chile Frequency Proportion of firms (%) 1.00 0 0.00 1.00 0 0.00 1.20 0 0.00 1.20 0 0.00 1.40 1 0.08 1.40 1 0.32 1.60 0 0.00 1.60 3 0.95 1.80 10 0.82 1.80 6 1.89 2.00 9 0.73 2.00 24 7.57 2.20 28 2.29 2.20 15 4.73 2.40 46 3.76 2.40 30 9.46 2.60 58 4.73 2.60 25 7.89 2.80 96 7.84 2.80 49 15.46 3.00 139 11.35 3.00 52 16.40 3.20 118 9.63 3.20 28 8.83 3.40 164 13.39 3.40 27 8.52 3.60 111 9.06 3.60 21 6.62 3.80 145 11.84 3.80 20 6.31 4.00 114 9.31 4.00 9 2.84 4.20 60 4.90 4.20 6 1.89 4.40 60 4.90 4.40 1 0.32 4.60 31 2.53 4.60 0 0.00 4.80 31 2.53 4.80 0 0.00 5.00 4 0.33 5.00 0 0.00 Frequency tables for the US and Chile. Solution figure 6.4 Frequency tables for the US and Chile. Solution figure 6.5 provides the column chart for the US and Chile. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Comparing the distribution of management scores for the US and Chile. '' /> Comparing the distribution of management scores for the US and Chile. Solution figure 6.5 Comparing the distribution of management scores for the US and Chile. Similarities: The data is not evenly distributed across the scale: the proportion of observations tends to decrease for scores further away from the mean. The spread of the data and the range look similar. Differences: The distribution of the US is more right-centred than that of Chile, with a larger proportion of observations at higher values. Solution figure 6.6 shows box and whisker plots for the US and Chile. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt='' : Box and whisker plots for the US and Chile. '' /> Box and whisker plots for the US and Chile. Solution figure 6.6 Box and whisker plots for the US and Chile. From the box and whisker plots, we can now see that the mean and median for the US is higher than that of Chile. In fact, all quartiles of the distribution of the US are higher than those for Chile. The width of the boxes and length of the whiskers indicates that the shapes of the two distributions are similar. There is also one outlier for the US, which was not clearly visible from the column chart. Solution figures 6.7 and 6.8 show the tables for hospitals and schools. Country Average of management Average of monitoring Average of targets Average of people Canada 2.52 2.82 2.44 2.17 France 2.40 2.59 2.29 2.03 Germany 2.64 2.85 2.55 2.45 Italy 2.48 2.67 2.33 2.20 Sweden 2.57 2.90 2.68 2.36 UK 2.82 3.07 2.71 2.62 US 3.00 3.21 2.87 2.92 Mean scores for hospitals. Solution figure 6.7 Mean scores for hospitals. Country Average of management Average of monitoring Average of targets Average of people Canada 2.78 2.92 2.86 2.33 Germany 2.54 2.70 2.49 2.26 Sweden 2.80 3.09 2.72 2.51 UK 2.96 3.07 2.97 2.75 US 2.72 2.88 2.63 2.47 Mean scores for schools. Solution figure 6.8 Mean scores for schools. Solution figures 6.9 and 6.10 provide separate bar charts for hospitals and schools. The country rankings for both hospitals and schools are different from that of manufacturing. For example, while the UK ranks below the US, Sweden, and Canada in manufacturing, it ranks above these countries in schools. Similarly, Germany has a high ranking for both manufacturing and hospitals, but a low ranking for schools. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-09.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-09-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-09-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-09-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-09.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt='' : Bar chart of mean management score for hospitals. '' /> Bar chart of mean management score for hospitals. Solution figure 6.9 Bar chart of mean management score for hospitals. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-10.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-10-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-10-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-10-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-10.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt='' : Bar chart of mean management score for schools. '' /> Bar chart of mean management score for schools. Solution figure 6.10 Bar chart of mean management score for schools. There are many possible explanations for the observed patterns, for example: UK schools have recently undergone some reforms to improve management, such as decentralization (allowing schools autonomy over their management policies) and sharing of better practices across different schools. These policies could explain why the average management score for schools in the UK is relatively high. Part 6.2 Do management practices differ between countries? Chile is used as an example in Solution figure 6.11. Chile is used as an example. The width of the 95% confidence interval is 0.066. Country Mean Standard deviation Number of firms Width of CI Chile 2.83 0.60 317 0.07 United States 3.35 0.64 1,224 0.04 Mean management score in manufacturing firms for the US and Chile. Solution figure 6.11 Mean management score in manufacturing firms for the US and Chile. Solution figure 6.12 shows the column chart for the US and Chile with 95% confidence intervals. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-12.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-12-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-12-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-12-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-12.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt='' : Bar chart of mean management score in manufacturing firms for the US and Chile, with 95% confidence intervals. '' /> Bar chart of mean management score in manufacturing firms for the US and Chile, with 95% confidence intervals. Solution figure 6.12 Bar chart of mean management score in manufacturing firms for the US and Chile, with 95% confidence intervals. The confidence interval for Chile is wider than that of the US, indicating the mean for Chile is less precisely estimated. Looking at Solution figure 6.11, the low precision is likely due to the smaller number of observations for Chile. The mean score for the US is likely to be different from that of Chile, since both means lie outside each other’s 95% confidence interval. Also, since the confidence intervals are quite far apart, we are quite sure that this result is not just because of our chosen probability of 95%. For example, even if we used 99% confidence intervals, which would be wider, the means would still not be inside each other’s confidence interval. In this case we say our result is robust to the choice of significance level. (In other cases, the results may change depending on the significance level (or probability) chosen.) Solution figure 6.13 provides summary tables for both hospitals and schools. Hospitals Schools Country Average SD Number Average SD Number Width (hospitals) Width (schools) Canada 2.52 0.45 175 2.78 0.39 151 0.07 0.06 France 2.40 0.43 158 0.07 Germany 2.64 0.39 130 2.54 0.43 143 0.07 0.07 Italy 2.48 0.52 166 0.08 Sweden 2.57 0.44 43 2.80 0.44 89 0.13 0.09 UK 2.82 0.43 184 2.96 0.40 110 0.06 0.07 US 3.00 0.54 327 2.72 0.45 285 0.06 0.05 Mean management score and 95% confidence interval width for hospitals and schools. Solution figure 6.13 Mean management score and 95% confidence interval width for hospitals and schools. Solution figures 6.14 and 6.15 provide the bar charts for hospitals and schools, and the discussions follow. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-14.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-14-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-14-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-14-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-14.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Bar chart of mean management score for hospitals, with 95% confidence intervals. '' /> Bar chart of mean management score for hospitals, with 95% confidence intervals. Solution figure 6.14 Bar chart of mean management score for hospitals, with 95% confidence intervals. For hospitals, the mean for the US is likely to be higher than all other countries’ means (i.e. under the assumption that the populations have the same mean, it is unlikely for us to obtain the differences that we see). (We would reach the same conclusion even if 99% confidence intervals were used.) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-15.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-15-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-15-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-15-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-15.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Bar chart of mean management score for schools, with 95% confidence intervals. '' /> Bar chart of mean management score for schools, with 95% confidence intervals. Solution figure 6.15 Bar chart of mean management score for schools, with 95% confidence intervals. For schools, the UK is likely to have a higher mean than the US (i.e. it is unlikely that we could obtain the differences that we see in the sample means, under the assumption that the populations have the same mean), and Germany is likely to have a lower mean. (Without doing formal calculations, we cannot say anything about the means for Sweden and Canada.) Holding all other things fixed, we would expect the confidence intervals to be wider if the standard deviation was larger (the sample mean is estimated less precisely). Holding all other things fixed, we would expect the confidence intervals to be narrower if the number of observations was larger (we sampled more of the whole population). The more observations we have, the closer we can approximate the population mean. Looking at the confidence intervals, standard deviation, and number of observations in our data, we can confirm this relationship. Part 6.3 What factors affect the quality of management? As shown in Solution figures 6.16 and 6.17. As shown in Solution figures 6.16 and 6.17. Private Public Country Mean SD Number Mean SD Number Width (private) Width (public) Canada 2.78 0.79 4 2.52 0.45 171 1.25 0.07 France 2.65 0.51 20 2.37 0.41 138 0.24 0.07 Germany 2.61 0.39 68 2.68 0.38 62 0.09 0.10 Italy 2.71 0.50 33 2.42 0.51 133 0.17 0.09 Sweden 3.10 0.07 2 2.54 0.43 41 0.64 0.13 UK 3.00 0.39 64 2.73 0.42 120 0.10 0.07 US 3.14 0.53 164 2.87 0.52 163 0.08 0.08 Mean management score and 95% confidence interval width for private and public hospitals. Solution figure 6.16 Mean management score and 95% confidence interval width for private and public hospitals. Private Public Country Mean SD Number Mean SD Number Width (private) Width (public) Canada 2.76 0.45 21 2.78 0.38 129 0.21 0.07 Germany 2.73 0.49 16 2.51 0.41 127 0.26 0.07 Sweden 3.07 0.63 23 2.71 0.31 66 0.27 0.08 UK 2.89 0.41 11 2.97 0.40 99 0.28 0.08 US 2.66 0.48 74 2.75 0.44 211 0.11 0.06 Mean management score and 95% confidence interval width for private and public schools. Solution figure 6.17 Mean management score and 95% confidence interval width for private and public schools. Solution figures 6.18 and 6.19 provide the bar charts for hospitals and schools, and the discussions follow. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-18.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-18-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-18-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-18-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-18.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Bar chart of mean management score for public and private hospitals, with 95% confidence intervals. '' /> Bar chart of mean management score for public and private hospitals, with 95% confidence intervals. Solution figure 6.18 Bar chart of mean management score for public and private hospitals, with 95% confidence intervals. Hospitals: After we condition on ownership type, the US still has the highest average score and France has the lowest average score (for both public and private), though compared to Question 5, the rankings in the middle have changed. For example, among private hospitals, Sweden ranks higher than in the overall rankings (though we should interpret this result with caution as there are only two observations for private hospitals). In most countries, private hospitals are better managed than public hospitals (except for Germany, but it is quite likely for us to obtain the differences that we see, under the assumption that the populations have the same mean). Note the very wide confidence intervals for Canada and Sweden, which have very few observations. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-19.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-19-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-19-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-19-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-19.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Bar chart of mean management score for public and private schools, with 95% confidence intervals. '' /> Bar chart of mean management score for public and private schools, with 95% confidence intervals. Solution figure 6.19 Bar chart of mean management score for public and private schools, with 95% confidence intervals. Schools: Looking across countries, it is quite likely that we would observe the differences shown between public and private school management under the assumption that the populations have the same mean, except in Sweden, where we have evidence that private schools have better management than public schools. Also, there is no clear difference in the patterns of management practices in public vs private schools: in some countries public schools have a higher mean, in others, private schools have a higher mean. The solution depends on your software and is not shown. Brazil and Canada are used as examples in Solution figure 6.20. Note that when there is only one observation in a group, there is no standard deviation. Larger Smaller Country; ownership type Mean SD Number Mean SD Number Width (Larger) Width (Smaller) Brazil Dispersed shareholders 3.48 0.73 45 3.06 0.67 28 0.22 0.26 Family owned, external CEO 2.99 0.69 10 2.82 0.73 8 0.49 0.61 Family owned, family CEO 2.70 0.64 41 2.50 0.67 80 0.20 0.15 Founder 2.66 0.59 72 2.35 0.52 124 0.14 0.09 Government 2.44 1.18 2 4.00 1 10.59 Managers 2.51 0.63 7 2.64 0.57 23 0.58 0.25 Other 3.01 0.54 29 2.57 0.40 13 0.21 0.24 Private equity 3.23 0.59 5 0.73 Private individuals 2.94 0.52 42 2.69 0.71 39 0.16 0.23 Canada Dispersed shareholders 3.52 0.58 53 3.43 0.60 53 0.16 0.16 Family owned, external CEO 3.31 0.49 9 2.90 0.47 6 0.37 0.49 Family owned, family CEO 3.02 0.61 14 2.75 0.55 25 0.36 0.23 Founder 3.01 0.69 14 2.86 0.56 37 0.40 0.19 Government 3.00 1 Managers 3.01 0.57 5 3.17 0.49 5 0.70 0.61 Other 3.33 0.40 12 3.15 0.44 16 0.26 0.24 Private equity 3.12 0.58 21 3.34 0.67 11 0.26 0.45 Private individuals 3.46 0.45 37 2.90 0.60 66 0.15 0.15 United States Dispersed Shareholders 3.50 0.56 295 3.45 0.56 158 0.06 0.09 Family owned, external CEO 3.45 0.54 22 2.86 0.63 6 0.24 0.66 Family owned, family CEO 3.44 0.58 42 2.96 0.68 73 0.18 0.16 Founder 3.14 0.51 28 3.14 0.61 60 0.20 0.16 Government 4.06 1 Managers 3.80 0.73 6 3.57 0.66 6 0.77 0.69 Other 3.48 0.48 31 3.06 0.74 21 0.17 0.33 Private equity 3.50 0.43 27 3.34 0.48 27 0.17 0.19 Private individuals 3.40 0.68 68 3.07 0.61 93 0.16 0.13 Table of mean management score and 95% confidence interval width, according to ownership type. Solution figure 6.20 Table of mean management score and 95% confidence interval width, according to ownership type. As shown in Solution figure 6.20. Solution figures 6.21, 6.22 and 6.23 provide the bar charts for Brazil, Canada and the US, and the discussions follow. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-21.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-21-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-21-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-21-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-21.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Brazil: Bar chart of mean management score by ownership type, with 95% confidence intervals. '' /> Brazil: Bar chart of mean management score by ownership type, with 95% confidence intervals. Solution figure 6.21 Brazil: Bar chart of mean management score by ownership type, with 95% confidence intervals. For Brazil: On average, larger firms tend to be managed better, regardless of ownership type. (We cannot say anything meaningful about government-run firms because there are only a few observations.) Firms owned by shareholders (dispersed or private equity) have a higher mean than family-owned firms. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-22.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-22-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-22-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-22-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-22.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Canada: Bar chart of mean management score by ownership type, with 95% confidence intervals. '' /> Canada: Bar chart of mean management score by ownership type, with 95% confidence intervals. Solution figure 6.22 Canada: Bar chart of mean management score by ownership type, with 95% confidence intervals. For Canada: No clear pattern between firm size and management practice is apparent: larger firms are managed better (on average) for some ownership types, while smaller firms are managed better for other types. For most ownership types, it is quite likely that we would observe the differences shown under the assumption that the populations have the same mean (based on the rule of thumb). As with Brazil, shareholder-owned firms are better managed than family-owned firms, though the difference is smaller in absolute terms. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-23.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-23-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-23-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-23-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-06-23.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''US: Bar chart of mean management score by ownership type, with 95% confidence intervals. '' /> US: Bar chart of mean management score by ownership type, with 95% confidence intervals. Solution figure 6.23 US: Bar chart of mean management score by ownership type, with 95% confidence intervals. For the US: Here there is a clearer pattern linking firm size and management practices, with larger firms being managed better (for founder-owned firms, the means are the same). For larger firms, the means for family-owned firms and shareholder-owned firms are similar (it is quite likely that we would observe the differences shown under the assumption that the populations have the same mean). Education level of managers: Better-educated managers may have a broader knowledge of management practices (for example, ideas learned from business school) and can therefore improve the management of the firm. However, it is possible that a well-managed firm would require managers to acquire more education (for example, it could be company policy for employees to have a degree before they can be promoted to managerial positions). Number of competitors: The threat of being driven out by competition could motivate firms to seek better management practices (for example, adopting modern production techniques to cut costs, or retraining employees to increase productivity). However, if good management practices were easy to replicate in a given market, this would attract more firms into the market, resulting in more competition. Firm size: Better management practices might lead to larger firms because this would enable firms to grow while remaining productive. However, larger firms could lead to better management practices because managers have greater incentives to research and implement better management practices in order to find the most efficient way to manage their employees. (The efficiency gains from doing so increase with the number of employees.) In a randomized field experiment, subjects (in this case, firms in India) are randomly assigned to either a treatment or a control group. Researchers try to ensure that there are no other differences between the groups besides the treatment (as in laboratory experiments where researchers only change one thing). Since the assignment is random, we are more confident that any observed differences between the treatment and control groups after the treatment phase are due to the treatment itself (improved management practices), rather than other variables. As long as the randomization has been done properly, we can make causal statements such as ‘improved management practices caused a productivity increase’. The treatment and control groups had similar productivity before the management changes. After receiving ‘treatment’, the treatment group’s productivity increased whereas the control group’s productivity remained at roughly the same level. Using the rule of thumb, we can say that the improvement in productivity due to the treatment was unlikely to be due to chance after Week 32 (approximate estimate)."
});
index.addDoc({
    id: 37,
    title: "Doing Economics: Empirical Project 7: Supply and demand",
    content: "Empirical Project 7 Supply and demand Learning objectives In this project you will: convert from the natural logarithm of a number to the number itself (Part 7.1) draw graphs based on equations (Part 7.1) give an economic interpretation of coefficients in supply and demand equations (Part 7.2) distinguish between exogenous and endogenous shocks (Part 7.2) explain how we can use exogenous supply/demand shocks to identify the demand/supply curve (Part 7.2). Key concepts Concepts needed for this project: the natural log transformation and confidence interval. Concepts introduced in this project: dummy variable and simultaneity. Introduction CORE projects This empirical project is related to material in: Unit 7 of Economy, Society, and Public Policy Unit 7 and Unit 8 of The Economy. You may be familiar with supply and demand diagrams similar to the one shown in Figure 7.1. To find out more about demand and supply curves, read Sections 7.3, 7.9 and 7.10 in Economy, Society, and Public Policy. But how do we know what the supply and demand curves look like in the real world? Textbook models of supply and demand are based on firms’ profit-maximizing decisions and consumers’ willingness to pay, but it is rarely possible to find data on these. Instead, usually the best data available are prices and quantities over a number of periods (both of the product we are interested in and of other products), and information about policies and other events that happened in those periods. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-01.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-01-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-01-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-01-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-01.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Example of supply and demand diagram: Equilibrium in the market for bread. '' /> Example of supply and demand diagram: Equilibrium in the market for bread. Figure 7.1 Example of supply and demand diagram: Equilibrium in the market for bread. We will be looking at the US market for watermelons in 1930–1951 described in the paper ‘Suits’ Watermelon Model’ as an example of how to model demand and supply using available data and interpret the results. Working in Excel Working in R Working in Google Sheets"
});
index.addDoc({
    id: 38,
    title: "Doing Economics: Empirical Project 7: Working in Excel",
    content: "Empirical Project 7 Working in Excel Part 7.1 Drawing supply and demand diagrams Learning objectives for this part convert from the natural logarithm of a number to the number itself draw graphs based on equations. First download the data on the watermelon market. Read the Data dictionary tab and make sure you know what each variable represents. Download the paper ‘Suits’ Watermelon Model’ on the watermelon market. The data is in natural logs: for example, the numbers in the price column are the logs of the prices of watermelons in each year, rather than the prices in dollars. Before plotting supply and demand curves we will first practise converting natural logarithms to numbers. In Part 7.2 we will discuss why it is useful to express relationships between variables (for example, price and quantity) in natural logs. To make charts that look like those in Figure 1 in the paper, you need to convert the relevant variables to their actual values. Excel’s EXP function does the inverse of the LN function, converting the natural log of a number to the number itself. (See Excel walk-through 4.3 for an example of the use of the LN function.) Create two new variables containing the actual values of P and Q. Plot separate line charts for P and Q, with time (in years) on the horizontal axis. Make sure to label your vertical axes appropriately. Your charts should look the same as Figure 1 in the paper. Now we will plot supply and demand curves for a simplified version of the model given in the paper. We will define Q as the quantity of watermelons, in millions, and P as the price per thousand watermelons, and assume that the supply curve is given by the following equation: text{log} P = -2.0 + 1.7~text{log}Q text{ (Supply curve)} Technical note Whenever log (or ln) is used in economics, it refers to natural logarithms. Since this equation shows the price in terms of quantity (instead of quantity in terms of price), it is technically referred to as the inverse supply curve. However, we will be using the terms ‘supply curve’ and ‘demand curve’ to refer to both the supply/demand curve and the inverse supply/demand curve. Using the same notation, the following equation describes the demand curve: text{log} P = 8.5 -0.82~text{log}Q text{ (Demand curve)} To plot a curve, we need to generate a series of points (vertical axis values that correspond to particular horizontal axis values) and join them up. First we will work with the variables in natural log format, and then we will convert them to the actual prices and quantities so that our supply and demand curves will be in familiar units. In a new tab on your spreadsheet: Create a table as shown in Figure 7.2. The first column contains values of Q from 20 to 100, in intervals of 5. (Remember that quantity is measured in millions, so Q = 20 corresponds to 20 million watermelons.) Q Log Q Supply (log P) Demand (log P) Supply (P) Demand (P) 20 25 … 95 100 Calculating supply and demand. Figure 7.2 Calculating supply and demand. Convert the values of Q to natural log format (second column of your table) and use these values, along with the numbers in the equations above, to calculate the corresponding values of log P for supply (third column) and demand (fourth column). Use Excel’s EXP function to convert the log P values into the actual prices, P (fifth and sixth columns). Plot your calculated supply and demand curves on a line chart, with price (P) on the vertical axis and quantity (Q) on the horizontal axis. Make sure to label your curves (for example, using a legend). exogenousComing from outside the model rather than being produced by the workings of the model itself. See also: endogenous. During the time period considered (1930–1951), the market for watermelons experienced a negative supply shock due to the Second World War. Supply was limited because production inputs (land and labour) were being used for the war effort. This shock shifted the entire supply curve because the cause (Second World War) was not part of the supply equation, but was external (also known as being exogenous). Before doing the next question, draw a supply and demand diagram to illustrate what you would expect to happen to price and quantity as a result of the shock (all other things being equal). To see how oil shocks in the 1970s caused by wars in the Middle East shifted the supply curve in the oil market, see Section 7.13 in Economy, Society, and Public Policy. Now we will use equations to show the effects of a negative supply shock on your Excel chart. Suppose that the supply curve after the shock is: text{log} P = -2.0 + 1.7~text{log}Q + 0.4 Add the new supply curve to your line chart and interpret the outcomes, as follows: Create a new column in your table from Question 2 called ‘New supply (log P)’, showing the supply in terms of log prices after the shock. Make another column called ‘New supply (P)’ showing the supply in terms of the actual price in dollars. Add the New supply (P) values to your line chart and verify that your chart looks as expected. Make sure to label the new supply curve. Consumer and producer surplus are explained in Sections 7.6 and 7.11 of Economy, Society, and Public Policy. From your chart, what can you say about the change in total surplus, consumer surplus, and producer surplus as a result of the supply shock? (Hint: You may find the following information useful: the old equilibrium point is Q = 64.5, P = 161.3; the new equilibrium point is Q = 55.0, P = 183.7). Part 7.2 Interpreting supply and demand curves Learning objectives for this part give an economic interpretation of coefficients in supply and demand equations distinguish between exogenous and endogenous shocks explain how we can use exogenous supply/demand shocks to identify the demand/supply curve. You may be wondering why it is useful to express relationships in natural log form. In economics, we do this because there is a convenient interpretation of the coefficients: in the equation log Y = a + b log X, the coefficient b represents the elasticity of Y with respect to X. That is, the coefficient is the percentage change in Y for a 1 per cent change in X. To look at the concept of elasticity in more detail, see Section 7.8 of The Economy. Supply curve: text{log} P = -2.0 + 1.7~text{log}Q Demand curve: text{log} P = 8.5 -0.82~text{log}Q Use the supply and demand equations from Part 7.1 which are shown here, and carry out the following: Calculate the price elasticity of supply (the percentage change in quantity supplied divided by the percentage change in price) and comment on its size (in absolute value). (Hint: You will have to rearrange the equation so that log Q is in terms of log P.) Calculate the price elasticity of demand in the same way and comment on its size (in absolute value). Now we will use this information to take a closer look at the model of the watermelon market in the paper and interpret the equations. The paper assumes that in practice farmers decide how many watermelons to grow (supply) based on last season’s prices of watermelons and other crops they could grow instead (cotton and vegetables), and the current political conditions that support or limit the amount grown. The reasoning for using last season’s prices is that watermelons take time to grow and are also perishable, so farmers cannot wait to see what prices will be in the next season before deciding how many watermelons to plant. The estimated supply equation for watermelons is shown below (this is equation (1) in the paper): text{log}~Q_t = 2.42 + 0.58~text{log}P_{t-1} - 0.32~text{log}C_{t-1} -0.12~text{log}T_{t-1} + 0.07~text{CP}_t - 0.36~text{WW2}_t dummy variable (indicator variable)A variable that takes the value 1 if a certain condition is met, and 0 otherwise. Here, C and T are the prices of cotton and vegetables, and CP is a dummy variable that equals 1 if the government cotton-acreage-allotment program was in effect (1934–1951). This program was intended to prevent cotton prices from falling by limiting the supply of cotton, so farmers who reduced their cotton production were given government compensation according to the size of their reduction. WW2 is a dummy variable that equals 1 if the US was involved in the Second World War at the time (1943–1946). You can read more about the government farm programs for cotton during this time period on pages 67–69 of the report ‘The cotton industry in the United States’. exogenousComing from outside the model rather than being produced by the workings of the model itself. See also: endogenous. endogenousProduced by the workings of a model rather than coming from outside the model. See also: exogenous In this model, the dummy variables and the prices of other crops are exogenous factors that affect the decisions of farmers, and hence also affect the endogenous variables P and Q that are determined by the interaction of supply and demand. The supply curve (right-hand panel of Figure 7.3) shows that if the price rose with no change in exogenous factors, then the quantity supplied by farmers would rise, along the supply curve. But if there is an exogenous shock, captured by a dummy variable, it shifts the entire supply curve by changing its intercept (left hand panel). This changes the supply price for any given quantity. (In this specific example of watermelons, the vertical axis variable would be the log price in the previous period, and the horizontal axis variable would be the quantity in the current period). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Supply curve: Dummy variables shift the entire curve (left-hand panel) while changes in endogenous variables move along the curve (right-hand panel). '' /> Supply curve: Dummy variables shift the entire curve (left-hand panel) while changes in endogenous variables move along the curve (right-hand panel). Figure 7.3 Supply curve: Dummy variables shift the entire curve (left-hand panel) while changes in endogenous variables move along the curve (right-hand panel). With reference to Figure 7.4, for each variable in the supply equation, give an economic interpretation of the coefficient (for example, explain the effect on the farmers’ supply decision) and (where relevant) relate the coefficient to an elasticity. Variable Coefficient 95% confidence interval P (price of watermelons) 0.580 [0.572, 0.586] C (price of cotton) –0.321 [–0.328, –0.314] T (price of vegetables) –0.124 [–0.126, –0.122] CP (cotton program) 0.073 [0.068, 0.077] WW2 (Second World War) –0.360 [–0.365, –0.355] Supply equation coefficients and 95% confidence intervals. Figure 7.4 Supply equation coefficients and 95% confidence intervals. Now we will look at the demand curve (equation (3) in the paper). The paper specifies per capita demand (X_t/N_t) in terms of price and other variables. (alpha_0) is the demand curve intercept: text{log}~(X_t/N_t) = alpha_0 - 1.13 ~text{log}(P_t) + 1.75 ~text{log}(Y_t/N_t) - 0.97~text{log}F_t Using the demand equation and Figure 7.5, give an economic interpretation of each coefficient and (where relevant) relate the coefficient to an elasticity. Variable Coefficient 95% confidence interval P (price of watermelons) –1.125 [–1.738, –0.512] Y/N (per capita income) 1.750 [0.778, 2.722] F (railway freight costs) –0.968 [–1.674, –0.262] Demand equation coefficients and 95% confidence intervals. Figure 7.5 Demand equation coefficients and 95% confidence intervals. Earlier, we mentioned that exogenous supply/demand shocks shift the entire supply/demand curve, whereas endogenous changes (such as changes in price) result in movements along the supply or demand curve. Exogenous shocks that only shift supply or only shift demand come in handy when we try to estimate the shape of the supply and demand curves. Read the information on simultaneity below to understand why exogenous shocks are important for identifying the supply and demand curves. simultaneityWhen the right-hand and left-hand variables in a model equation affect each other at the same time, so that the direction of causality runs both ways. For example, in supply and demand models, the market price affects the quantity supplied and demanded, but quantity supplied and demanded can in turn affect the market price. The simultaneity problem Why we need exogenous shocks that shift only supply or demand In the model of supply and demand, the price and quantity we observe in the data are jointly determined by the supply and demand equations, meaning that they are chosen simultaneously. In other words, the market price affects the quantity supplied and demanded, but the quantity supplied and demanded can in turn affect the market price. In economics we refer to this problem as simultaneity. We cannot estimate the supply and demand curves with price and quantity data alone, because the right-hand-side variable is not independent, but is instead dependent on the left-hand-side variable. In the watermelon dataset, the price and quantity we observe for each year is the equilibrium of supply and demand in that year. The changes in the equilibrium from year to year happen as a result of both shifts and movements along the supply and demand curves, and we cannot disentangle these shifts or movements of the supply and demand curves without additional information. Figure 7.6 illustrates that there can be many different supply and demand curve shifts to explain the same data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Many possible supply and demand curves can explain the data. '' /> <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Many possible supply and demand curves can explain the data. '' /> <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Many possible supply and demand curves can explain the data. '' /> Many possible supply and demand curves can explain the data. Figure 7.6 Many possible supply and demand curves can explain the data. To address this issue, we need to find an exogenous variable that affects one equation but not the other. That way we can be sure that what we observe is due to a shift in one curve, holding the other curve fixed. In the watermelon market, we used the Second World War as an exogenous supply shock in Part 7.1. The war affected the amount of farmland dedicated to producing watermelons, but arguably did not affect demand for watermelons. Figure 7.7 shows how we can use the exogenous supply shock to learn about the demand curve. The solid line shows the part of the demand curve revealed by the supply shock. Under the assumption that the demand curve is a straight line, we can infer what the rest of the curve looks like. If we had more information, for example if the size of the shock varied in each period, then we could use this information to learn more about the shape of the demand curve (for example, check whether it is actually linear). We use similar reasoning (exogenous demand shocks) to identify the supply curve. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Using exogenous supply shocks to identify the demand curve. '' /> Using exogenous supply shocks to identify the demand curve. Figure 7.7 Using exogenous supply shocks to identify the demand curve. Given the supply and demand equations in the watermelon model, give two examples of an exogenous demand shock and explain why they are exogenous."
});
index.addDoc({
    id: 39,
    title: "Doing Economics: Empirical Project 7: Working in R",
    content: "Empirical Project 7 Working in R Download the code To download the code chunks used in this project, right-click on the download link and select ‘Save Link As…’. You’ll need to save the code download to your working directory, and open it in RStudio. Don’t forget to also download the data into your working directory by following the steps in this project. Getting started in R For this project you will need the following packages: tidyverse, to help with data manipulation readxl, to import an Excel spreadsheet. If you need to install either of these packages, run the following code: install.packages(c(''readxl'',''tidyverse'')) You can import the libraries now, or when they are used in the R walk-through below. library(readxl) library(tidyverse) Part 7.1 Drawing supply and demand diagrams Learning objectives for this part convert from the natural logarithm of a number to the number itself draw graphs based on equations. First download the data on the watermelon market. Read the Data dictionary tab and make sure you know what each variable represents. Download the paper ‘Suits’ Watermelon Model’ on the watermelon market. The data is in natural logs: for example, the numbers in the price column are the logs of the prices of watermelons in each year, rather than the prices in dollars. Before plotting supply and demand curves we will first practise converting natural logarithms to numbers. In Part 7.2 we will discuss why it is useful to express relationships between variables (for example, price and quantity) in natural logs. To make charts that look like those in Figure 1 in the paper, you need to convert the relevant variables to their actual values. Follow R walk-through 7.1 to answer the following questions. Create two new variables containing the actual values of P and Q. Plot separate line charts for P and Q, with time (in years) on the horizontal axis. Make sure to label your vertical axes appropriately. Your charts should look the same as Figure 1 in the paper. R walk-through 7.1 Importing data into R and creating tables and charts First we import the data with the read_excel function, using the na = ''NA'' option to indicate how missing data is recorded. library(tidyverse) ## -- Attaching packages ------------------------------------------------------------------------------------------------------------ tidyverse 1.2.1 -- ## v ggplot2 2.2.1 v purrr 0.2.4 ## v tibble 1.4.2 v dplyr 0.7.4 ## v tidyr 0.8.0 v stringr 1.3.0 ## v readr 1.1.1 v forcats 0.3.0 ## -- Conflicts --------------------------------------------------------------------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(readxl) # Set your working directory to the correct folder. # Insert your file path for 'YOURFILEPATH'. setwd(''YOURFILEPATH'') wm_data <- read_excel(''Project 7 datafile.xlsx'', sheet = ''Sheet1'', na = ''NA'') str(wm_data) ## Classes 'tbl_df', 'tbl' and 'data.frame': 22 obs. of 10 variables: ## $ Year : num 1930 1931 1932 1933 1934 ... ## $ log q (Q) : num 4.45 4.36 4.2 4.03 4.1 ... ## $ log h (X) : num 4.38 4.33 4.05 4.01 4.09 ... ## $ log p (P) : num 4.76 4.61 4.37 4.53 4.64 ... ## $ log_pc (C) : num 2.25 1.73 1.87 2.32 2.51 ... ## $ log_pv (T) : num 0.845 2.726 2.588 2.286 1.476 ... ## $ log w (W) : num 3.37 3.14 2.83 2.77 2.92 ... ## $ log n (N) : num 4.81 4.82 4.83 4.83 4.84 ... ## $ log(y/n) (Y/N) : num 6.4 6.24 5.97 5.9 6.02 ... ## $ log p_f (F) : num 2.54 2.55 2.6 2.65 2.62 ... Let’s use the exp function to create the variables p and q from their log counterparts (renamed as log.p and log.q respectively). We also transform the harvest variable (renamed as log.h) and save it as h. The harvest will be at most as large as the crop (q). names(wm_data) <- c(''Year'', ''log.q'', ''log.h'', ''log.p'', ''log.pc'', ''log.pv'', ''log.w'', ''log.n'', ''log.yn'', ''log.pf'') wm_data$p <- exp(wm_data$log.p) # Price wm_data$h <- exp(wm_data$log.h) # Harvest quantity wm_data$q <- exp(wm_data$log.q) # Crop quantity Let’s use plot to produce the chart for the prices, with Year as the horizontal axis variable (xlab) and price (p) as the vertical axis variable (ylab). # type: ''p'' = points, ''l'' = lines, ''o'' = points and lines plot(wm_data$Year, wm_data$p, type = ''o'', xlab = ''Year'', ylab = ''Price'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-07-02.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-07-02-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-07-02-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-07-02-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-07-02.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Line chart for prices for watermelons. '' /> Line chart for prices for watermelons. Figure 7.2 Line chart for prices for watermelons. Now we create the line chart for harvest and crop quantities (the variables h and q, respectively). First, we plot the crop quantities as a dashed line (lty = ''dashed''), then use lines to add a solid line for the harvest data. The legend function adds a chart legend at the specified coordinates (the first two arguments in the function). # type: ''p'' = points, ''l'' = lines, ''o'' = points and lines plot(wm_data$Year, wm_data$q, type = ''o'', pch = 1, lty = ''dashed'', xlab = ''Year'', ylab = ''Price'') # Add the harvest data lines(wm_data$Year, wm_data$h, type = ''o'', pch = 16) # Add a legend legend(1947.5, 55, legend = c(''Crop'', ''Harvest''), col = c(''black'', ''black''), pch = c(1, 16), lty = c(''dashed'', ''solid''), cex = 0.8) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-07-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-07-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-07-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-07-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-07-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Line chart for harvest and crop for watermelons. '' /> Line chart for harvest and crop for watermelons. Figure 7.3 Line chart for harvest and crop for watermelons. Now we will plot supply and demand curves for a simplified version of the model given in the paper. We will define Q as the quantity of watermelons, in millions, and P as the price per thousand watermelons, and assume that the supply curve is given by the following equation: text{log} P = -2.0 + 1.7~text{log}Q text{ (Supply curve)} Technical note Whenever log (or ln) is used in economics, it refers to natural logarithms. Since this equation shows the price in terms of quantity (instead of quantity in terms of price), it is technically referred to as the inverse supply curve. However, we will be using the terms ‘supply curve’ and ‘demand curve’ to refer to both the supply/demand curve and the inverse supply/demand curve. Using the same notation, the following equation describes the demand curve: text{log} P = 8.5 -0.82~text{log}Q text{ (Demand curve)} To plot a curve, we need to generate a series of points (vertical axis values that correspond to particular horizontal axis values) and join them up. First we will work with the variables in natural log format, and then we will convert them to the actual prices and quantities so that our supply and demand curves will be in familiar units. Plot supply and demand curves: Create a table as shown in Figure 7.4. The first column contains values of Q from 20 to 100, in intervals of 5. (Remember that quantity is measured in millions, so Q = 20 corresponds to 20 million watermelons.) Q Log Q Supply (log P) Demand (log P) Supply (P) Demand (P) 20 25 … 95 100 Calculating supply and demand. Figure 7.4 Calculating supply and demand. Convert the values of Q to natural log format (second column of your table) and use these values, along with the numbers in the equations above, to calculate the corresponding values of log P for supply (third column) and demand (fourth column). Convert the log P numbers into the actual prices (fifth and sixth columns). Plot your calculated supply and demand curves on a line chart, with price (P) on the vertical axis and quantity (Q) on the horizontal axis. Make sure to label your curves (for example, using a legend). exogenousComing from outside the model rather than being produced by the workings of the model itself. See also: endogenous. During the time period considered (1930–1951), the market for watermelons experienced a negative supply shock due to the Second World War. Supply was limited because production inputs (land and labour) were being used for the war effort. This shock shifted the entire supply curve because the cause (Second World War) was not part of the supply equation, but was external (also known as being exogenous. Before doing the next question, draw a supply and demand diagram to illustrate what you would expect to happen to price and quantity as a result of the shock (all other things being equal). To see how oil shocks in the 1970s caused by wars in the Middle East shifted the supply curve in the oil market, see Section 7.13 in Economy, Society, and Public Policy. Now we will use equations to show the effects of a negative supply shock on your chart from Question 2. Suppose that the supply curve after the shock is: text{log} P = -2.0 + 1.7~text{log}Q + 0.4 Add the new supply curve to your line chart and interpret the outcomes, as follows: Create a new column in your table from Question 2 called ‘New supply (log P)’, showing the supply in terms of log prices after the shock. Make another column called ‘New supply (P)’ showing the supply in terms of the actual price in dollars. Add the New supply (P) values to your line chart and verify that your chart looks as expected. Make sure to label the new supply curve. Consumer and producer surplus are explained in Sections 7.6 and 7.11 of Economy, Society, and Public Policy. From your chart, what can you say about the change in total surplus, consumer surplus, and producer surplus as a result of the supply shock? (Hint: You may find the following information useful: the old equilibrium point is Q = 64.5, P = 161.3; the new equilibrium point is Q = 55.0, P = 183.7). Part 7.2 Interpreting supply and demand curves Learning objectives for this part give an economic interpretation of coefficients in supply and demand equations distinguish between exogenous and endogenous shocks explain how we can use exogenous supply/demand shocks to identify the demand/supply curve. You may be wondering why it is useful to express relationships in natural log form. In economics, we do this because there is a convenient interpretation of the coefficients: in the equation log Y = a + b log X, the coefficient b represents the elasticity of Y with respect to X. That is, the coefficient is the percentage change in Y for a 1 per cent change in X. To look at the concept of elasticity in more detail, see Section 7.8 of The Economy. Supply curve: text{log} P = -2.0 + 1.7~text{log}Q Demand curve: text{log} P = 8.5 -0.82~text{log}Q Use the supply and demand equations from Part 7.1 which are shown here, and carry out the following: Calculate the price elasticity of supply (the percentage change in quantity supplied divided by the percentage change in price) and comment on its size (in absolute value). (Hint: You will have to rearrange the equation so that log Q is in terms of log P.) Calculate the price elasticity of demand in the same way and comment on its size (in absolute value). Now we will use this information to take a closer look at the model of the watermelon market in the paper and interpret the equations. The paper assumes that in practice farmers decide how many watermelons to grow (supply) based on last season’s prices of watermelons and other crops they could grow instead (cotton and vegetables), and the current political conditions that support or limit the amount grown. The reasoning for using last season’s prices is that watermelons take time to grow and are also perishable, so farmers cannot wait to see what prices will be in the next season before deciding how many watermelons to plant. The estimated supply equation for watermelons is shown below (this is equation (1) in the paper): text{log}~Q_t = 2.42 + 0.58~text{log}P_{t-1} - 0.32~text{log}C_{t-1} -0.12~text{log}T_{t-1} + 0.07~text{CP}_t - 0.36~text{WW2}_t dummy variable (indicator variable)A variable that takes the value 1 if a certain condition is met, and 0 otherwise. Here, C and T are the prices of cotton and vegetables, and CP is a dummy variable that equals 1 if the government cotton-acreage-allotment program was in effect (1934–1951). This program was intended to prevent cotton prices from falling by limiting the supply of cotton, so farmers who reduced their cotton production were given government compensation according to the size of their reduction. WW2 is a dummy variable that equals 1 if the US was involved in the Second World War at the time (1943–1946). You can read more about the government farm programs for cotton during this time period on pages 67–69 of the report ‘The cotton industry in the United States’. exogenousComing from outside the model rather than being produced by the workings of the model itself. See also: endogenous. endogenousProduced by the workings of a model rather than coming from outside the model. See also: exogenous In this model, the dummy variables and the prices of other crops are exogenous factors that affect the decisions of farmers, and hence also affect the endogenous variables P and Q that are determined by the interaction of supply and demand. The supply curve (right-hand panel of Figure 7.3) shows that if the price rose with no change in exogenous factors, then the quantity supplied by farmers would rise, along the supply curve. But if there is an exogenous shock, captured by a dummy variable, it shifts the entire supply curve by changing its intercept (left hand panel). This changes the supply price for any given quantity. (In this specific example of watermelons, the vertical axis variable would be the log price in the previous period, and the horizontal axis variable would be the quantity in the current period). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Supply curve: Dummy variables shift the entire curve (left-hand panel) while changes in endogenous variables move along the curve (right-hand panel). '' /> Supply curve: Dummy variables shift the entire curve (left-hand panel) while changes in endogenous variables move along the curve (right-hand panel). Figure 7.5 Supply curve: Dummy variables shift the entire curve (left-hand panel) while changes in endogenous variables move along the curve (right-hand panel). With reference to Figure 7.6, for each variable in the supply equation, give an economic interpretation of the coefficient (for example, explain the effect on the farmers’ supply decision) and (where relevant) relate the coefficient to an elasticity. Variable Coefficient 95% confidence interval P (price of watermelons) 0.580 [0.572, 0.586] C (price of cotton) –0.321 [–0.328, –0.314] T (price of vegetables) –0.124 [–0.126, –0.122] CP (cotton program) 0.073 [0.068, 0.077] WW2 (Second World War) –0.360 [–0.365, –0.355] Supply equation coefficients and 95% confidence intervals. Figure 7.6 Supply equation coefficients and 95% confidence intervals. Now we will look at the demand curve (equation (3) in the paper). The paper specifies per capita demand (X_t/N_t) in terms of price and other variables. (alpha_0) is the demand curve intercept: text{log}~(X_t/N_t) = alpha_0 - 1.13 ~text{log}(P_t) + 1.75 ~text{log}(Y_t/N_t) - 0.97~text{log}F_t Using the demand equation and Figure 7.7 below, give an economic interpretation of each coefficient and (where relevant) relate the coefficient to an elasticity. Variable Coefficient 95% confidence interval P (price of watermelons) –1.125 [–1.738, –0.512] Y/N (per capita income) 1.750 [0.778, 2.722] F (railway freight costs) –0.968 [–1.674, –0.262] Demand equation coefficients and 95% confidence intervals. Figure 7.7 Demand equation coefficients and 95% confidence intervals. Earlier, we mentioned that exogenous supply/demand shocks shift the entire supply/demand curve, whereas endogenous changes (such as changes in price) result in movements along the supply or demand curve. Exogenous shocks that only shift supply or only shift demand come in handy when we try to estimate the shape of the supply and demand curves. Read the information on simultaneity below to understand why exogenous shocks are important for identifying the supply and demand curves. simultaneityWhen the right-hand and left-hand variables in a model equation affect each other at the same time, so that the direction of causality runs both ways. For example, in supply and demand models, the market price affects the quantity supplied and demanded, but quantity supplied and demanded can in turn affect the market price. The simultaneity problem Why we need exogenous shocks that shift only supply or demand In the model of supply and demand, the price and quantity we observe in the data are jointly determined by the supply and demand equations, meaning that they are chosen simultaneously. In other words, the market price affects the quantity supplied and demanded, but the quantity supplied and demanded can in turn affect the market price. In economics we refer to this problem as simultaneity. We cannot estimate the supply and demand curves with price and quantity data alone, because the right-hand-side variable is not independent, but is instead dependent on the left-hand-side variable. In the watermelon dataset, the price and quantity we observe for each year is the equilibrium of supply and demand in that year. The changes in the equilibrium from year to year happen as a result of both shifts and movements along the supply and demand curves, and we cannot disentangle these shifts or movements of the supply and demand curves without additional information. Figure 7.8 illustrates that there can be many different supply and demand curve shifts to explain the same data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Many possible supply and demand curves can explain the data. '' /> <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Many possible supply and demand curves can explain the data. '' /> <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Many possible supply and demand curves can explain the data. '' /> Many possible supply and demand curves can explain the data. Figure 7.8 Many possible supply and demand curves can explain the data. To address this issue, we need to find an exogenous variable that affects one equation but not the other. That way we can be sure that what we observe is due to a shift in one curve, holding the other curve fixed. In the watermelon market, we used the Second World War as an exogenous supply shock in Part 7.1. The war affected the amount of farmland dedicated to producing watermelons, but arguably did not affect demand for watermelons. Figure 7.9 shows how we can use the exogenous supply shock to learn about the demand curve. The solid line shows the part of the demand curve revealed by the supply shock. Under the assumption that the demand curve is a straight line, we can infer what the rest of the curve looks like. If we had more information, for example if the size of the shock varied in each period, then we could use this information to learn more about the shape of the demand curve (for example, check whether it is actually linear). We use similar reasoning (exogenous demand shocks) to identify the supply curve. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Using exogenous supply shocks to identify the demand curve. '' /> Using exogenous supply shocks to identify the demand curve. Figure 7.9 Using exogenous supply shocks to identify the demand curve. Given the supply and demand equations in the watermelon model, give two examples of an exogenous demand shock and explain why they are exogenous."
});
index.addDoc({
    id: 40,
    title: "Doing Economics: Empirical Project 7: Working in Google Sheets",
    content: "Empirical Project 7 Working in Google Sheets Part 7.1 Drawing supply and demand diagrams Learning objectives for this part convert from the natural logarithm of a number to the number itself draw graphs based on equations. First download the data on the watermelon market. Read the Data dictionary tab and make sure you know what each variable represents. Download the paper ‘Suits’ Watermelon Model’ on the watermelon market. The data is in natural logs: for example the numbers in the price column are logs of the prices of watermelons in each year, rather than the prices in dollars. Before plotting supply and demand curves we will first practise converting natural logarithms to numbers. In Part 7.2 we will discuss why it is useful to express relationships between variables (for example, price and quantity) in natural logs. To make charts that look like those in Figure 1 in the paper, you need to convert the relevant variables to their actual values. Google Sheets’ EXP function does the inverse of the LN function, converting the natural log of a number to the number itself. (See Google Sheets walk-through 4.3 for an example of the use of the LN function.) Create two new variables containing the actual values of P and Q. Plot separate line charts for P and Q, with time (in years) on the horizontal axis. Make sure to label your vertical axes appropriately. Your charts should look the same as Figure 1 in the paper. Now we will plot supply and demand curves for a simplified version of the model given in the paper. We will define Q as the quantity of watermelons, in millions, and P as the price per thousand watermelons, and assume that the supply curve is given by the following equation: text{log} P = -2.0 + 1.7~text{log}Q text{ (Supply curve)} Technical note Whenever log (or ln) is used in economics, it refers to natural logarithms. Since this equation shows the price in terms of quantity (instead of quantity in terms of price), it is technically referred to as the inverse supply curve. However, we will be using the terms ‘supply curve’ and ‘demand curve’ to refer to both the supply/demand curve and the inverse supply/demand curve. Using the same notation, the following equation describes the demand curve: text{log} P = 8.5 -0.82~text{log}Q text{ (Demand curve)} To plot a curve, we need to generate a series of points (vertical axis values that correspond to particular horizontal axis values) and join them up. First we will work with the variables in natural log format, and then we will convert them to the actual prices and quantities so that our supply and demand curves will be in familiar units. In a new tab on your spreadsheet: Create a table as shown in Figure 7.2. The first column contains values of Q from 20 to 100, in intervals of 5. (Remember that quantity is measured in millions, so Q = 20 corresponds to 20 million watermelons.) Q Log Q Supply (log P) Demand (log P) Supply (P) Demand (P) 20 25 … 95 100 Calculating supply and demand. Figure 7.2 Calculating supply and demand. Convert the values of Q to natural log format (second column of your table) and use these values, along with the numbers in the equations above, to calculate the corresponding values of log P for supply (third column) and demand (fourth column). Use Google Sheets’ EXP function to convert the log P values into the actual prices, P (fifth and sixth columns). Plot your calculated supply and demand curves on a line chart, with price (P) on the vertical axis and quantity (Q) on the horizontal axis. Make sure to label your curves (for example, using a legend). exogenousComing from outside the model rather than being produced by the workings of the model itself. See also: endogenous. During the time period considered (1930–1951), the market for watermelons experienced a negative supply shock due to the Second World War. Supply was limited because production inputs (land and labour) were being used for the war effort. This shock shifted the entire supply curve because the cause (Second World War) was not part of the supply equation, but was external (also known as being exogenous). Before doing the next question, draw a supply and demand diagram to illustrate what you would expect to happen to price and quantity as a result of the shock (all other things being equal). To see how oil shocks in the 1970s caused by wars in the Middle East shifted the supply curve in the oil market, see Section 7.13 in Economy, Society, and Public Policy. Now we will use equations to show the effects of a negative supply shock on your Google Sheets chart. Suppose that the supply curve after the shock is: text{log} P = -2.0 + 1.7~text{log}Q + 0.4 Add the new supply curve to your line chart and interpret the outcomes, as follows: Create a new column in your table from Question 2 called ‘New supply (log P)’, showing the supply in terms of log prices after the shock. Make another column called ‘New supply (P)’ showing the supply in terms of the actual price in dollars. Add the New supply (P) values to your line chart and verify that your chart looks as expected. Make sure to label the new supply curve. Consumer and producer surplus are explained in Sections 7.6 and 7.11 of Economy, Society, and Public Policy. From your chart, what can you say about the change in total surplus, consumer surplus, and producer surplus as a result of the supply shock? (Hint: You may find the following information useful: the old equilibrium point is Q = 64.5, P = 161.3; the new equilibrium point is Q = 55.0, P = 183.7). Part 7.2 Interpreting supply and demand curves Learning objectives for this part give an economic interpretation of coefficients in supply and demand equations distinguish between exogenous and endogenous shocks explain how we can use exogenous supply/demand shocks to identify the demand/supply curve. You may be wondering why it is useful to express relationships in natural log form. In economics, we do this because there is a convenient interpretation of the coefficients: in the equation log Y = a + b log X, the coefficient b represents the elasticity of Y with respect to X. That is, the coefficient is the percentage change in Y for a 1 per cent change in X. To look at the concept of elasticity in more detail, see Section 7.8 of The Economy. Supply curve: text{log} P = -2.0 + 1.7~text{log}Q Demand curve: text{log} P = 8.5 -0.82~text{log}Q Use the supply and demand equations from Part 7.1 which are shown here, and carry out the following: Calculate the price elasticity of supply (the percentage change in quantity supplied divided by the percentage change in price) and comment on its size (in absolute value). (Hint: You will have to rearrange the equation so that log Q is in terms of log P.) Calculate the price elasticity of demand in the same way and comment on its size (in absolute value). Now we will use this information to take a closer look at the model of the watermelon market in the paper and interpret the equations. The paper assumes that in practice farmers decide how many watermelons to grow (supply) based on last season’s prices of watermelons and other crops they could grow instead (cotton and vegetables), and the current political conditions that support or limit the amount grown. The reasoning for using last season’s prices is that watermelons take time to grow and are also perishable, so farmers cannot wait to see what prices will be in the next season before deciding how many watermelons to plant. The estimated supply equation for watermelons is shown below (this is equation (1) in the paper): text{log}~Q_t = 2.42 + 0.58~text{log}P_{t-1} - 0.32~text{log}C_{t-1} -0.12~text{log}T_{t-1} + 0.07~text{CP}_t - 0.36~text{WW2}_t dummy variable (indicator variable)A variable that takes the value 1 if a certain condition is met, and 0 otherwise. Here, C and T are the prices of cotton and vegetables, and CP is a dummy variable that equals 1 if the government cotton-acreage-allotment program was in effect (1934–1951). This program was intended to prevent cotton prices from falling by limiting the supply of cotton, so farmers who reduced their cotton production were given government compensation according to the size of their reduction. WW2 is a dummy variable that equals 1 if the US was involved in the Second World War at the time (1943–1946). You can read more about the government farm programs for cotton during this time period on pages 67–69 of the report ‘The cotton industry in the United States’. exogenousComing from outside the model rather than being produced by the workings of the model itself. See also: endogenous. endogenousProduced by the workings of a model rather than coming from outside the model. See also: exogenous In this model, the dummy variables and the prices of other crops are exogenous factors that affect the decisions of farmers, and hence also affect the endogenous variables P and Q that are determined by the interaction of supply and demand. The supply curve (right-hand panel of Figure 7.3) shows that if the price rose with no change in exogenous factors, then the quantity supplied by farmers would rise, along the supply curve. But if there is an exogenous shock, captured by a dummy variable, it shifts the entire supply curve by changing its intercept (left hand panel). This changes the supply price for any given quantity. (In this specific example of watermelons, the vertical axis variable would be the log price in the previous period, and the horizontal axis variable would be the quantity in the current period). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Supply curve: Dummy variables shift the entire curve (left-hand panel) while changes in endogenous variables move along the curve (right-hand panel). '' /> Supply curve: Dummy variables shift the entire curve (left-hand panel) while changes in endogenous variables move along the curve (right-hand panel). Figure 7.3 Supply curve: Dummy variables shift the entire curve (left-hand panel) while changes in endogenous variables move along the curve (right-hand panel). With reference to Figure 7.4, for each variable in the supply equation, give an economic interpretation of the coefficient (for example, explain the effect on the farmers’ supply decision) and (where relevant) relate the coefficient to an elasticity. Variable Coefficient 95% confidence interval P (price of watermelons) 0.580 [0.572, 0.586] C (price of cotton) –0.321 [–0.328, –0.314] T (price of vegetables) –0.124 [–0.126, –0.122] CP (cotton program) 0.073 [0.068, 0.077] WW2 (Second World War) –0.360 [–0.365, –0.355] Supply equation coefficients and 95% confidence intervals. Figure 7.4 Supply equation coefficients and 95% confidence intervals. Now we will look at the demand curve (equation (3) in the paper). The paper specifies per capita demand (X_t/N_t) in terms of price and other variables. (alpha_0) is the demand curve intercept: text{log}~(X_t/N_t) = alpha_0 - 1.13 ~text{log}(P_t) + 1.75 ~text{log}(Y_t/N_t) - 0.97~text{log}F_t Using the demand equation and Figure 7.5, give an economic interpretation of each coefficient and (where relevant) relate the coefficient to an elasticity. Variable Coefficient 95% confidence interval P (price of watermelons) –1.125 [–1.738, –0.512] Y/N (per capita income) 1.750 [0.778, 2.722] F (railway freight costs) –0.968 [–1.674, –0.262] Demand equation coefficients and 95% confidence intervals. Figure 7.5 Demand equation coefficients and 95% confidence intervals. Earlier, we mentioned that exogenous supply/demand shocks shift the entire supply/demand curve, whereas endogenous changes (such as changes in price) result in movements along the supply or demand curve. Exogenous shocks that only shift supply or only shift demand come in handy when we try to estimate the shape of the supply and demand curves. Read the information on simultaneity below to understand why exogenous shocks are important for identifying the supply and demand curves. simultaneityWhen the right-hand and left-hand variables in a model equation affect each other at the same time, so that the direction of causality runs both ways. For example, in supply and demand models, the market price affects the quantity supplied and demanded, but quantity supplied and demanded can in turn affect the market price. The simultaneity problem Why we need exogenous shocks that shift only supply or demand In the model of supply and demand, the price and quantity we observe in the data are jointly determined by the supply and demand equations, meaning that they are chosen simultaneously. In other words, the market price affects the quantity supplied and demanded, but the quantity supplied and demanded can in turn affect the market price. In economics we refer to this problem as simultaneity. We cannot estimate the supply and demand curves with price and quantity data alone, because the right-hand-side variable is not independent, but is instead dependent on the left-hand-side variable. In the watermelon dataset, the price and quantity we observe for each year is the equilibrium of supply and demand in that year. The changes in the equilibrium from year to year happen as a result of both shifts and movements along the supply and demand curves, and we cannot disentangle these shifts or movements of the supply and demand curves without additional information. Figure 7.6 illustrates that there can be many different supply and demand curve shifts to explain the same data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Many possible supply and demand curves can explain the data. '' /> <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Many possible supply and demand curves can explain the data. '' /> <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-06-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Many possible supply and demand curves can explain the data. '' /> Many possible supply and demand curves can explain the data. Figure 7.6 Many possible supply and demand curves can explain the data. To address this issue, we need to find an exogenous variable that affects one equation but not the other. That way we can be sure that what we observe is due to a shift in one curve, holding the other curve fixed. In the watermelon market, we used the Second World War as an exogenous supply shock in Part 7.1. The war affected the amount of farmland dedicated to producing watermelons, but arguably did not affect demand for watermelons. Figure 7.7 shows how we can use the exogenous supply shock to learn about the demand curve. The solid line shows the part of the demand curve revealed by the supply shock. Under the assumption that the demand curve is a straight line, we can infer what the rest of the curve looks like. If we had more information, for example if the size of the shock varied in each period, then we could use this information to learn more about the shape of the demand curve (for example, check whether it is actually linear). We use similar reasoning (exogenous demand shocks) to identify the supply curve. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-07-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Using exogenous supply shocks to identify the demand curve. '' /> Using exogenous supply shocks to identify the demand curve. Figure 7.7 Using exogenous supply shocks to identify the demand curve. Given the supply and demand equations in the watermelon model, give two examples of an exogenous demand shock and explain why they are exogenous."
});
index.addDoc({
    id: 41,
    title: "Doing Economics: Empirical Project 7 Solutions",
    content: "Empirical Project 7 Solutions These are not model answers. They are provided to help students, including those doing the project outside a formal class, to check their progress while working through the questions using the Excel, R, or Google Sheets walk-throughs. There are also brief notes for the more interpretive questions. Students taking courses using Doing Economics should follow the guidance of their instructors. Part 7.1 Drawing supply and demand diagrams Solution figure 7.1 shows the two new variables and the actual values of P and Q. Year log (Q) log (P) Q P 1930 4.45 4.76 85.51 116.95 1931 4.36 4.61 77.99 100.93 1932 4.20 4.37 66.99 78.89 1933 4.03 4.53 56.37 92.90 1934 4.10 4.64 60.12 104.00 1935 4.20 4.56 66.38 95.94 1936 4.14 4.85 62.52 127.94 1937 4.26 4.66 70.96 105.93 1938 4.26 4.69 70.80 108.90 1939 4.14 4.78 63.10 118.85 1940 4.35 4.69 77.09 108.90 1941 4.17 4.90 64.42 133.97 1942 4.03 5.48 56.50 239.89 1943 3.88 6.10 48.53 444.65 1944 4.26 5.91 70.80 368.14 1945 4.29 6.02 72.95 410.22 1946 4.39 5.95 80.91 383.72 1947 4.40 5.77 81.10 319.17 1948 4.30 5.96 73.79 389.06 1949 4.36 5.74 78.35 311.90 1950 4.41 5.78 82.42 322.86 1951 4.42 5.91 83.37 368.99 Prices and quantities of watermelons (values rounded to two decimal places). Solution figure 7.1 Prices and quantities of watermelons (values rounded to two decimal places). Solution figures 7.2 and 7.3 provide line charts for P and Q. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-02.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-02-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-02-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-02-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-02.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Price of watermelons (USD per 1,000, 1931–1950). '' /> Price of watermelons (USD per 1,000, 1931–1950). Solution figure 7.2 Price of watermelons (USD per 1,000, 1931–1950). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Quantity of watermelons planted (millions, 1931–1950). '' /> Quantity of watermelons planted (millions, 1931–1950). Solution figure 7.3 Quantity of watermelons planted (millions, 1931–1950). Solution figure 7.4 provides the solution, with the solutions to (b) and (c). Solution figure 7.4 provides the solution, with the solutions to (a) and (c). Solution figure 7.4 provides the solution, with the solutions to (a) and (b). Q Log Q Supply (log P) Demand (log P) Supply (P) Demand (P) 20 3.00 3.09 6.04 22.04 421.37 25 3.22 3.47 5.86 32.20 350.91 30 3.40 3.78 5.71 43.91 302.18 35 3.56 4.04 5.58 57.06 266.30 40 3.69 4.27 5.48 71.60 238.68 45 3.81 4.47 5.38 87.47 216.70 50 3.91 4.65 5.29 104.63 198.77 55 4.01 4.81 5.21 123.03 183.83 60 4.09 4.96 5.14 142.65 171.17 65 4.17 5.10 5.08 163.44 160.29 70 4.25 5.22 5.02 185.39 150.84 75 4.32 5.34 4.96 208.46 142.55 80 4.38 5.45 4.91 232.63 135.20 85 4.44 5.55 4.86 257.88 128.64 90 4.50 5.65 4.81 284.20 122.75 95 4.55 5.74 4.77 311.56 117.43 100 4.61 5.83 4.72 339.95 112.59 Calculated prices and quantities (in natural logs and actual values). Solution figure 7.4 Calculated prices and quantities (in natural logs and actual values). Solution figure 7.5 shows plotted supply and demand curves. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Supply and demand diagram. '' /> Supply and demand diagram. Solution figure 7.5 Supply and demand diagram. Solution figure 7.5 provides the full table with columns added for ‘New supply (log P)’ and ‘New supply (P)’. Q Log Q Supply (log P) Demand (log P) Supply (P) Demand (P) New supply (log P) New supply (P) 20 3.00 3.09 6.04 22.04 421.37 3.49 32.88 25 3.22 3.47 5.86 32.20 350.91 3.87 48.04 30 3.40 3.78 5.71 43.91 302.18 4.18 65.50 35 3.56 4.04 5.58 57.06 266.30 4.44 85.12 40 3.69 4.27 5.48 71.60 238.68 4.67 106.81 45 3.81 4.47 5.38 87.47 216.70 4.87 130.49 50 3.91 4.65 5.29 104.63 198.77 5.05 156.09 55 4.01 4.81 5.21 123.03 183.83 5.21 183.55 60 4.09 4.96 5.14 142.65 171.17 5.36 212.81 65 4.17 5.10 5.08 163.44 160.29 5.50 243.83 70 4.25 5.22 5.02 185.39 150.84 5.62 276.56 75 4.32 5.34 4.96 208.46 142.55 5.74 310.98 80 4.38 5.45 4.91 232.63 135.20 5.85 347.04 85 4.44 5.55 4.86 257.88 128.64 5.95 384.72 90 4.50 5.65 4.81 284.20 122.75 6.05 423.98 95 4.55 5.74 4.77 311.56 117.43 6.14 464.79 100 4.61 5.83 4.72 339.95 112.59 6.23 507.14 New supply after the shock. Solution figure 7.6 New supply after the shock. Solution figure 7.7 shows the line chart with the additional New supply (P) values plotted. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-07-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Supply and demand after a negative supply shock. '' /> Supply and demand after a negative supply shock. Solution figure 7.7 Supply and demand after a negative supply shock. From Solution figure 7.7, we can see that total surplus, producer surplus, and consumer surplus have all decreased. Part 7.2 Interpreting supply and demand curves We rewrite the supply equation as log Q = 1.18 + 0.59 log P. The price elasticity of supply is therefore 0.59, which is inelastic (so supply is not very responsive to changes in price). The demand equation can be written as log Q = 10.37 – 1.22 log P. The price elasticity of demand is therefore 1.22, which is elastic (demand is relatively responsive to changes in price). Note: This data exercise highlights why it is necessary to calculate the elasticities of supply and demand from the data rather than simply looking at the shapes of the supply and demand curves as they appear with the chosen scales for the horizontal and vertical axes. Economic interpretation and elasticity (where relevant) of given coefficients: Price of watermelons: The coefficient is positive, meaning that farmers supply more when the price is higher. The (own) price elasticity of supply is 0.58 and this coefficient is estimated quite precisely (small confidence interval), so we can conclude that supply is price inelastic. Price of cotton; price of vegetables: The coefficient represents the cross-price elasticity of supply (how responsive supply is to the price of other goods). The coefficient is negative. Cotton and vegetables are alternative crops to plant instead of watermelons, so when they become more expensive, farmers will produce fewer watermelons and produce cotton/vegetables instead. The cross-price elasticities of supply are both smaller than 1 (in absolute value) and quite precisely estimated, so we can conclude that watermelon supply is not very responsive to the prices of cotton and vegetables. Cotton program: This program was intended to limit production of cotton (one alternative crop to watermelons). The coefficient is positive and precisely estimated, so we can be quite confident that the policy resulted in some land being dedicated to watermelon production instead. Second World War: This event was a negative supply shock, because some land was dedicated to the war effort instead of watermelon production. The coefficient is negative and quite precisely estimated, so we can conclude that the war did have the effect of reducing supply. Economic interpretation and elasticity (where relevant) of given coefficients: Price of watermelons: The own-price elasticity is negative (as expected), but since the confidence intervals are quite wide, we cannot make any conclusions about how responsive demand is to price. Per-capita income: Per-capita demand for watermelons increases with income, but again, the confidence intervals are quite wide so we do not have a precise estimate of the income elasticity of demand. Railway freight costs: The coefficient is negative, meaning that demand decreases as transportation costs increase (as expected). The 95% confidence interval is quite wide so we do not have a precise estimate of how responsive demand is to railway freight costs. Many possible examples, including: A sudden change in consumers’ preferences for watermelon: If a medical article is published that extols the benefits of eating watermelons, watermelons may suddenly become a trendy diet food and the demand curve would shift to the right. Demand-side policy changes: A government health initiative that involves subsidies for purchasing any type of fruits would increase the demand for watermelons, shifting the demand curve to the right."
});
index.addDoc({
    id: 42,
    title: "Doing Economics: Empirical project 8: Measuring the non-monetary cost of unemployment",
    content: "Empirical Project 8 Measuring the non-monetary cost of unemployment Learning objectives In this project you will: practise working with fairly large datasets detect and correct entries in a dataset (Part 8.1) recode variables to make them easier to analyse (Part 8.1) calculate percentiles for subsets of the data (Part 8.1) use column charts, line charts, and scatterplots to visualize data (Part 8.2) calculate and interpret correlation coefficients (Part 8.2) calculate and interpret confidence intervals for the difference in mean between two groups (Part 8.3). Key concepts Concepts needed for this project: mean, standard deviation, range, percentile, correlation, correlation coefficient, confidence interval. Concepts introduced in this project: confidence interval for the difference in means. Introduction CORE projects This empirical project is related to material in: Unit 6 and Unit 8 of Economy, Society, and Public Policy Unit 6 and Unit 9 of The Economy. Unemployment is a problem for society, not only because of the lost output and wages, but also because it can have a direct impact on individuals’ wellbeing and life satisfaction. If societies have a conviction that able people of a working age should be working, then being unemployed could result in a fear of being stigmatized, a sense of shame from being unemployed, and feeling inferior to others who are employed, all of which would reduce an individual’s life satisfaction. Disutility from unemployment is a concept that we cannot measure directly, so instead we will use self-reported life satisfaction. This measure has its limitations but is widely used to quantify costs that we cannot observe, such as the effect of becoming chronically ill or other life-changing events. We will be using an approach and data that is similar to the study ‘Employment status and subjective well-being’, which used the European Values Study (EVS), a cross-country survey, to investigate the differences in life satisfaction between people of different employment statuses. The hypothesis was that unemployed people would on average be less satisfied with life than employed people, and that this relationship between employment status and life satisfaction would vary depending on social norms. There is evidence to support this hypothesis in specific countries, so we will explore this relationship in EU countries.123 As discussed earlier, one explanation for a relationship between employment status and reported life satisfaction is social norms regarding a work ethic. If social norms are an important determinant of life satisfaction, then we expect the gap in life satisfaction between employed and unemployed to be larger in countries with a stronger self-reported work ethic. While the main focus of this project is on the (full-time) employed and the unemployed, we will also consider whether life satisfaction differs for other employment statuses, such as being retired. Norms of working may not be as strong for the elderly, so the lack of formal employment would have less of an effect on life satisfaction compared with working-age people who are unemployed. Working in Excel Working in R Working in Google Sheets Winkelmann, L., and Winkelmann, R. (1998). ‘Why are the unemployed so unhappy? Evidence from panel data’. Economica, 65(257), 1–15. ↩ Clark, A. E. (2003). ‘Unemployment as a social norm: Psychological evidence from panel data’. Journal of labor economics, 21(2), 323–351. ↩ Clark, A. E., and Oswald, A. J. (1994). ‘Unhappiness and unemployment’. The Economic Journal, 104(424), 648–659. ↩"
});
index.addDoc({
    id: 43,
    title: "Doing Economics: Empirical Project 8: Working in Excel",
    content: "Empirical Project 8 Working in Excel Part 8.1 Cleaning and summarizing the data Learning objectives for this part detect and correct entries in a dataset recode variables to make them easier to analyse calculate percentiles for subsets of the data. So far we have been working with data that was formatted correctly. However, sometimes the datasets will be messier and we will need to ‘clean’ the data before starting any analysis. Data cleaning involves checking that all variables are formatted correctly, all observations are entered correctly (e.g. no typos), and missing values are coded in a way that the software you are using can understand. Otherwise, the software will either not be able to analyse your data, or only analyse the observations it recognizes, which could lead to incorrect results and conclusions. In the data we will use, the European Values Study (EVS) data has been converted to Excel from another program, so there are some entries that were not converted correctly and some variables that need to be recoded (for example, replacing words with numbers, or replacing one number with another). Download the EVS data and documentation: Download the EVS data. For the documentation, go to the data download site. Click on the ‘Data and Documents’ button in the middle of the page, then click the ‘Other Documents’ button and download the PDF file called ‘ZA4804_EVS_VariableCorrespondence.pdf’. This file contains information about each variable in the dataset (e.g. variable name and what it measures). The data spreadsheet contains an incomplete Data dictionary and four tabs with the data collected from each wave of the survey. The variable names currently do not tell us what the variable is, so we need to relabel them to avoid confusion. Use the PDF you have downloaded to fill in Column C (Variable de­scription) of the Data dictionary tab. The second column of the PDF lists all variables in the dataset (alphabetically) and tells you what it measures. Fill in Column B with an appropriate new name for each variable and rename the variables in the other four tabs accordingly. Throughout this project we will refer to the variables using their original names, so the Data dictionary tab will come in handy. In general, data dictionaries and variable correspondences are useful because they contain important information about what each variable represents and how it is measured, which usually cannot be summarized in a short variable name. Now we will take a closer look at how some of the variables were measured. Variable A170 is reported life satisfaction on a scale of 1 (dissatisfied) to 10 (satisfied). Respondents answered the question ‘All things considered, how satisfied are you with your life as a whole these days?’ Discuss the assumptions needed to use this measure in interpersonal and cross-country comparisons, and whether you think they are plausible. (You may find it helpful to refer to Box 2.1 of the ‘OECD guidelines on measuring subjective well-being’.) An individual’s employment status (variable X028) was self-reported. Explain whether misreporting of employment status is likely to be an issue, and give some factors that may affect the likelihood of misreporting in this context. Variables C036 to C041 ask about an individual’s attitudes towards work. With self-reports, we may also be concerned that individuals are using a heuristic (rule of thumb) to answer the questions. Table 2.1 of the ‘OECD Guidelines on measuring subjective well-being’ lists some response biases and heuristics that individuals could use. Pick three that you think particularly apply to questions about life satisfaction or work ethic and describe how we might check whether this issue may be present in our data. Before doing any data analysis, it is important to check that all variables are coded in a way that the software can recognize. This process involves checking how missing values are coded (usually these need to be coded in a particular way for each software), and that numerical variables (numbers) only contain numbers and not text (in order to calculate summary statistics). We will now check and clean the dataset so it is ready to use. Make the following changes to all relevant data tabs: Currently, missing values are recorded as ‘.a’, but we would like them to be blank cells. Use Excel’s Find and Replace tool to change the ‘.a’ to blank cells for variables A009 to X047D. (See Excel walk-through 8.1 for help on how to do this). Variable A170 (life satisfaction) is currently a mixture of numbers (2 to 9) and words (‘Satisfied’ and ‘Dissatisfied’), but we would like it to be all numbers. Replace the word ‘Dissatisfied’ with the number 1, and the word ‘Satisfied’ with the number 10. Similarly, variable X011_01 (number of children) has recorded no children as a word rather than a number. Replace ‘No children’ with the number 0. The variables C036 to C041 should be replaced with numbers ranging from 1 (‘Strongly disagree’) to 5 (‘Strongly agree’) so we can take averages of them later. Similarly, variable A009 should be recoded as 1 = ‘Very Poor’, 2 = ‘Poor’, 3 = ‘Fair’, 4 = ‘Good’, 5 = ‘Very Good’. (Hint: You may find it helpful to do the find and replace in ascending/descending order i.e. recode ’Strongly Disagree’/‘Very Poor’ as 1, then ‘Disagree’/‘Poor’ as 2, and so on.) We would like to split X025A into two variables, one for the number before the colon, and the other containing the words after the colon. Use Excel’s LEFT and/or RIGHT functions to create two new variables accordingly. (See Excel walk-through 8.1 for help on how to do this). Excel walk-through 8.1 Cleaning data and splitting variables Follow the walk-through in the CORE video, or in Figure 8.1, to find out how to clean data and split variables in Excel. How to clean data and split variables <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to clean data using find and replace, and split variables using Excel’s LEFT and RIGHT functions. '' /> How to clean data using find and replace, and split variables using Excel’s LEFT and RIGHT functions. Figure 8.1 How to clean data using find and replace, and split variables using Excel’s LEFT and RIGHT functions. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the data looks like. Currently some cells have entries ‘.a’ where the data is missing. We will find and replace the variables A009 to X047D (not pictured here). '' /> The data This is what the data looks like. Currently some cells have entries ‘.a’ where the data is missing. We will find and replace the variables A009 to X047D (not pictured here). Figure 8.1a This is what the data looks like. Currently some cells have entries ‘.a’ where the data is missing. We will find and replace the variables A009 to X047D (not pictured here). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Find and replace specific cell entries : ‘Find and Replace’ in Excel is similar to this feature in Microsoft Word, except that it searches for text in individual cells rather than in lines of text. You can adapt steps 1–3 to change specific values of cells in a column, for example replacing a particular word with a particular number. '' /> Find and replace specific cell entries ‘Find and Replace’ in Excel is similar to this feature in Microsoft Word, except that it searches for text in individual cells rather than in lines of text. You can adapt steps 1–3 to change specific values of cells in a column, for example replacing a particular word with a particular number. Figure 8.1b ‘Find and Replace’ in Excel is similar to this feature in Microsoft Word, except that it searches for text in individual cells rather than in lines of text. You can adapt steps 1–3 to change specific values of cells in a column, for example replacing a particular word with a particular number. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Extract a specific part of a string of text : Excel’s LEFT and RIGHT functions can help you extract parts of a long string of text, which is useful if you want to shorten variables or only keep particular information in a cell. Here, we are going to save variable X025A as two variables: ‘Education_1’ will contain the numbers before the colon, and ‘Education_2’ will contain the words after the colon. '' /> Extract a specific part of a string of text Excel’s LEFT and RIGHT functions can help you extract parts of a long string of text, which is useful if you want to shorten variables or only keep particular information in a cell. Here, we are going to save variable X025A as two variables: ‘Education_1’ will contain the numbers before the colon, and ‘Education_2’ will contain the words after the colon. Figure 8.1c Excel’s LEFT and RIGHT functions can help you extract parts of a long string of text, which is useful if you want to shorten variables or only keep particular information in a cell. Here, we are going to save variable X025A as two variables: ‘Education_1’ will contain the numbers before the colon, and ‘Education_2’ will contain the words after the colon. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-01-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Extract a specific part of a string of text : The RIGHT function counts the specified number of characters starting from the right end of the string of text. Since words have different lengths, we use LEN() to calculate the length of the string, then subtract the colon’s position in the string (using the FIND function). For example, if the string is 10 characters long, and the colon is the third character, we want Excel to extract characters 4 to 10 (the first 6 characters from the right). '' /> Extract a specific part of a string of text The RIGHT function counts the specified number of characters starting from the right end of the string of text. Since words have different lengths, we use LEN() to calculate the length of the string, then subtract the colon’s position in the string (using the FIND function). For example, if the string is 10 characters long, and the colon is the third character, we want Excel to extract characters 4 to 10 (the first 6 characters from the right). Figure 8.1d The RIGHT function counts the specified number of characters starting from the right end of the string of text. Since words have different lengths, we use LEN() to calculate the length of the string, then subtract the colon’s position in the string (using the FIND function). For example, if the string is 10 characters long, and the colon is the third character, we want Excel to extract characters 4 to 10 (the first 6 characters from the right). Although the paper we are following only considered individuals aged 25–80 who were not students, we will retain the other observations in our analysis. However, we need to remove any observations that have missing values for A170, X028, or any of the other variables except X047D. Removing missing data ensures that any summary statistics or analysis we do is done on the same set of data (without having to always account for the fact that some values are missing), and is fine as long as the data are missing at random (i.e. there is no particular reason why certain observations are missing and not others). In your spreadsheet, remove all rows in all waves that have missing data for A170. Do the same for: X003, X028, X007 and X001 in all waves A009 in Waves 1, 2, and 4 only C036, C037, C038, C039, C041 and X047D in Waves 3 and 4 only X011_01 and X025A, in Wave 4. Excel walk-through 8.2 gives guidance on how to do this. (Note: Excel may take some time to process the commands.) Excel walk-through 8.2 Dropping observations that satisfy particular conditions <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to drop observations that satisfy particular conditions using ‘filter’, ‘select’ and ‘delete’. '' /> How to drop observations that satisfy particular conditions using ‘filter’, ‘select’ and ‘delete’. Figure 8.2 How to drop observations that satisfy particular conditions using ‘filter’, ‘select’ and ‘delete’. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data. : This is what the data looks like. In this example, we will remove any rows with the variable A170 recorded as missing (blank). First, we will filter the data so that only rows with blank cells in this column are showing. '' /> The data. This is what the data looks like. In this example, we will remove any rows with the variable A170 recorded as missing (blank). First, we will filter the data so that only rows with blank cells in this column are showing. Figure 8.2a This is what the data looks like. In this example, we will remove any rows with the variable A170 recorded as missing (blank). First, we will filter the data so that only rows with blank cells in this column are showing. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the rows of interest : After step 3, you will see all the data that needs to be deleted. '' /> Filter the rows of interest After step 3, you will see all the data that needs to be deleted. Figure 8.2b After step 3, you will see all the data that needs to be deleted. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Delete these rows from the dataset : Excel will remove all the selected rows from the dataset. If there are many rows to delete, this command may take some time to process. '' /> Delete these rows from the dataset Excel will remove all the selected rows from the dataset. If there are many rows to delete, this command may take some time to process. Figure 8.2c Excel will remove all the selected rows from the dataset. If there are many rows to delete, this command may take some time to process. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Clear the filter to see the rest of the data : After removing the specified rows, clear the filter to see the rest of your data. You will see that the number of rows has decreased. '' /> Clear the filter to see the rest of the data After removing the specified rows, clear the filter to see the rest of your data. You will see that the number of rows has decreased. Figure 8.2d After removing the specified rows, clear the filter to see the rest of your data. You will see that the number of rows has decreased. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-02-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The modified data : This is what the data looks like after removing rows with missing entries in A170. You can adapt steps 1–7 to remove rows that satisfy other conditions, such as containing a particular value or range of values. '' /> The modified data This is what the data looks like after removing rows with missing entries in A170. You can adapt steps 1–7 to remove rows that satisfy other conditions, such as containing a particular value or range of values. Figure 8.2e This is what the data looks like after removing rows with missing entries in A170. You can adapt steps 1–7 to remove rows that satisfy other conditions, such as containing a particular value or range of values. We will now create two variables, work ethic and relative income, which we will use in our comparison of life satisfaction. Work ethic is measured as the average of C036 to C041. Using Excel’s AVERAGE function, create a new variable showing this work ethic measure. Since unemployed individuals and students may not have income, the study calculated relative income as the relative household income of that individual, measured as a deviation from the average income in that individual’s country. Explain the issues with using this method if the income distribution is skewed (for example, a long right tail). Instead of using average income, we will define relative income as the percentile of that individual’s household in the income distribution. Create a new variable showing this information. (See Excel walk-through 8.3 for one way to do this). Excel walk-through 8.3 Calculating percentiles from actual values <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to calculate percentiles from actual values. '' /> How to calculate percentiles from actual values. Figure 8.3 How to calculate percentiles from actual values. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the data looks like. In this example, we will calculate the percentile of X047D (log income), according to the country (S003). You will need to use two new columns (R and S in this case). '' /> The data This is what the data looks like. In this example, we will calculate the percentile of X047D (log income), according to the country (S003). You will need to use two new columns (R and S in this case). Figure 8.3a This is what the data looks like. In this example, we will calculate the percentile of X047D (log income), according to the country (S003). You will need to use two new columns (R and S in this case). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the rank of each observation : The intuition behind our rank calculation is that an observation’s rank is the number of observations larger than it, plus 1. (For example, if there were only two numbers, the smaller number should be ranked 2 and the larger number ranked 1). By default, Excel will rank observations from largest to smallest (higher numbers will have lower ranks). We need this setting to calculate percentiles. In this example, we need to rank observations only within the country it belongs to, not the entire dataset, so we use this as a condition in COUNTIFS. '' /> Calculate the rank of each observation The intuition behind our rank calculation is that an observation’s rank is the number of observations larger than it, plus 1. (For example, if there were only two numbers, the smaller number should be ranked 2 and the larger number ranked 1). By default, Excel will rank observations from largest to smallest (higher numbers will have lower ranks). We need this setting to calculate percentiles. In this example, we need to rank observations only within the country it belongs to, not the entire dataset, so we use this as a condition in COUNTIFS. Figure 8.3b The intuition behind our rank calculation is that an observation’s rank is the number of observations larger than it, plus 1. (For example, if there were only two numbers, the smaller number should be ranked 2 and the larger number ranked 1). By default, Excel will rank observations from largest to smallest (higher numbers will have lower ranks). We need this setting to calculate percentiles. In this example, we need to rank observations only within the country it belongs to, not the entire dataset, so we use this as a condition in COUNTIFS. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-03-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use the calculated rank to calculate the percentile. : The intuition behind the percentile calculation is that an observation’s percentile is the percentage of observations smaller than it (the +1 in the formula is needed to exclude the observation itself). The 1– at the start of the formula is because we ranked observations from largest to smallest (so we need to ‘reverse’ all the numbers). Here we have multiplied by 100 so the numbers correspond to actual percentiles (e.g. 92 percentile). You can see that higher incomes have lower ranks and higher percentiles, indicating that we have done this correctly. Note: This example was based on observations with non-missing income only (not the fully-cleaned dataset), so the numbers you get will be slightly different. '' /> Use the calculated rank to calculate the percentile. The intuition behind the percentile calculation is that an observation’s percentile is the percentage of observations smaller than it (the +1 in the formula is needed to exclude the observation itself). The 1– at the start of the formula is because we ranked observations from largest to smallest (so we need to ‘reverse’ all the numbers). Here we have multiplied by 100 so the numbers correspond to actual percentiles (e.g. 92 percentile). You can see that higher incomes have lower ranks and higher percentiles, indicating that we have done this correctly. Note: This example was based on observations with non-missing income only (not the fully-cleaned dataset), so the numbers you get will be slightly different. Figure 8.3c The intuition behind the percentile calculation is that an observation’s percentile is the percentage of observations smaller than it (the +1 in the formula is needed to exclude the observation itself). The 1– at the start of the formula is because we ranked observations from largest to smallest (so we need to ‘reverse’ all the numbers). Here we have multiplied by 100 so the numbers correspond to actual percentiles (e.g. 92 percentile). You can see that higher incomes have lower ranks and higher percentiles, indicating that we have done this correctly. Note: This example was based on observations with non-missing income only (not the fully-cleaned dataset), so the numbers you get will be slightly different. Now we have all the variables we need in the format we need. We will make some tables to summarize the data. Using the data for Wave 4: Hint: For help on creating this table, see Excel walk-through 3.1. For help on changing the values in the table to percentages of the row total, follow steps 18–19 in Excel walk-through 3.2, but instead of selecting ‘Average’ (as in step 19), select ‘Show Values As’ (or ‘Show data as’) then ‘% of Row Total’. Create a table showing the breakdown of each country’s population according to employment status, with country (S003) as the row variable, and employment status (X028) as the column variable. Express the values as percentages of the row total rather than frequencies. Discuss any differences or similarities between countries that you find interesting. Create a new table as shown in Figure 8.4 (similar to Table 1 in the study ‘Employment status and subjective well-being’) and fill in the missing values. Reported life satisfaction is measured by A170. Summary tables such as these give a useful overview of what each variable looks like. Male Female Mean Standard deviation Mean Standard deviation Life satisfaction Self-reported health Work ethic Age Education Number of children Summary statistics by gender, European Values Study. Figure 8.4 Summary statistics by gender, European Values Study. Part 8.2 Visualizing the data Note You will need to have done Questions 1–5 in Part 8.1 before doing this part. Learning objectives for this part use column charts, line charts, and scatterplots to visualize data calculate and interpret correlation coefficients. We will now create some summary charts of the self-reported measures (work ethic and life satisfaction), starting with column charts to show the distributions of values. Along with employment status, these are the main variables of interest, so it is important to look at them carefully before doing further data analysis. The distribution of work ethic and life satisfaction may vary across countries but may also change over time within a country, especially since the surveys are conducted around once a decade. To compare distributions for a particular country over time, we have to use the same horizontal axis, so we will first need to make a separate frequency table for each distribution of interest (survey wave). Also, since the number of people surveyed in each wave may differ, we will use percentages instead of frequencies as the vertical axis variable. Using the data from Wave 3 and Wave 4 only, for three countries of your choice: Using the work ethic measure, create a separate frequency table for Wave 3 and for Wave 4, similar to Figure 8.5. The column ‘Percentage of individuals (%)’ refers to the number of individuals in that wave. The values in the first column should range from 1 to 5, in intervals of 0.2. (Hint: To count observations only for a particular country, you will need to use the IF function and FREQUENCY function together. See Excel walk-through 6.1 for help on how to do this). Range of work ethic score Frequency Percentage of individuals (%) 1.00 1.20 … 4.80 5.00 Frequency table for self-reported work ethic. Figure 8.5 Frequency table for self-reported work ethic. Plot a separate column chart for each country to show the distribution of work ethic scores in Wave 3, with the percentage of individuals on the vertical axis and the range of work ethic score on the horizontal axis. On each chart, plot the distribution of scores in Wave 4 on top of the Wave 3 distribution. (See Excel walk-through 6.2 in Empirical Project 6 for guidance on what your charts should look like). Based on your charts from 1(b), does it appear that attitudes towards work in each country of your choice have changed over time? (Hint: For example, look at where the distribution is centred, the percentages of observations on the left tail or the right tail of the distribution, and how spread out the data is.) We will use line charts to make a similar comparison for life satisfaction (A170) over time. Using data for countries that are present in Waves 1 to 4: Create a table showing the average life satisfaction, by wave (column variable) and by country (row variable). (Hint: You may find it easier to make four separate pivot tables and copy-paste the results into a new table). Plot a line chart with wave number (1 to 4) on the horizontal axis and average life satisfaction on the vertical axis. Make sure to include a legend. From your results in 2(a) and (b), how has the distribution of life satisfaction changed over time? What other information about the distribution of life satisfaction could we use to supplement these results? Choose one or two countries and research events that could explain the observed changes in average life satisfaction over time shown in 2(a) and (b). correlation coefficientA numerical measure, ranging between 1 and −1, of how closely associated two variables are—whether they tend to rise and fall together, or move in opposite directions. A positive coefficient indicates that when one variable takes a high (low) value, the other tends to be high (low) too, and a negative coefficient indicates that when one variable is high the other is likely to be low. A value of 1 or −1 indicates that knowing the value of one of the variables would allow you to perfectly predict the value of the other. A value of 0 indicates that knowing one of the variables provides no information about the value of the other. After describing patterns in our main variables over time, we will use correlation coefficients and scatterplots to look at the relationship between these variables and the other variables in our dataset. Using the Wave 4 data: Create a table as shown in Figure 8.6 and calculate the required correlation coefficients. For employment status and gender, you will need to create new variables: full-time employment should be equal to 1 if full-time employed and 0 if unemployed, and treated as missing data (left as a blank cell) otherwise. Gender should be 0 if male and 1 if female. Variable Life satisfaction Work ethic Age Education Full-time employment Gender Self-reported health Income Number of children Relative income Life satisfaction 1 Work ethic 1 Correlation between life satisfaction, work ethic and other variables, Wave 4. Figure 8.6 Correlation between life satisfaction, work ethic and other variables, Wave 4. Interpret the coefficients, paying close attention to how the variables are coded. (For example, you could comment on the absolute magnitude and sign of the coefficients). Explain whether the relationships implied by the coefficients are what you expected (for example, would you expect life satisfaction to increase or decrease with health, income, etc.) Next, we will look at the relationship between employment status and life satisfaction and investigate the paper’s hypothesis that this relationship varies with the average work ethic in a country. Using the data from Wave 4, carry out the following: Create a table showing the average life satisfaction according to employment status (showing the full-time employed, retired, and unemployed categories only), with country (S003) as the row variable, and employment status (X028) as the column variable. Comment on any differences in average life satisfaction between these three groups, and whether social norms is a plausible explanation for these differences. Use the table from 4(a) to calculate the difference in average life satisfaction (full-time employed minus unemployed, and full-time employed minus retired). Make a separate scatterplot for each of these differences in life satisfaction, with average work ethic on the horizontal axis and difference in life satisfaction on the vertical axis. For each difference (employed vs unemployed, employed vs retired), calculate and interpret the correlation coefficient between average work ethic and difference in life satisfaction. So far we have described the data using tables and charts, but have not made any statements about whether what we observe is consistent with an assumption that work ethic has no bearing on differences in life satisfaction between different subgroups. In the next part, we will assess the relationship between employment status and life satisfaction and assess whether the observed data lead us to reject the above assumption. Part 8.3 Confidence intervals for difference in the mean Note You will need to have done Questions 1–5 in Part 8.1 before doing this part. Part 8.2 is not necessary but is helpful to get an idea of what the data looks like. Learning objectives for this part calculate and interpret confidence intervals for the difference in means between two groups. The aim of this project was to look at the empirical relationship between employment and life satisfaction. When we calculate differences between groups, we collect evidence which may or may not support a hypothesis that life satisfaction is identical between different subgroups. Economists often call this testing for statistical significance. In Part 6.2 of Empirical Project 6, we constructed 95% confidence intervals for means, and used a rule of thumb to decide whether we could maintain the assumption that the two subgroups were really identical. Now we will learn how to construct confidence intervals for the difference in two means, which allows us to make such a judgment on the basis of a single confidence interval. Remember that the width of a confidence interval depends on the standard deviation and number of observations. (Read Part 6.2 of Empirical Project 6 to understand why.) When making a confidence interval for a sample mean (such as the mean life satisfaction of the unemployed), we use the standard deviation and number of observations in that sample (unemployed people) to obtain the standard error of the sample mean. Standard error of sample mean (SE)=SD of sample data√number of observations (n)text{Standard error of sample mean (SE)} = frac{text{SD of sample data}}{sqrt{text{number of observations (n)}}} When we look at the difference in means (such as life satisfaction of employed minus unemployed), we are using data from two groups (the unemployed and the employed) to make one confidence interval, so the number of observations is the sum of observations across both groups. To calculate the standard deviation for the difference in means, we use the standard deviations (SD) of each group: SD (difference in means)=√(SD of group 1)2+(SD of group 2)2text{SD (difference in means)} = sqrt{(text{SD of group 1})^2 + (text{SD of group 2})^2} This formula requires the two groups of data to be independent, meaning that the data from one group is not related, paired, or matched with data from the other group. This assumption is reasonable for the life satisfaction data we are using. However, if the two groups of data are not independent, for example if the same people generated both groups of data (as in the public goods experiments of Empirical Project 2), then we cannot use this formula. Once we have the new standard deviation and number of observations, we can calculate the width of the confidence interval (distance from the mean to one end of the interval) as before. The width of the confidence interval tells us how precisely the difference in means was estimated, and this precision tells us how confident we can be that the difference is not due to chance. If the value 0 falls outside the 95% confidence interval, this is strong evidence of a real difference in means. For example, if the estimated difference is positive, and the 95% confidence interval is not wide enough to include 0, we can say with 95% confidence that the true difference is positive too. In other words, it tells us that the p-value for the difference we have found is less than 0.05, so we would be very unlikely to find such a big difference by chance. In Figure 8.7, for Great Britain, we can say with 95% confidence that the true difference in average life satisfaction between the full-time employed and the unemployed is positive. However, for Spain, we do not have strong evidence of a real difference in mean life satisfaction. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''95% confidence intervals for the average difference in life satisfaction (full-time employed minus unemployed) in Great Britain and Spain. '' /> 95% confidence intervals for the average difference in life satisfaction (full-time employed minus unemployed) in Great Britain and Spain. Figure 8.7 95% confidence intervals for the average difference in life satisfaction (full-time employed minus unemployed) in Great Britain and Spain. We will apply this method to make confidence intervals for differences in life satisfaction. Choose three countries: one with an average work ethic in the top third of scores, one in the middle third, and one in the lower third. (See Excel walk-through 6.4 for help on calculating confidence intervals and adding them to a chart.) Create a pivot table for these countries, showing the average life satisfaction score, standard deviation of life satisfaction, and number of observations, with country (S003) as the row variable, and employment status (full-time employed, retired, and unemployed only) as the column variable. Use your pivot table from 1(a) to calculate the difference in means (full-time employed minus unemployed, and full-time employed minus retired), the standard deviation of these differences, and the number of observations. Use Excel’s CONFIDENCE.T function and the calculated values in 1(b) to determine the 95% confidence interval width (distance between the mean and one end of the interval) of the difference in means. Plot a column chart for your chosen countries showing the difference in life satisfaction (employed vs unemployed and employed vs retired) on the vertical axis, and country on the horizontal axis (sorted according to low, medium, and high work ethic). Add the confidence intervals from 1(c) to your chart. Interpret your findings from Question 1(d), commenting on the size of the observed differences in means, and the precision of your estimates. The method we used to compare life satisfaction relied on making comparisons between people with different employment statuses, but a person’s employment status is not entirely random. We cannot therefore make causal statements such as ‘being unemployed causes life satisfaction to decrease’. Describe how we could use the following methods to assess better the effect of being unemployed on life satisfaction, and make some statements about causality: a natural experiment panel data (data on the same individuals, taken at different points in time)."
});
index.addDoc({
    id: 44,
    title: "Doing Economics: Empirical Project 8: Working in R",
    content: "Empirical Project 8 Working in R Download the code To download the code chunks used in this project, right-click on the download link and select ‘Save Link As…’. You’ll need to save the code download to your working directory, and open it in RStudio. Don’t forget to also download the data into your working directory by following the steps in this project. Getting started in R For this project you will need the following packages: tidyverse, to help with data manipulation readxl, to import an Excel spreadsheet knitr, to format tables. If you need to install any of these packages, run the following code: install.packages(c(''tidyverse'', ''readxl'', ''knitr'')) You can import the libraries now, or when they are used in the R walk-throughs below. library(tidyverse) library(readxl) library(knitr) Part 8.1 Cleaning and summarizing the data Learning objectives for this part detect and correct entries in a dataset recode variables to make them easier to analyse calculate percentiles for subsets of the data. So far we have been working with data that was formatted correctly. However, sometimes the datasets will be messier and we will need to clean the data before starting any analysis. Data cleaning involves checking that all variables are formatted correctly, all observations are entered correctly (e.g. no typos), and missing values are coded in a way that the software you are using can understand. Otherwise, the software will either not be able to analyse your data, or only analyse the observations it recognizes, which could lead to incorrect results and conclusions. In the data we will use, the European Values Study (EVS) data has been converted to an Excel spreadsheet from another program, so there are some entries that were not converted correctly and some variables that need to be recoded (for example, replacing words with numbers, or replacing one number with another). Download the EVS data and documentation: Download the EVS data. For the documentation, go to the data download site. Click on the ‘Data and Documents’ button in the middle of the page, then click the ‘Other Documents’ button and download the PDF file called ‘ZA4804_EVS_VariableCorrespondence.pdf’. This file contains information about each variable in the dataset (e.g. variable name and what it measures). Although we will be performing our analysis in R, the data has been provided as an Excel spreadsheet, so look at the spreadsheet in Excel first to understand its structure. The Excel spreadsheet contains multiple worksheets, one for each wave, that need to be joined together to create a single ‘tibble’ (like a spreadsheet for R). The variable names currently do not tell us what the variable is, so we need to provide labels and short descriptions to avoid confusion. Using the rbind function, load each worksheet containing data (‘Wave 1’ through to ‘Wave 4’) and combine the worksheets into a single dataset. Use the PDF you have downloaded to create a label and a short description (attribute) for each variable using the attr function. (See R walk-through 8.1 for details.) The second column of the PDF lists all variables in the dataset (alphabetically) and tells you what it measures. R walk-through 8.1 Importing the data into R As we are importing an Excel file, we use the read_excel function from the readxl package. The file is called Project-8-datafile.xlsx and contains four worksheets that contain the data, named ‘Wave 1’ through to ‘Wave 4’. We will load the worksheets one by one and add them to the previous worksheets using the rbind function, which attaches objects with the same number of rows to each other. The final output is called lifesat_data. library(tidyverse) library(readxl) library(knitr) # Set your working directory to the correct folder. # Insert your file path for 'YOURFILEPATH'. setwd(''YOURFILEPATH'') lifesat_data <- read_excel(''Project-8-datafile.xlsx'', sheet = ''Wave 1'') lifesat_data <- rbind(lifesat_data, read_excel(''Project-8-datafile.xlsx'', sheet = ''Wave 2'')) lifesat_data <- rbind(lifesat_data, read_excel(''Project-8-datafile.xlsx'', sheet = ''Wave 3'')) lifesat_data <- rbind(lifesat_data, read_excel(''Project-8-datafile.xlsx'', sheet = ''Wave 4'')) The variable names provided in the spreadsheet are not very specific (a combination of letters and numbers that don’t tell us what the variable measures), we can assign a label (”labels”) and a short description (shortDescription, taken from the Variable Correspondence file) to each variable for reference later. The attr function does this for us (note that the order in each list should be the same as the order of the variable names). attr(lifesat_data, ''labels'') <- c(''EVS-wave'', ''Country/region'', ''Respondent number'', ''Health'', ''Life satisfaction'', ''Work Q1'', ''Work Q2'', ''Work Q3'', ''Work Q4'', ''Work Q5'', ''Sex'', ''Age'', ''Marital status'', ''Number of children'', ''Education'', ''Employment'', ''Monthly household income'') attr(lifesat_data, ''shortDescription'') <- c(''EVS-wave'', ''Country/region'', ''Original respondent number'', ''State of health (subjective)'', ''Satisfaction with your life'', ''To develop talents you need to have a job'', ''Humiliating to receive money w/o working for it'', ''People who don't work become lazy'', ''Work is a duty towards society'', ''Work comes first even if it means less spare time'', ''Sex'', ''Age'', ''Marital status'', ''How many living children do you have'', ''Educational level (ISCED-code one digit)'', ''Employment status'', ''Monthly household income (× 1,000s PPP euros)'') Throughout this project we will refer to the variables using their original names, but you can use the attr function again to look up the attributes of each variable (the following code uses the variable X028 to show how you can do this): attr(lifesat_data, ''labels'')[ attr(lifesat_data, ''names'') == ''X028''] ## [1] ''Employment'' attr(lifesat_data, ''shortDescription'')[ attr(lifesat_data, ''names'') == ''X028''] ## [1] ''Employment status'' In general, data dictionaries and variable correspondences are useful because they contain important information about what each variable represents and how it is measured, which usually cannot be summarized in a short variable name. Now we will take a closer look at how some of the variables were measured. Variable A170 is reported life satisfaction on a scale of 1 (dissatisfied) to 10 (satisfied). Respondents answered the question ‘All things considered, how satisfied are you with your life as a whole these days?’ Discuss the assumptions needed to use this measure in interpersonal and cross-country comparisons, and whether you think they are plausible. (You may find it helpful to refer to Box 2.1 of the OECD Guidelines on Measuring Subjective Well-being.) An individual’s employment status (variable X028) was self-reported. Explain whether misreporting of employment status is likely to be an issue, and give some factors that may affect the likelihood of misreporting in this context. Variables C036 to C041 ask about an individual’s attitudes towards work. With self-reports, we may also be concerned that individuals are using a heuristic (rule of thumb) to answer the questions. Table 2.1 of the OECD Guidelines on Measuring Subjective Well-being lists some response biases and heuristics that individuals could use. Pick three that you think particularly apply to questions about life satisfaction or work ethic and describe how we might check whether this issue may be present in our data. Before doing any data analysis, it is important to check that all variables are coded in a way that the software can recognize. This process involves checking how missing values are coded (usually these need to be coded in a particular way for each software), and that numerical variables (numbers) only contain numbers and not text (in order to calculate summary statistics). We will now check and clean the dataset so it is ready to use. Inspect the data and understand what type of data each variable is stored as. Currently, missing values are recorded as ‘.a’, but we would like them to be recorded as ‘NA’ (not available). Variable A170 (life satisfaction) is currently a mixture of numbers (2 to 9) and words (‘Satisfied’ and ‘Dissatisfied’), but we would like it to be all numbers. Replace the word ‘Dissatisfied’ with the number 1, and the word ‘Satisfied’ with the number 10. Similarly, variable X011_01 (number of children) has recorded no children as a word rather than a number. Replace ‘No children’ with the number 0. The variables C036 to C041 should be replaced with numbers ranging from 1 (‘Strongly disagree’) to 5 (‘Strongly agree’) so we can take averages of them later. Similarly, variable A009 should be recoded as 1 = ‘Very Poor’, 2 = ‘Poor’, 3 = ‘Fair’, 4 = ‘Good’, 5 = ‘Very Good’. Split X025A into two variables, one for the number before the colon, and the other containing the words after the colon. R walk-through 8.2 Cleaning data and splitting variables Inspect the data and recode missing values R stores variables as different types depending on the kind of information the variable represents. For categorical data, where, as the name suggests, data is divided into a number of groups, such as country or occupation, the variables can be stored as text (chr). Numerical data (numbers that do not represent categories) should be stored as numbers (num), but sometimes R imports numerical values as text, which can cause problems later when we use these variables in calculations. The str function shows what type each variable is being stored as. str(lifesat_data) ## Classes 'tbl_df', 'tbl' and 'data.frame': 164997 obs. of 17 variables: ## $ S002EVS: chr ''1981-1984'' ''1981-1984'' ''1981-1984'' ''1981-1984'' ... ## $ S003 : chr ''Belgium'' ''Belgium'' ''Belgium'' ''Belgium'' ... ## $ S006 : num 1001 1002 1003 1004 1005 ... ## $ A009 : chr ''Fair'' ''Very good'' ''Poor'' ''Very good'' ... ## $ A170 : chr ''9'' ''9'' ''3'' ''9'' ... ## $ C036 : chr ''.a'' ''.a'' ''.a'' ''.a'' ... ## $ C037 : chr ''.a'' ''.a'' ''.a'' ''.a'' ... ## $ C038 : chr ''.a'' ''.a'' ''.a'' ''.a'' ... ## $ C039 : chr ''.a'' ''.a'' ''.a'' ''.a'' ... ## $ C041 : chr ''.a'' ''.a'' ''.a'' ''.a'' ... ## $ X001 : chr ''Male'' ''Male'' ''Male'' ''Female'' ... ## $ X003 : num 53 30 61 60 60 19 38 39 44 76 ... ## $ X007 : chr ''Single/Never married'' ''Married'' ''Separated'' ''Married'' ... ## $ X011_01: chr ''.a'' ''.a'' ''.a'' ''.a'' ... ## $ X025A : chr ''.a'' ''.a'' ''.a'' ''.a'' ... ## $ X028 : chr ''Full time'' ''Full time'' ''Unemployed'' ''Housewife'' ... ## $ X047D : chr ''.a'' ''.a'' ''.a'' ''.a'' ... ## - attr(*, ''labels'')= chr ''EVS-wave'' ''Country/region'' ''Respondent number'' ''Health'' ... ## - attr(*, ''shortDescription'')= chr ''EVS-wave'' ''Country/region'' ''Original respondent number'' ''State of health (subjective)'' ... You can see that some variables have the value “.a”, which is how missing values were coded in the original dataset. We start by recoding all missing values as NA (which is how R recognizes and usually codes missing values). lifesat_data[lifesat_data == ''.a''] <- NA Recode the life satisfaction variable To recode the life satisfaction variable (A170), we can use logical indexing (asking R to check if a certain condition is satisfied or not) to select all observations where the variable A170 is equal to either ‘Dissatisfied’ or ‘Satisfied’ and assign the value 1 or 10 respectively. This variable was imported as a chr (text) instead of num variable because some of the entries included text (the .a for missing values). After changing the text into numerical values, we use the as.numeric function to convert the variable into a numerical type. lifesat_data$A170[lifesat_data$A170 == ''Dissatisfied''] <- 1 lifesat_data$A170[lifesat_data$A170 == ''Satisfied''] <- 10 lifesat_data$A170 <- as.numeric(lifesat_data$A170) Recode the variable for number of children We repeat this process (using logical indexing and then converting to numerical variable) for the variable indicating the number of children (X011_01). lifesat_data$X011_01[lifesat_data$X011_01 == ''No children''] <- 0 lifesat_data$X011_01 <- as.numeric(lifesat_data$X011_01) Replace text with numbers for multiple variables When we have to recode multiple variables with the same mapping of text to numerical value (i.e. a word/phrase corresponds to the same number for all variables), we can use the tidyverse library and the piping (%>%) operator to run the same sequence of steps in one go for all variables, instead of repeating the process used above for each variable. The punctuation %>% links the commands together. (At this stage, don’t worry if you can’t understand exactly what this code does, as long as you can adapt it for similar situations. For a more detailed introduction to piping, see the University of Manchester’s Econometric Computing Learning Resource.) lifesat_data <- lifesat_data %>% mutate_at(c(''C036'', ''C037'', ''C038'', ''C039'', ''C041''), funs(recode(., ''Strongly disagree'' = ''1'', ''Disagree'' = ''2'', ''Neither agree nor disagree'' = ''3'', ''Agree'' = ''4'', ''Strongly agree'' = ''5''))) %>% mutate_at(c(''A009''), funs(recode(., ''Very poor'' = ''1'', ''Poor'' = ''2'', ''Fair'' = ''3'', ''Good'' = ''4'', ''Very good'' = ''5''))) %>% mutate_at(c(''A009'', ''C036'', ''C037'', ''C038'', ''C039'', ''C041'', ''X047D''), funs(as.numeric)) Split a variable containing numbers and text To split the education variable (X025A) into two new columns, we use the separate function, which creates two new variables called Education_1 and Education_2 containing the numeric value and the text description respectively. Then we use the mutate_at function to convert Education_1 into a numeric variable. lifesat_data <- lifesat_data %>% separate(X025A, c(''Education_1'', ''Education_2''), '' : '', remove = FALSE) %>% mutate_at(''Education_1'', funs(as.numeric)) Although the paper we are following only considered individuals aged 25–80 who were not students, we will retain the other observations in our analysis. However, we need to remove any observations that have missing values for A170, X028, or any of the other variables except X047D. Removing missing data ensures that any summary statistics or analysis we do is done on the same set of data (without having to always account for the fact that some values are missing), and is fine as long as the data are missing at random (i.e. there is no particular reason why certain observations are missing and not others). In your dataset, remove all rows in all waves that have missing data for A170. Do the same for: X003, X028, X007 and X001 in all waves A009 in Waves 1, 2, and 4 only C036, C037, C038, C039, C041 and X047D in Waves 3 and 4 only X011_01 and X025A, in Wave 4. R walk-through 8.3 gives guidance on how to do this. R walk-through 8.3 Dropping specific observations As not all questions were asked in all waves, we have to be careful when dropping observations with missing values for certain questions, to avoid accidentally dropping an entire wave of data. For example, information on self-reported health (A009) was not recorded in Wave 3, and questions on work attitudes (C036 to C041) and information on household income are only asked in Waves 3 and 4. Furthermore, information on the number of children (X011_01) and education (X025A) are only collected in the final wave. We will first use the complete.cases function to keep observations with complete information on variables present in all waves (X003, A170, X028, X007, and X001—stored in the list include). This function tells us which rows in the dataframe have complete information. When used in the code lifesat_data[complete.cases(lifesat_data[, include]), ], it tells R to only keep the rows that contain complete information of the variables in the list include. include <- names(lifesat_data) %in% c( ''X003'', ''A170'', ''X028'', ''X007'', ''X001'') lifesat_data <- lifesat_data[ complete.cases(lifesat_data[, include]), ] Next we will look at variables that were only present in some waves. For each variable/group of variables, we have to only look at the particular wave(s) in which the question was asked, then keep the observations with complete information on those variables. As before, we make lists of variables that only feature in Waves 1, 2, and 4 (A009—stored in include_wave_1_2_4), Waves 3 and 4 (C036 to C041, X047D—stored in include_wave_3_4), and Wave 4 only (X011_01 and X025A—stored in include_wave_4). Again we use the complete.cases function, but combine it with the logical OR operator | to include all observations for waves that did not ask that question, along with the complete cases for that question in the other waves. For example, the first command keeps an observations if it is in Wave 1 (1981-1984) or Wave 2 (1990-1993), or is in Waves 3 or 4 with complete information. # A009 is not in Wave 3. include_wave_1_2_4 <- names(lifesat_data) %in% c(''A009'') include_wave_3_4 <- names(lifesat_data) %in% # Work attitudes and income are in Waves 3 and 4. c(''C036'', ''C037'', ''C038'', ''C039'', ''C041'', ''X047D'') # Number of children and education are in Wave 4. include_wave_4 <- names(lifesat_data) %in% c(''X011_01'', ''X025A'') lifesat_data <- lifesat_data %>% filter(S002EVS == ''1981-1984'' | S002EVS == ''1990-1993'' | complete.cases(lifesat_data[, include_wave_3_4])) lifesat_data <- lifesat_data %>% filter(S002EVS != ''2008-2010'' | complete.cases(lifesat_data[, include_wave_4])) lifesat_data <- lifesat_data %>% filter(S002EVS == ''1999-2001'' | complete.cases(lifesat_data[, include_wave_1_2_4])) We will now create two variables, work ethic and relative income, which we will use in our comparison of life satisfaction. Work ethic is measured as the average of C036 to C041. Create a new variable showing this work ethic measure. Since unemployed individuals and students may not have income, the study calculated relative income as the relative household income of that individual, measured as a deviation from the average income in that individual’s country. Explain the issues with using this method if the income distribution is skewed (for example, if it has a long right tail). Instead of using average income, we will define relative income as the percentile of that individual’s household in the income distribution. Create a new variable showing this information. (See R walk-through 8.4 for one way to do this). R walk-through 8.4 Calculating averages and percentiles Calculate average work ethic score We use the rowMeans function to calculate the average work ethic score for each observation (workethic) based on the five survey questions related to working attitudes (C036 to C041). lifesat_data$work_ethic <- rowMeans(lifesat_data[, c(''C036'', ''C037'', ''C038'', ''C039'', ''C041'')]) Calculate income percentiles for each individual R provides a handy function (ecdf) to obtain an individual’s relative income as a percentile. As we want a separate income distribution for each wave, we use the group_by function. Finally, we tidy up the results by converting each value into a percentage and rounding to a single decimal place. The ecdf function will not work if there is missing data. Since we don’t have income data for Waves 1 and 2, we will have to split the data when working out the percentiles. First we store all observations from Waves 3 and 4 separately in a temporary dataset (df.new), only keep observations that have income values (!is.na(X047D)), calculate the percentile values with the ecdf function, and save the values in a variable called percentile (which will be ‘NA’ for Waves 1 and 2). Then we recombine this data with the original observations from Waves 1 and 2. # Create the percentile variable for Waves 3 and 4 df.new <- lifesat_data %>% # Select Waves 3 and 4 subset(S002EVS %in% c(''1999-2001'', ''2008-2010'')) %>% # Drop observations with missing income data filter(!is.na(X047D)) %>% # Calculate the percentiles per wave group_by(S002EVS) %>% # Calculate relative income as a percentile mutate(., percentile = ecdf(X047D)(X047D)) %>% # Express the variable as a percentage mutate(., percentile = round(percentile * 100, 1)) # Recombine Waves 3 and 4 with the Waves 1 and 2 lifesat_data <- lifesat_data %>% # Select Waves 1 and 2 subset(S002EVS %in% c(''1981-1984'', ''1990-1993'')) %>% # Create an empty variable so old and new dataframes are # same size mutate(., percentile = NA) %>% # Recombine the data bind_rows(., df.new) Now we have all the variables we need in the format we need. We will make some tables to summarize the data. Using the data for Wave 4: Create a table showing the breakdown of each country’s population according to employment status, with country (S003) as the row variable, and employment status (X028) as the column variable. Express the values as percentages of the row total rather than as frequencies. Discuss any differences or similarities between countries that you find interesting. Create a new table as shown in Figure 8.1 (similar to Table 1 in the study ‘Employment status and subjective well-being’) and fill in the missing values. Life satisfaction is measured by A170. Summary tables such as these give a useful overview of what each variable looks like. Male Female Mean Standard deviation Mean Standard deviation Life satisfaction Self-reported health Work ethic Age Education Number of children Summary statistics by gender, European Values Study. Figure 8.1 Summary statistics by gender, European Values Study. R walk-through 8.5 Calculating summary statistics Create a table showing employment status, by country One of the most useful R functions to obtain summary information is summarize, which produces many different summary statistics for a set of data. First we select the data for Wave 4 and group it by country (S003) and employment type (X028). Then we use the summarize function to obtain the number of observations for each combination of country and employment type. With the data effectively grouped by country only (a single observation contains the number of observations for each level (category) of X028), we can use the sum function to obtain a total count (number of observations) per country and calculate relative frequencies (saved as freq). lifesat_data %>% # Select Wave 4 only subset(S002EVS == ''2008-2010'') %>% # Group by country and employment group_by(S003, X028) %>% # Count the number of observations by group summarize(n = n()) %>% # Calculate the relative frequency (percentages) mutate(freq = round(n / sum(n), 4) * 100) %>% # Drop the count variable select(-n) %>% # Reshape the data into a neat table spread(X028, freq) %>% kable(., format = ''markdown'') |S003 | Full time| Housewife| Other| Part time| Retired| Self employed| Students| Unemployed| |:------------------|---------:|---------:|-----:|---------:|-------:|-------------:|--------:|----------:| |Albania | 29.42| 7.42| 1.50| 5.50| 9.08| 22.08| 7.33| 17.67| |Armenia | 23.86| 20.92| 1.14| 8.09| 18.38| 5.96| 6.70| 14.95| |Austria | 39.80| 7.24| 1.89| 9.95| 25.49| 5.02| 8.39| 2.22| |Belarus | 57.88| 2.43| 1.21| 6.95| 18.59| 3.40| 6.87| 2.67| |Belgium | 42.89| 5.96| 3.72| 8.94| 23.01| 3.57| 5.21| 6.70| |Bosnia Herzegovina | 34.06| 9.33| 0.82| 2.90| 14.67| 3.08| 8.15| 26.99| |Bulgaria | 46.32| 2.62| 0.76| 2.79| 31.28| 5.58| 2.37| 8.28| |Croatia | 41.58| 3.37| 0.93| 2.78| 26.01| 2.86| 8.75| 13.72| |Cyprus | 46.32| 13.68| 1.29| 2.84| 24.39| 6.58| 1.68| 3.23| |Czech Republic | 46.56| 3.06| 4.66| 1.68| 31.27| 3.82| 5.43| 3.52| |Denmark | 55.89| 0.28| 1.32| 6.69| 24.32| 5.94| 4.15| 1.41| |Estonia | 50.35| 4.08| 2.20| 5.11| 28.52| 3.61| 3.38| 2.75| |Finland | 52.34| 1.38| 3.94| 5.11| 22.77| 6.17| 3.72| 4.57| |France | 46.83| 5.59| 1.94| 6.04| 28.78| 2.76| 3.13| 4.92| |Georgia | 19.46| 11.60| 0.81| 6.57| 19.38| 7.06| 2.60| 32.52| |Germany | 38.44| 4.58| 3.03| 8.44| 28.64| 2.97| 2.67| 11.23| |Great Britain | 33.50| 7.32| 4.01| 11.23| 28.99| 5.72| 1.40| 7.82| |Greece | 28.49| 17.42| 0.40| 2.97| 26.73| 13.72| 6.18| 4.09| |Hungary | 46.39| 1.20| 7.21| 2.00| 24.04| 3.53| 6.57| 9.05| |Iceland | 54.50| 2.25| 6.01| 9.91| 7.06| 11.41| 4.95| 3.90| |Ireland | 41.87| 19.84| 1.59| 9.72| 13.89| 4.96| 1.59| 6.55| |Italy | 32.88| 8.33| 0.46| 9.13| 23.06| 13.70| 7.08| 5.37| |Kosovo | 19.57| 11.65| 0.52| 5.23| 5.83| 9.41| 18.00| 29.80| |Latvia | 52.05| 6.18| 2.34| 3.76| 23.22| 3.26| 4.26| 4.93| |Lithuania | 50.13| 4.11| 2.89| 5.16| 23.97| 3.41| 6.04| 4.29| |Luxembourg | 51.33| 9.36| 1.12| 7.38| 15.36| 3.00| 9.87| 2.58| |Macedonia | 35.74| 4.34| 1.24| 1.71| 16.74| 3.72| 8.53| 27.98| |Malta | 33.84| 32.33| 0.68| 3.84| 23.42| 2.33| 0.55| 3.01| |Moldova | 30.49| 7.24| 1.87| 7.58| 25.64| 4.86| 4.43| 17.89| |Montenegro | 39.02| 4.63| 0.60| 2.14| 16.64| 4.97| 4.80| 27.19| |Netherlands | 32.40| 9.52| 3.68| 18.24| 27.76| 6.48| 0.80| 1.12| |Northern Cyprus | 31.19| 19.55| 2.23| 5.20| 8.91| 8.91| 13.61| 10.40| |Northern Ireland | 30.10| 10.36| 4.53| 8.74| 29.45| 3.56| 1.29| 11.97| |Norway | 53.23| 2.22| 6.55| 9.48| 12.60| 8.17| 7.06| 0.71| |Poland | 41.81| 6.00| 0.10| 3.14| 28.00| 5.81| 7.62| 7.52| |Portugal | 46.20| 5.24| 1.57| 3.27| 33.51| 1.70| 1.05| 7.46| |Romania | 41.07| 10.54| 1.95| 3.22| 33.95| 3.02| 3.61| 2.63| |Russian Federation | 54.36| 5.81| 2.72| 5.08| 23.77| 1.27| 2.72| 4.26| |Serbia | 34.21| 5.02| 1.07| 2.38| 25.16| 6.91| 4.11| 21.13| |Slovakia | 40.98| 1.73| 4.89| 2.21| 39.73| 3.55| 1.25| 5.66| |Slovenia | 47.44| 2.50| 2.75| 1.25| 31.59| 4.37| 6.99| 3.12| |Spain | 41.52| 16.30| 0.11| 4.63| 19.93| 6.28| 3.19| 8.04| |Sweden | 54.82| 0.38| 6.60| 7.36| 15.36| 7.23| 4.06| 4.19| |Switzerland | 48.50| 6.42| 3.21| 14.03| 21.31| 2.89| 1.39| 2.25| |Turkey | 16.42| 42.39| 0.60| 2.14| 10.00| 7.66| 5.92| 14.88| |Ukraine | 40.92| 6.79| 1.02| 4.84| 32.09| 4.41| 3.23| 6.71| Calculate summary statistics by gender We can also obtain summary statistics on a number of variables at the same time using the summarize_at function. To obtain the mean value for each of the required variables, grouped by the gender variable (X001), we can use the piping operator again: # Obtain mean value lifesat_data %>% # We only require Wave 4. subset(S002EVS == ''2008-2010'') %>% # Group by gender group_by(X001) %>% summarize_at(c(''A009'', ''A170'', ''work_ethic'', ''X003'', ''Education_1'', ''X011_01''), mean) ## # A tibble: 2 x 7 ## X001 A009 A170 work_ethic X003 Education_1 X011_01 ## <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> ## 1 Female 3.60 6.93 3.64 47.3 3.05 1.69 ## 2 Male 3.77 7.03 3.72 46.9 3.14 1.55 We can repeat the same steps to obtain the standard deviation, again grouped by gender. # Obtain standard deviation lifesat_data %>% subset(S002EVS == ''2008-2010'') %>% group_by(X001) %>% summarize_at(c(''A009'', ''A170'', ''work_ethic'', ''X003'', ''Education_1'', ''X011_01''), sd) ## # A tibble: 2 x 7 ## X001 A009 A170 work_ethic X003 Education_1 X011_01 ## <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> ## 1 Female 0.969 2.32 0.765 17.5 1.40 1.39 ## 2 Male 0.929 2.28 0.757 17.4 1.31 1.43 We can combine these two operations into a single code block and ask summarize_at to perform multiple functions on multiple variables at the same time. # Obtain mean and standard deviations lifesat_data %>% subset(S002EVS == ''2008-2010'') %>% group_by(X001) %>% summarise_at(c(''A009'', ''A170'', ''work_ethic'', ''X003'', ''Education_1'', ''X011_01''), funs(mean, sd)) ## # A tibble: 2 x 13 ## X001 A009_mean A170_mean work_ethic_mean X003_mean Education_1_mean ## <chr> <dbl> <dbl> <dbl> <dbl> <dbl> ## 1 Female 3.60 6.93 3.64 47.3 3.05 ## 2 Male 3.77 7.03 3.72 46.9 3.14 ## # ... with 7 more variables: X011_01_mean <dbl>, A009_sd <dbl>, ## # A170_sd <dbl>, work_ethic_sd <dbl>, X003_sd <dbl>, ## # Education_1_sd <dbl>, X011_01_sd <dbl> In this example, R automatically appends the function name (in this case, _mean or _sd) to the end of the variables. If you need these variables in subsequent calculations, you need to reformat the data and variable names. Part 8.2 Visualizing the data Note You will need to have done Questions 1–5 in Part 8.1 before doing this part. Learning objectives for this part use column charts, line charts, and scatterplots to visualize data calculate and interpret correlation coefficients. We will now create some summary charts of the self-reported measures (work ethic and life satisfaction), starting with column charts to show the distributions of values. Along with employment status, these are the main variables of interest, so it is important to look at them carefully before doing further data analysis. The distribution of work ethic and life satisfaction may vary across countries but may also change over time within a country, especially since the surveys are conducted around once a decade. To compare distributions for a particular country over time, we have to use the same horizontal axis, so we will first need to make a separate frequency table for each distribution of interest (survey wave). Also, since the number of people surveyed in each wave may differ, we will use percentages instead of frequencies as the vertical axis variable. Use the data from Wave 3 and Wave 4 only, for three countries of your choice: For each of your chosen countries create a frequency table that contains the frequency of each unique value of the work ethic scores. Also include the percentage of individuals at each value, grouped by Wave 3 and Wave 4 separately. Plot a separate column chart for each country to show the distribution of work ethic scores in Wave 3, with the percentage of individuals on the vertical axis and the range of work ethic scores on the horizontal axis. On each chart, plot the distribution of scores in Wave 4 on top of the Wave 3 distribution. Based on your charts from 1(b), does it appear that the attitudes towards work in each country of your choice have changed over time? (Hint: For example, look at where the distribution is centred, the percentages of observations on the left tail or the right tail of the distribution, and how spread out the data is.) R walk-through 8.6 Calculating frequencies and percentages First we need to create a frequency table of the work_ethic variable for each wave. This variable only takes values from 1.0 to 5.0 in increments of 0.2 (since it is an average of five whole numbers), so we can group by each value and count the number of observations in each group using the summarize function. Once we have counted the number of observations that have each value (separately for each wave), we compute the percentages by dividing these numbers by the total number of observations for that wave. For example, if there are 50 observations between 1 and 1.2, and 1,000 observations in that wave, the percentage would be 5%. waves <- c(''1999-2001'', ''2008-2010'') df.new <- lifesat_data %>% # Select Waves 3 and 4 subset(., S002EVS %in% waves) %>% # We use Germany in this example. subset(., S003 == ''Germany'') %>% # Group by each wave and the discrete values of # work_ethic group_by(S002EVS, work_ethic) %>% summarize (freq = n()) %>% mutate(percentage = (freq / sum(freq))*100) The frequencies and percentages are saved in a new dataframe called df.new. If you want to look at it, type view(df.new) into the console window. Now that we have the percentages and frequency data, we use ggplot to plot a column chart. To overlay the column charts for both waves and make sure that the plot for each wave is visible, we use the alpha option in the geom_bar function to set the transparency level (try changing the transparency to see how it affects your chart’s appearance). p <- ggplot(df.new, aes(work_ethic, percentage, fill = S002EVS, color = S002EVS)) + # Set the colour and fill of each wave geom_bar(stat = ''identity'', position = ''identity'', alpha = 0.2) + theme_bw() + ggtitle(paste(''Distribution of work ethic for Germany'')) print(p) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-02.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-02-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-02-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-02-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-02.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Distribution of work ethic scores for Germany. '' /> Distribution of work ethic scores for Germany. Figure 8.2 Distribution of work ethic scores for Germany. We will use line charts to make a similar comparison for life satisfaction (A170) over time. Using data for countries that are present in Waves 1 to 4: Create a table showing the average life satisfaction, by wave (column variable) and by country (row variable). Plot a line chart with wave number (1 to 4) on the horizontal axis and the average life satisfaction on the vertical axis. From your results in 2(a) and (b), how has the distribution of life satisfaction changed over time? What other information about the distribution of life satisfaction could we use to supplement these results? Choose one or two countries and research events that could explain the observed changes in average life satisfaction over time shown in 2(a) and (b). R walk-through 8.7 Plotting multiple lines on a chart Calculate average life satisfaction, by wave and country Before we can plot the line charts, we have to calculate the average life satisfaction for each country in each wave. In R walk-through 8.5 we produced summary tables, grouped by country and employment status. We will copy this process, but now we only require mean values. Countries that do not report the life satisfaction variable for all waves will have an average life satisfaction of ‘NA’. Since each country is represented by a row in the summary table, we use the rowwise and na.omit functions to drop any countries that do not have a value for the average life satisfaction for all four waves. df.new <- lifesat_data %>% group_by(S002EVS, S003) %>% # Calculate average life satisfaction per wave and country summarize(avg_lifesat = round(mean(A170), 2)) %>% # Reshape the data to one row per country spread(S002EVS, avg_lifesat) %>% # Set subsequent operations to be performed on each row rowwise() %>% # Drop any rows with missing observations na.omit() %>% print() ## # A tibble: 11 x 5 ## S003 `1981-1984` `1990-1993` `1999-2001` `2008-2010` ## <chr> <dbl> <dbl> <dbl> <dbl> ## 1 Belgium 7.37 7.6 7.42 7.63 ## 2 Denmark 8.21 8.17 8.31 8.41 ## 3 France 6.71 6.77 6.98 7.05 ## 4 Germany 7.22 7.03 7.43 6.77 ## 5 Iceland 8.05 8.01 8.08 8.07 ## 6 Ireland 7.82 7.88 8.21 7.82 ## 7 Italy 6.65 7.3 7.18 7.4 ## 8 Netherlands 7.75 7.77 7.83 7.99 ## 9 Northern Ireland 7.66 7.88 8.07 7.82 ## 10 Spain 6.6 7.15 6.97 7.29 ## 11 Sweden 8.03 7.99 7.62 7.68 Create a line chart for average life satisfaction To draw the lines for all countries on a single plot, we need to use the gather function to transform the data into the correct format to use in ggplot. df.new %>% gather(Wave, mean, `1981-1984`:`2008-2010`) %>% # Group the data by country ggplot(., aes(x = Wave, y = mean, color = S003)) + geom_line(aes(group = S003, linetype = S003), size = 1) + theme_bw() <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Line chart of average life satisfaction across countries and survey waves. '' /> Line chart of average life satisfaction across countries and survey waves. Figure 8.3 Line chart of average life satisfaction across countries and survey waves. correlation coefficientA numerical measure, ranging between 1 and −1, of how closely associated two variables are—whether they tend to rise and fall together, or move in opposite directions. A positive coefficient indicates that when one variable takes a high (low) value, the other tends to be high (low) too, and a negative coefficient indicates that when one variable is high the other is likely to be low. A value of 1 or −1 indicates that knowing the value of one of the variables would allow you to perfectly predict the value of the other. A value of 0 indicates that knowing one of the variables provides no information about the value of the other. After describing patterns in our main variables over time, we will use correlation coefficients and scatter plots to look at the relationship between these variables and the other variables in our dataset. Using the Wave 4 data: Create a table as shown in Figure 8.4 and calculate the required correlation coefficients. For employment status and gender, you will need to create new variables: full-time employment should be equal to 1 if full-time employed and 0 if unemployed, and treated as missing data (left as a blank cell) otherwise. Gender should be 0 if male and 1 if female. Variable Life satisfaction Work ethic Age Education Full-time employment Gender Self-reported health Income Number of children Relative income Life satisfaction 1 Work ethic 1 Correlation between life satisfaction, work ethic and other variables, Wave 4. Figure 8.4 Correlation between life satisfaction, work ethic and other variables, Wave 4. Interpret the coefficients, paying close attention to how the variables are coded. (For example, you could comment on the absolute magnitude and sign of the coefficients). Explain whether the relationships implied by the coefficients are what you expected (for example, would you expect life satisfaction to increase or decrease with health, income, etc.) R walk-through 8.8 Creating dummy variables and calculating correlation coefficients To obtain the correlation coefficients between variables, we have to make sure that all variables are numeric. However, the data on sex and employment status are coded using text, so we need to create two new variables (gender and employment). (We choose to create a new variable rather than overwrite the original variable so that even if we make a mistake, the raw data is still preserved). We can use the ifelse function to make the value of the variable conditional on whether a logical statement (e.g. X001 == ''Male'') is satisfied or not. As shown below, we can nest ifelse statements to create more complex conditions, which is useful if the variable contains more than two values. We used two ifelse statements for the unemployment variable (X028) so that the new variable will be 1 for full-time employed, 0 for unemployed, and NA if neither condition is satisfied. Although we had previously removed observations with missing values (‘NA’), we have reintroduced ‘NA’ values in the previous step when creating the employment variable. This would cause problems when obtaining the correlation coefficients using the cor function, so we include the option use = ''pairwise.complete.obs'' to exclude observations containing ‘NA’ from the calculations. lifesat_data %>% # We only require Wave 4. subset(S002EVS == ''2008-2010'') %>% # Create a gender variable mutate(gender = as.numeric( ifelse(X001 == ''Male'', 0, 1))) %>% # Provide nested condition for employment mutate(employment = as.numeric( ifelse(X028 == ''Full time'', 1, ifelse(X028 == ''Unemployed'', 0, NA)))) %>% select(X003, Education_1, employment, gender, A009, X047D, X011_01, percentile, A170, work_ethic) %>% # Store output in matrix cor(., use = ''pairwise.complete.obs'') -> M # Only interested in two columns, so use matrix indexing M[, c(''A170'', ''work_ethic'')] ## A170 work_ethic ## X003 -0.08220504 0.13333312 ## Education_1 0.09363900 -0.14535643 ## employment 0.18387286 -0.02710835 ## gender -0.02105535 -0.04781557 ## A009 0.37560287 -0.07320865 ## X047D 0.23542497 -0.15226999 ## X011_01 -0.01729372 0.08925452 ## percentile 0.29592968 -0.18679869 ## A170 1.00000000 -0.03352464 ## work_ethic -0.03352464 1.00000000 Next we will look at the relationship between employment status and life satisfaction, and investigate the paper’s hypothesis that this relationship varies with a country’s average work ethic. Using the data from Wave 4, carry out the following: Create a table showing the average life satisfaction according to employment status (showing the full-time employed, retired, and unemployed categories only) with country (S003) as the row variable, and employment status (X028) as the column variable. Comment on any differences in average life satisfaction between these three groups, and whether social norms is a plausible explanation for these differences. Use the table from Question 4(a) to calculate the difference in average life satisfaction (full-time employed minus unemployed, and full-time employed minus retired). Make a separate scatterplot for each of these differences in life satisfaction, with average work ethic on the horizontal axis and difference in life satisfaction on the vertical axis. For each difference (employed vs unemployed, employed vs retired), calculate and interpret the correlation coefficient between average work ethic and difference in life satisfaction. R walk-through 8.9 Calculating group means Calculate average life satisfaction and differences in average life satisfaction We can achieve the tasks in Question 4(a) and (b) in one go using an approach similar to that used in R walk-through 8.5, although now we are interested in calculating the average life satisfaction by country and employment type. Once we have tabulated these means, we can compute the difference in the average values. We will create two new variables: D1 for the difference between the average life satisfaction for full-time employed and unemployed, and D2 for the difference in average life satisfaction for full-time employed and retired individuals. # Set the employment types that we want to report employment_list = c(''Full time'', ''Retired'', ''Unemployed'') df.employment <- lifesat_data %>% # Select Wave 4 subset(S002EVS == ''2008-2010'') %>% # Select only observations with these employment types subset(X028 %in% employment_list) %>% # Group by country and then employment type group_by(S003, X028) %>% # Calculate the mean by country/employment type group summarize(mean = mean(A170)) %>% # Reshape to one row per country spread(X028, mean) %>% # Create the difference in means mutate(D1 = `Full time` - Unemployed, D2 = `Full time` - Retired) kable(df.employment, format = ''markdown'') |S003 | Full time| Retired| Unemployed| D1| D2| |:------------------|---------:|--------:|----------:|----------:|----------:| |Albania | 6.634561| 5.807339| 6.066038| 0.5685232| 0.8272215| |Armenia | 6.037671| 4.853333| 5.459016| 0.5786548| 1.1843379| |Austria | 7.435950| 7.741936| 6.074074| 1.3618763| -0.3059851| |Belarus | 6.099162| 5.617391| 5.606061| 0.4931014| 0.4817707| |Belgium | 7.717014| 7.834951| 6.366667| 1.3503472| -0.1179376| |Bosnia Herzegovina | 7.332447| 7.006173| 6.771812| 0.5606347| 0.3262740| |Bulgaria | 6.177007| 4.972973| 4.693878| 1.4831297| 1.2040343| |Croatia | 7.305668| 6.478964| 7.165644| 0.1400238| 0.8267036| |Cyprus | 7.384401| 7.031746| 6.560000| 0.8244011| 0.3526551| |Czech Republic | 7.302135| 6.885086| 6.065217| 1.2369173| 0.4170491| |Denmark | 8.541315| 8.205426| 7.200000| 1.3413153| 0.3358890| |Estonia | 6.925117| 6.253444| 4.971429| 1.9536884| 0.6716735| |Finland | 7.817073| 8.018692| 5.767442| 2.0496313| -0.2016184| |France | 7.200637| 6.974093| 6.242424| 0.9582127| 0.2265437| |Georgia | 6.116667| 4.690377| 5.421446| 0.6952203| 1.4262901| |Germany | 7.258114| 6.848548| 4.608466| 2.6496488| 0.4095667| |Great Britain | 7.532934| 7.930796| 6.025641| 1.5072931| -0.3978617| |Greece | 7.152113| 6.621622| 5.980392| 1.1717205| 0.5304911| |Hungary | 6.645941| 5.890000| 4.858407| 1.7875342| 0.7559413| |Iceland | 8.203857| 8.446808| 7.230769| 0.9730875| -0.2429518| |Ireland | 7.895735| 7.828571| 7.181818| 0.7139164| 0.0671632| |Italy | 7.427083| 7.440594| 6.595745| 0.8313387| -0.0135107| |Kosovo | 6.297710| 6.038462| 6.784461| -0.4867512| 0.2592484| |Latvia | 6.516854| 5.928058| 5.305085| 1.2117692| 0.5887964| |Lithuania | 6.586387| 5.631387| 4.530612| 2.0557752| 0.9550006| |Luxembourg | 7.866221| 8.240224| 5.500000| 2.3662207| -0.3740027| |Macedonia | 7.193059| 6.671296| 6.609418| 0.5836403| 0.5217623| |Malta | 7.700405| 7.760234| 5.954546| 1.7458594| -0.0598291| |Moldova | 7.117318| 5.976744| 6.071429| 1.0458899| 1.1405742| |Montenegro | 7.632967| 7.216495| 7.473186| 0.1597809| 0.4164722| |Netherlands | 8.037037| 7.945245| 7.000000| 1.0370370| 0.0917921| |Northern Cyprus | 6.738095| 6.444444| 5.642857| 1.0952381| 0.2936508| |Northern Ireland | 7.677419| 7.769231| 7.540540| 0.1368788| -0.0918114| |Norway | 8.193182| 8.264000| 8.000000| 0.1931818| -0.0708182| |Poland | 7.464692| 6.574830| 7.063291| 0.4014013| 0.8898626| |Portugal | 6.838527| 5.878906| 5.438597| 1.3999304| 0.9596207| |Romania | 7.135392| 6.574713| 7.407407| -0.2720155| 0.5606793| |Russian Federation | 6.861436| 5.702290| 6.404255| 0.4571804| 1.1591456| |Serbia | 7.170673| 6.673203| 6.731517| 0.4391556| 0.4974705| |Slovakia | 7.468384| 6.710145| 6.118644| 1.3497400| 0.7582391| |Slovenia | 7.834211| 7.130435| 6.760000| 1.0742105| 0.7037757| |Spain | 7.336870| 7.209945| 7.191781| 0.1450892| 0.1269253| |Sweden | 7.877315| 8.173554| 6.515151| 1.3621633| -0.2962389| |Switzerland | 8.037528| 8.115578| 5.761905| 2.2756228| -0.0780503| |Turkey | 6.500000| 6.606965| 5.762542| 0.7374582| -0.1069652| |Ukraine | 6.344398| 5.444444| 4.949367| 1.3950313| 0.8999539| Make a scatterplot, sorted according to work ethic In order to plot the differences ordered by the average work ethic, we first need to get all data from Wave 4 (using subset), summarize the work_ethic variable by country (group_by then summarize), and store the results in a temporary dataframe (df.work.ethic). df.work_ethic <- lifesat_data %>% subset(S002EVS == ''2008-2010'') %>% select(S003, work_ethic) %>% group_by(S003) %>% summarize(mean_work = mean(work_ethic)) We can now combine the average work_ethic data with the table containing the difference in means (using inner_join to match the data correctly by country) and make a scatterplot using ggplot. This process can be repeated for the difference in means between full-time employed and retired individuals by changing y = D1 to y = D2 in the ggplot function. df.employment %>% # Combine with the average work ethic data inner_join(., df.work_ethic, by = ''S003'') %>% ggplot(., aes(y = D1, x = mean_work)) + geom_point(stat = ''identity'') + xlab(''Work ethic'') + ylab(''Difference'') + ggtitle(''Difference in wellbeing between the full-time employed and the unemployed'') + theme_bw() + # Rotate the country names theme(axis.text.x = element_text( angle = 90, hjust = 1), plot.title = element_text(hjust = 0.5)) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Difference in life satisfaction (wellbeing) between the full-time employed and the unemployed vs average work ethic. '' /> Difference in life satisfaction (wellbeing) between the full-time employed and the unemployed vs average work ethic. Figure 8.5 Difference in life satisfaction (wellbeing) between the full-time employed and the unemployed vs average work ethic. To calculate correlation coefficients, use the cor function. You can see that the correlation between average work ethic and difference in life satisfaction is quite weak for employed vs unemployed, but moderate and positive for employed vs retired. # Full-time vs unemployed cor(df.employment$D1, df.work_ethic$mean_work) ## [1] -0.1575654 # Full-time vs retired cor(df.employment$D2, df.work_ethic$mean_work) ## [1] 0.4842609 So far we have described the data using tables and charts, but have not made any statements about whether what we observe is consistent with an assumption that work ethic has no bearing on differences in life satisfaction between different subgroups. In the next part, we will assess the relationship between employment status and life satisfaction and assess whether the observed data lead us to reject the above assumption. Part 8.3 Confidence intervals for difference in the mean Note You will need to have done Questions 1–5 in Part 8.1 before doing this part. Part 8.2 is not necessary but is helpful to get an idea of what the data looks like. Learning objectives for this part calculate and interpret confidence intervals for the difference in means between two groups. The aim of this project was to look at the empirical relationship between employment and life satisfaction. When we calculate differences between groups, we collect evidence which may or may not support a hypothesis that life satisfaction is identical between different subgroups. Economists often call this testing for statistical significance. In Part 6.2 of Empirical Project 6, we constructed 95% confidence intervals for means, and used a rule of thumb to decide whether we could maintain the assumption that the two subgroups were really identical. Now we will learn how to construct confidence intervals for the difference in two means, which allows us to make such a judgment on the basis of a single confidence interval. Remember that the width of a confidence interval depends on the standard deviation and number of observations. (Read Part 6.2 of Empirical Project 6 to understand why.) When making a confidence interval for a sample mean (such as the mean life satisfaction of the unemployed), we use the sample standard deviation and number of observations in that sample (unemployed people) to obtain the standard error of the sample mean. text{Standard error of sample mean (SE)} = frac{text{SD of sample data}}{sqrt{text{number of observations (n)}}} When we look at the difference in means (such as life satisfaction of employed minus unemployed), we are using data from two groups (the unemployed and the employed) to make one confidence interval. To calculate the standard error for the difference in means, we use the standard errors (SE) of each group: text{SE (difference in means)} = sqrt{(text{SE of group 1})^2 + (text{SE of group 2})^2} This formula requires the two groups of data to be independent, meaning that the data from one group is not related, paired, or matched with data from the other group. This assumption is reasonable for the life satisfaction data we are using. However, if the two groups of data are not independent, for example if the same people generated both groups of data (as in the public goods experiments of Empirical Project 2), then we cannot use this formula. Once we have the new standard error and number of observations, we can calculate the width of the confidence interval (distance from the mean to one end of the interval) as before. The width of the confidence interval tells us how precisely the difference in means was estimated, and this precision tells us how confident we can be that the difference is not due to chance. If the value 0 falls outside the 95% confidence interval, this is strong evidence of a real difference in means. For example, if the estimated difference is positive, and the 95% confidence interval is not wide enough to include 0, we can say with 95% confidence that the true difference is positive too. In other words, it tells us that the p-value for the difference we have found is less than 0.05, so we would be very unlikely to find such a big difference by chance. In Figure 8.6, for Great Britain, we can say with 95% confidence that the true difference in average life satisfaction between the full-time employed and the unemployed is positive. However, for Spain, we do not have strong evidence of a real difference in mean life satisfaction. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''95% confidence intervals for the average difference in life satisfaction (full-time employed minus unemployed) in Great Britain and Spain. '' /> 95% confidence intervals for the average difference in life satisfaction (full-time employed minus unemployed) in Great Britain and Spain. Figure 8.6 95% confidence intervals for the average difference in life satisfaction (full-time employed minus unemployed) in Great Britain and Spain. We will apply this method to make confidence intervals for differences in life satisfaction. Choose three countries: one with an average work ethic in the top third of scores, one in the middle third, and one in the lower third. (See R walk-through 6.3 for help on calculating confidence intervals and adding them to a chart.) In Question 4 of Part 8.2 you computed the average life satisfaction score by country and employment type (using Wave 4 data). Repeat this procedure to obtain the standard error for the means for each of your chosen countries. Create a table for these countries, showing the average life satisfaction score, standard deviation of life satisfaction, and number of observations, with country (S003) as the row variable, and employment status (full-time employed, retired, and unemployed only) as the column variable. Use your table from Question 1(a) to calculate the difference in means (full-time employed minus unemployed, and full-time employed minus retired), the standard deviation of these differences, the number of observations, and the confidence interval. Use R’s t.test function to determine the 95% confidence interval width of the difference in means and compare your results with Question 1(b). Plot a column chart for your chosen countries showing the difference in life satisfaction (employed vs unemployed and employed vs retired) on the vertical axis, and country on the horizontal axis (sorted according to low, medium, and high work ethic). Add the confidence intervals from Question 1(c) to your chart. Interpret your findings from Question 1(d), commenting on the size of the observed differences in means, and the precision of your estimates. R walk-through 8.10 Calculating confidence intervals and adding error bars We will use Turkey, Spain, and Great Britain as example countries in the top, middle, and bottom third of work ethic scores respectively. In the tasks in Questions 1(a) and (b) we will obtain the means, standard errors, and 95% confidence intervals step-by-step, then for Question 1(c) we show how to use a shortcut to obtain confidence intervals from a single function. Calculate confidence intervals manually We obtained the difference in means in R walk-through 8.9 (D1 and D2), so now we can calculate the standard error of the means for each country of interest. # List chosen countries country_list <- c(''Turkey'', ''Spain'', ''Great Britain'') df.employment.se <- lifesat_data %>% # Select Wave 4 subset(S002EVS == ''2008-2010'') %>% # Select the employment types we are interested in subset(X028 %in% employment_list) %>% # Select countries subset(S003 %in% country_list) %>% # Group by country and employment type group_by(S003, X028) %>% # Calculate the standard error of each group mean summarize(se = sd(A170) / sqrt(n())) %>% spread(X028, se) %>% # Calculate the SE of difference mutate(D1_SE = sqrt(`Full time`^2 + Unemployed^2), D2_SE = sqrt(`Full time`^2 + Retired^2)) We can now combine the standard errors with the difference in means, and compute the 95% confidence interval width. df.employment <- df.employment %>% # Select chosen countries subset(S003 %in% country_list) %>% # We only need the differences. select(-`Full time`, -Retired, -Unemployed) %>% # Join the means with the respective SEs inner_join(., df.employment.se, by = ''S003'') %>% select(-`Full time`, -Retired, -Unemployed) %>% # Compute confidence interval width for both differences mutate(CI_1 = 1.96 * D1_SE, CI_2 = 1.96 * D2_SE) %>% print() ## # A tibble: 3 x 7 ## # Groups: S003 [3] ## S003 D1 D2 D1_SE D2_SE CI_1 CI_2 ## <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> ## 1 Great Britain 1.51 -0.398 0.267 0.154 0.524 0.302 ## 2 Spain 0.145 0.127 0.265 0.168 0.520 0.330 ## 3 Turkey 0.737 -0.107 0.229 0.231 0.449 0.452 We now have a table containing the difference in means, the standard error of the difference in means, and the confidence intervals for each of the two differences. (Recall that D1 is the difference between the average life satisfaction for full-time employed and unemployed, and D2 is the difference in average life satisfaction for full-time employed and retired individuals.) Calculate confidence intervals using the t.test function We could obtain the confidence intervals directly by using the t.test function. First we need to prepare the data in two groups. In the following example we go through the difference in average life satisfaction for full-time employed and unemployed individuals in Turkey, but the process can be repeated for the difference between full-time employed and retired individuals by changing the code appropriately (also for your other two chosen countries). We start by selecting the data for full-time and unemployed people in Turkey, and storing it in two separate temporary matrices (arrays of rows and columns) called turkey_full and turkey_unemployed respectively, which is the format needed for the t.test function. turkey_full <- lifesat_data %>% # Select Wave 4 only subset(S002EVS == ''2008-2010'') %>% # Select country subset(S003 == ''Turkey'') %>% # Select employment type subset(X028 == ''Full time'') %>% # We only need the life satisfaction data. select(A170) %>% # We have to set it as a matrix for t.test. as.matrix() # Repeat for second group turkey_unemployed <- lifesat_data %>% subset(S002EVS == ''2008-2010'') %>% subset(S003 == ''Turkey'') %>% subset(X028 == ''Unemployed'') %>% select(A170) %>% as.matrix() We can now use the t.test function on the two newly created matrices. We also set the confidence level to 95%. The t.test function produces quite a bit of output, but we are only interested in the confidence interval, which we can obtain by using $conf.int. turkey_ci <- t.test(turkey_full, turkey_unemployed, conf.level = 0.95)$conf.int turkey_ci ## [1] 0.2873459 1.1875704 ## attr(,''conf.level'') ## [1] 0.95 We can then calculate the difference in means by finding the midpoint of the interval ((turkey_ci[2] + turkey_ci[1])/2 is 0.7374582), which should be the same as the figures obtained in Question 1(b) (df.employment[3, 2] is 0.7374582). Add error bars to the column charts We can now use these confidence intervals (and widths) to add error bars to our column charts. To do so, we use the geom_errorbar option, and specify the lower and upper levels of the confidence interval for the ymin and ymax options respectively. In this case it is easier to use the results from Questions 1(a) and (b), as we already have the values for the difference in means and the CI width stored as variables. ggplot(df.employment, aes(x = S003, y = D1)) + geom_bar(stat = ''identity'') + geom_errorbar(aes(ymin = D1 - CI_1, ymax = D1 + CI_1), width = .2) + ylab(''Difference in means'') + xlab(''Country'') + theme_bw() + ggtitle(''Difference in wellbeing (full-time and unemployed)'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-08-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Difference in life satisfaction (wellbeing) between full-time employed and unemployed. '' /> Difference in life satisfaction (wellbeing) between full-time employed and unemployed. Figure 8.7 Difference in life satisfaction (wellbeing) between full-time employed and unemployed. Again, this can be repeated for the difference in life satisfaction between full-time employed and retired. Remember to change y = D1 to y = D2, and change the upper and lower limits for the error bars. The method we used to compare life satisfaction relied on making comparisons between people with different employment statuses, but a person’s employment status is not entirely random. We cannot therefore make causal statements such as ‘being unemployed causes life satisfaction to decrease’. Describe how we could use the following methods to assess better the effect of being unemployed on life satisfaction, and make some statements about causality: a natural experiment panel data (data on the same individuals, taken at different points in time)."
});
index.addDoc({
    id: 45,
    title: "Doing Economics: Empirical Project 8: Working in Google Sheets",
    content: "Empirical Project 8 Working in Google Sheets Part 8.1 Cleaning and summarizing the data Learning objectives for this part detect and correct entries in a dataset recode variables to make them easier to analyse calculate percentiles for subsets of the data. So far we have been working with data that was formatted correctly. However, sometimes the datasets will be messier and we will need to ‘clean’ the data before starting any analysis. Data cleaning involves checking that all variables are formatted correctly, all observations are entered correctly (e.g. no typos), and missing values are coded in a way that the software you are using can understand. Otherwise, the software will either not be able to analyse your data, or only analyse the observations it recognizes, which could lead to incorrect results and conclusions. In the data we will use, the European Values Study (EVS) data has been converted to Excel from another program, so there are some entries that were not converted correctly and some variables that need to be recoded (for example, replacing words with numbers, or replacing one number with another). Download the EVS data and documentation: Download the EVS data. For the documentation, go to the data download site. Click on the ‘Data and Documents’ button in the middle of the page, then click the ‘Other Documents’ button and download the PDF file called ‘ZA4804_EVS_VariableCorrespondence.pdf’. This file contains information about each variable in the dataset (e.g. variable name and what it measures). The data spreadsheet contains an incomplete Data dictionary and four tabs with the data collected from each wave of the survey. The variable names currently do not tell us what the variable is, so we need to relabel them to avoid confusion. Use the PDF you have downloaded to fill in Column C (Variable description) of the Data dictionary tab. The second column of the PDF lists all variables in the dataset (alphabetically) and tells you what it measures. Fill in Column B with an appropriate new name for each variable and rename the variables in the other four tabs accordingly. Throughout this project we will refer to the variables using their original names, so the Data dictionary tab will come in handy. In general, data dictionaries and variable correspondences are useful because they contain important information about what each variable represents and how it is measured, which usually cannot be summarized in a short variable name. Now we will take a closer look at how some of the variables were measured. Variable A170 is reported life satisfaction on a scale of 1 (dissatisfied) to 10 (satisfied). Respondents answered the question ‘All things considered, how satisfied are you with your life as a whole these days?’ Discuss the assumptions needed to use this measure in interpersonal and cross-country comparisons, and whether you think they are plausible. (You may find it helpful to refer to Box 2.1 of the ‘OECD guidelines on measuring subjective well-being’.) An individual’s employment status (variable X028) was self-reported. Explain whether misreporting of employment status is likely to be an issue, and give some factors that may affect the likelihood of misreporting in this context. Variables C036 to C041 ask about an individual’s attitudes towards work. With self-reports, we may also be concerned that individuals are using a heuristic (rule of thumb) to answer the questions. Table 2.1 of the ‘OECD Guidelines on measuring subjective well-being’ lists some response biases and heuristics that individuals could use. Pick three that you think particularly apply to questions about life satisfaction or work ethic and describe how we might check whether this issue may be present in our data. Before doing any data analysis, it is important to check that all variables are coded in a way that the software can recognize. This process involves checking how missing values are coded (usually these need to be coded in a particular way for each software), and that numerical variables (numbers) only contain numbers and not text (in order to calculate summary statistics). We will now check and clean the dataset so it is ready to use. Make the following changes to all relevant data tabs: Currently, missing values are recorded as ‘.a’, but we would like them to be blank cells. Use Google Sheets’ Find and Replace tool to change the ‘.a’ to blank cells for variables A009 to X047D. (See Google Sheets walk-through 8.1 for help on how to do this). Variable A170 (life satisfaction) is currently a mixture of numbers (2 to 9) and words (‘Satisfied’ and ‘Dissatisfied’), but we would like it to be all numbers. Replace the word ‘Dissatisfied’ with the number 1, and the word ‘Satisfied’ with the number 10. Similarly, variable X011_01 (number of children) has recorded no children as a word rather than a number. Replace ‘No children’ with the number 0. The variables C036 to C041 should be replaced with numbers ranging from 1 (‘Strongly disagree’) to 5 (‘Strongly agree’) so we can take averages of them later. Similarly, variable A009 should be recoded as 1 = ‘Very Poor’, 2 = ‘Poor’, 3 = ‘Fair’, 4 = ‘Good’, 5 = ‘Very Good’. (Hint: You may find it helpful to do the find and replace in ascending/descending order i.e. recode ’Strongly Disagree’/‘Very Poor’ as 1, then ‘Disagree’/‘Poor’ as 2, and so on.) We would like to split X025A into two variables, one for the number before the colon, and the other containing the words after the colon. Use Google Sheets’ LEFT and/or RIGHT functions to create two new variables accordingly. (See Google Sheets walk-through 8.1 for help on how to do this). Google Sheets walk-through 8.1 Cleaning data and splitting variables <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to clean data and split variables '' /> How to clean data and split variables Figure 8.1 How to clean data and split variables <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the data looks like. Currently some cells have entries ‘.a’ where the data is missing. We will find and replace the variables A009 to X047D. '' /> The data This is what the data looks like. Currently some cells have entries ‘.a’ where the data is missing. We will find and replace the variables A009 to X047D. Figure 8.1a This is what the data looks like. Currently some cells have entries ‘.a’ where the data is missing. We will find and replace the variables A009 to X047D. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Find and replace specific cell entries : ‘Find and Replace’ in Google Sheets is similar to this feature in Microsoft Word, except that it searches for text in individual cells rather than in lines of text. You can adapt steps 1–4 to change specific values of cells in a column, for example replacing a particular word with a particular number. '' /> Find and replace specific cell entries ‘Find and Replace’ in Google Sheets is similar to this feature in Microsoft Word, except that it searches for text in individual cells rather than in lines of text. You can adapt steps 1–4 to change specific values of cells in a column, for example replacing a particular word with a particular number. Figure 8.1b ‘Find and Replace’ in Google Sheets is similar to this feature in Microsoft Word, except that it searches for text in individual cells rather than in lines of text. You can adapt steps 1–4 to change specific values of cells in a column, for example replacing a particular word with a particular number. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Extract a specific part of a string of text : The LEFT and RIGHT functions can help you extract parts of a long string of text, which is useful if you want to shorten variables or only keep particular information in a cell. Here, we are going to save variable X025A as two variables: ‘Education_1’ will contain the numbers before the colon, and ‘Education_2’ will contain the words after the colon. '' /> Extract a specific part of a string of text The LEFT and RIGHT functions can help you extract parts of a long string of text, which is useful if you want to shorten variables or only keep particular information in a cell. Here, we are going to save variable X025A as two variables: ‘Education_1’ will contain the numbers before the colon, and ‘Education_2’ will contain the words after the colon. Figure 8.1c The LEFT and RIGHT functions can help you extract parts of a long string of text, which is useful if you want to shorten variables or only keep particular information in a cell. Here, we are going to save variable X025A as two variables: ‘Education_1’ will contain the numbers before the colon, and ‘Education_2’ will contain the words after the colon. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-01-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Extract a specific part of a string of text : The RIGHT function counts the specified number of characters starting from the right end of the string of text. Since words have different lengths, we use the len() function to calculate the length of the string, then subtract the colon’s position in the string (using the find() function). For example, if the string is 10 characters long, and the colon is the third character, we want Google sheets to extract characters 4 to 10 (the first 6 characters from the right). '' /> Extract a specific part of a string of text The RIGHT function counts the specified number of characters starting from the right end of the string of text. Since words have different lengths, we use the len() function to calculate the length of the string, then subtract the colon’s position in the string (using the find() function). For example, if the string is 10 characters long, and the colon is the third character, we want Google sheets to extract characters 4 to 10 (the first 6 characters from the right). Figure 8.1d The RIGHT function counts the specified number of characters starting from the right end of the string of text. Since words have different lengths, we use the len() function to calculate the length of the string, then subtract the colon’s position in the string (using the find() function). For example, if the string is 10 characters long, and the colon is the third character, we want Google sheets to extract characters 4 to 10 (the first 6 characters from the right). Although the paper we are following only considered individuals aged 25–80 who were not students, we will retain the other observations in our analysis. However, we need to remove any observations that have missing values for A170, X028, or any of the other variables except X047D. Removing missing data ensures that any summary statistics or analysis we do is done on the same set of data (without having to always account for the fact that some values are missing), and is fine as long as the data are missing at random (i.e. there is no particular reason why certain observations are missing and not others). In your spreadsheet, remove all rows in all waves that have missing data for A170. Do the same for: X003, X028, X007 and X001 in all waves A009 in Waves 1, 2, and 4 only C036, C037, C038, C039, C041 and X047D in Waves 3 and 4 only X011_01 and X025A, in Wave 4. Google Sheets walk-through 8.2 gives guidance on how to do this. (Note: Google Sheets may take some time to process the commands.) Google Sheets walk-through 8.2 Dropping observations that satisfy particular conditions <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to drop observations that satisfy particular conditions using ‘filter’, ‘select’ and ‘delete’. '' /> How to drop observations that satisfy particular conditions using ‘filter’, ‘select’ and ‘delete’. Figure 8.2 How to drop observations that satisfy particular conditions using ‘filter’, ‘select’ and ‘delete’. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data. : This is what the data looks like. In this example, we will remove any rows with the variable A170 recorded as missing (blank). First, we will filter the data so that only rows with blank cells in this column are showing. '' /> The data. This is what the data looks like. In this example, we will remove any rows with the variable A170 recorded as missing (blank). First, we will filter the data so that only rows with blank cells in this column are showing. Figure 8.2a This is what the data looks like. In this example, we will remove any rows with the variable A170 recorded as missing (blank). First, we will filter the data so that only rows with blank cells in this column are showing. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Filter the relevant rows : After step 2, you will see all the data that needs to be deleted. '' /> Filter the relevant rows After step 2, you will see all the data that needs to be deleted. Figure 8.2b After step 2, you will see all the data that needs to be deleted. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Select the rows that need to be deleted : We will select all the rows that are visible and delete them in one go. '' /> Select the rows that need to be deleted We will select all the rows that are visible and delete them in one go. Figure 8.2c We will select all the rows that are visible and delete them in one go. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Delete these rows from the dataset : Google Sheets will remove all the selected rows from the dataset. If there are many rows to delete, this command may take some time to process. '' /> Delete these rows from the dataset Google Sheets will remove all the selected rows from the dataset. If there are many rows to delete, this command may take some time to process. Figure 8.2d Google Sheets will remove all the selected rows from the dataset. If there are many rows to delete, this command may take some time to process. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Clear the filter to see the rest of the data : After removing the specified rows, clear the filter to see the rest of your data. You will see that the number of rows has decreased. '' /> Clear the filter to see the rest of the data After removing the specified rows, clear the filter to see the rest of your data. You will see that the number of rows has decreased. Figure 8.2e After removing the specified rows, clear the filter to see the rest of your data. You will see that the number of rows has decreased. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-02-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The modified data : This is what the data looks like after removing rows with missing entries in A170. You can adapt steps 1–5 to remove rows that satisfy other conditions, such as containing a particular value or range of values. '' /> The modified data This is what the data looks like after removing rows with missing entries in A170. You can adapt steps 1–5 to remove rows that satisfy other conditions, such as containing a particular value or range of values. Figure 8.2f This is what the data looks like after removing rows with missing entries in A170. You can adapt steps 1–5 to remove rows that satisfy other conditions, such as containing a particular value or range of values. We will now create two variables, work ethic and relative income, which we will use in our comparison of life satisfaction. Work ethic is measured as the average of C036 to C041. Create a new variable showing this work ethic measure. Since unemployed individuals and students may not have income, the study calculated relative income as the relative household income of that individual, measured as a deviation from the average income in that individual’s country. Explain the issues with using this method if the income distribution is skewed (for example, a long right tail). Instead of using average income, we will define relative income as the percentile of that individual’s household in the income distribution. Create a new variable showing this information. (See Google Sheets walk-through 8.3 for one way to do this). Google Sheets walk-through 8.3 Calculating percentiles from actual values <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to calculate percentiles from actual values. '' /> How to calculate percentiles from actual values. Figure 8.3 How to calculate percentiles from actual values. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : This is what the data looks like. In this example, we will calculate the percentile of X047D (log income), according to the country (S003). You will need to use two new columns (R and S in this case). '' /> The data This is what the data looks like. In this example, we will calculate the percentile of X047D (log income), according to the country (S003). You will need to use two new columns (R and S in this case). Figure 8.3a This is what the data looks like. In this example, we will calculate the percentile of X047D (log income), according to the country (S003). You will need to use two new columns (R and S in this case). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Calculate the rank of each observation : The intuition behind our rank calculation is that an observation’s rank is the number of observations larger than it, plus 1. (For example, if there were only two numbers, the smaller number should be ranked 2 and the larger number ranked 1). By default, Google Sheets will rank observations from largest to smallest (higher numbers will have lower ranks). We need this setting to calculate percentiles. In this example, we need to rank observations only within the country it belongs to, not the entire dataset, so we use this as a condition in COUNTIFS. '' /> Calculate the rank of each observation The intuition behind our rank calculation is that an observation’s rank is the number of observations larger than it, plus 1. (For example, if there were only two numbers, the smaller number should be ranked 2 and the larger number ranked 1). By default, Google Sheets will rank observations from largest to smallest (higher numbers will have lower ranks). We need this setting to calculate percentiles. In this example, we need to rank observations only within the country it belongs to, not the entire dataset, so we use this as a condition in COUNTIFS. Figure 8.3b The intuition behind our rank calculation is that an observation’s rank is the number of observations larger than it, plus 1. (For example, if there were only two numbers, the smaller number should be ranked 2 and the larger number ranked 1). By default, Google Sheets will rank observations from largest to smallest (higher numbers will have lower ranks). We need this setting to calculate percentiles. In this example, we need to rank observations only within the country it belongs to, not the entire dataset, so we use this as a condition in COUNTIFS. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-08-03-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use the calculated rank to calculate the percentile : The intuition behind the percentile calculation is that an observation’s percentile is the percentage of observations smaller than it (the +1 in the formula is needed to exclude the observation itself). The 1– at the start of the formula is because we ranked observations from largest to smallest (so we need to ‘reverse’ all the numbers). Here we have multiplied by 100 so the numbers correspond to actual percentiles (e.g. 92 percentile). You can see that higher incomes have lower ranks and higher percentiles, indicating that we have done this correctly. Note: This example was based on observations with non-missing income only (not the fully-cleaned dataset), so the numbers you get will be slightly different. '' /> Use the calculated rank to calculate the percentile The intuition behind the percentile calculation is that an observation’s percentile is the percentage of observations smaller than it (the +1 in the formula is needed to exclude the observation itself). The 1– at the start of the formula is because we ranked observations from largest to smallest (so we need to ‘reverse’ all the numbers). Here we have multiplied by 100 so the numbers correspond to actual percentiles (e.g. 92 percentile). You can see that higher incomes have lower ranks and higher percentiles, indicating that we have done this correctly. Note: This example was based on observations with non-missing income only (not the fully-cleaned dataset), so the numbers you get will be slightly different. Figure 8.3c The intuition behind the percentile calculation is that an observation’s percentile is the percentage of observations smaller than it (the +1 in the formula is needed to exclude the observation itself). The 1– at the start of the formula is because we ranked observations from largest to smallest (so we need to ‘reverse’ all the numbers). Here we have multiplied by 100 so the numbers correspond to actual percentiles (e.g. 92 percentile). You can see that higher incomes have lower ranks and higher percentiles, indicating that we have done this correctly. Note: This example was based on observations with non-missing income only (not the fully-cleaned dataset), so the numbers you get will be slightly different. Now we have all the variables we need in the format we need. We will make some tables to summarize the data. Using the data for Wave 4: Create a table showing the breakdown of each country’s population according to employment status, with country (S003) as the row variable, and employment status (X028) as the column variable. Express the values as percentages of the row total rather than frequencies. Discuss any differences or similarities between countries that you find interesting. Hint: For help on creating this table, see Google Sheets walk-through 3.1. To change the values in the table to percentages of the row total, under ‘Values’, change the option in ‘Show as’ from ‘Default’ to ‘% of row’. Create a new table as shown in Figure 8.4 (similar to Table 1 in the study ‘Employment status and subjective well-being’) and fill in the missing values. Life satisfaction is measured by A170. Summary tables such as these give a useful overview of what each variable looks like. Male Female Mean Standard deviation Mean Standard deviation Life satisfaction Self-reported health Work ethic Age Education Number of children Summary statistics by gender, European Values Study. Figure 8.4 Summary statistics by gender, European Values Study. Part 8.2 Visualizing the data Note You will need to have done Questions 1–5 in Part 8.1 before doing this part. Learning objectives for this part use column charts, line charts, and scatterplots to visualize data calculate and interpret correlation coefficients. We will now create some summary charts of the self-reported measures (work ethic and life satisfaction), starting with column charts to show the distributions of values. Along with employment status, these are the main variables of interest, so it is important to look at them carefully before doing further data analysis. The distribution of work ethic and life satisfaction may vary across countries but may also change over time within a country, especially since the surveys are conducted around once a decade. To compare distributions for a particular country over time, we have to use the same horizontal axis, so we will first need to make a separate frequency table for each distribution of interest (survey wave). Also, since the number of people surveyed in each wave may differ, we will use percentages instead of frequencies as the vertical axis variable. Using the data from Wave 3 and Wave 4 only, for three countries of your choice: Using the work ethic measure, create a separate frequency table for Wave 3 and for Wave 4, similar to Figure 8.5. The column ‘Percentage of individuals (%)’ refers to the number of individuals in that wave. The values in the first column should range from 1 to 5, in intervals of 0.2. (Hint: To count observations only for a particular country, you will need to use the IF function and FREQUENCY function together. See Google Sheets walk-through 6.1 for help on how to do this). Range of work ethic score Frequency Percentage of individuals (%) 1.00 1.20 … 4.80 5.00 Frequency table for self-reported work ethic. Figure 8.5 Frequency table for self-reported work ethic. Plot a separate column chart for each country to show the distribution of work ethic scores in Wave 3, with the percentage of individuals on the vertical axis and the range of work ethic score on the horizontal axis. On each chart, plot the distribution of scores in Wave 4 on top of the Wave 3 distribution. (See Google Sheets walk-through 6.2 in Empirical Project 6 for guidance on what your charts should look like). Based on your charts from 1(b), does it appear that attitudes towards work in each country of your choice have changed over time? (Hint: For example, look at where the distribution is centred, the percentages of observations on the left tail or the right tail of the distribution, and how spread out the data is.) We will use line charts to make a similar comparison for life satisfaction (A170) over time. Using data for countries that are present in Waves 1 to 4: Create a table showing the average life satisfaction, by wave (column variable) and by country (row variable). (Hint: You may find it easier to make four separate pivot tables and copy-paste the results into a new table). Plot a line chart with wave number (1 to 4) on the horizontal axis and average life satisfaction on the vertical axis. Make sure to include a legend. From your results in 2(a) and (b), how has the distribution of life satisfaction changed over time? What other information about the distribution of life satisfaction could we use to supplement these results? Choose one or two countries and research events that could explain the observed changes in average life satisfaction over time shown in 2(a) and (b). correlation coefficientA numerical measure, ranging between 1 and −1, of how closely associated two variables are—whether they tend to rise and fall together, or move in opposite directions. A positive coefficient indicates that when one variable takes a high (low) value, the other tends to be high (low) too, and a negative coefficient indicates that when one variable is high the other is likely to be low. A value of 1 or −1 indicates that knowing the value of one of the variables would allow you to perfectly predict the value of the other. A value of 0 indicates that knowing one of the variables provides no information about the value of the other. After describing patterns in our main variables over time, we will use correlation coefficients and scatterplots to look at the relationship between these variables and the other variables in our dataset. Using the Wave 4 data: Create a table as shown in Figure 8.6 and calculate the required correlation coefficients. For employment status and gender, you will need to create new variables: full-time employment should be equal to 1 if full-time employed and 0 if unemployed, and treated as missing data (left as a blank cell) otherwise. Gender should be 0 if male and 1 if female. Variable Life satisfaction Work ethic Age Education Full-time employment Gender Self-reported health Income Number of children Relative income Life satisfaction 1 Work ethic 1 Correlation between life satisfaction, work ethic and other variables, Wave 4. Figure 8.6 Correlation between life satisfaction, work ethic and other variables, Wave 4. Interpret the coefficients, paying close attention to how the variables are coded. (For example, you could comment on the absolute magnitude and sign of the coefficients). Explain whether the relationships implied by the coefficients are what you expected (for example, would you expect life satisfaction to increase or decrease with health, income, etc.) Next, we will look at the relationship between employment status and life satisfaction and investigate the paper’s hypothesis that this relationship varies with the average work ethic in a country. Using the data from Wave 4, carry out the following: Create a table showing the average life satisfaction according to employment status (showing the full-time employed, retired, and unemployed categories only), with country (S003) as the row variable, and employment status (X028) as the column variable. Comment on any differences in average life satisfaction between these three groups, and whether social norms is a plausible explanation for these differences. Use the table from 4(a) to calculate the difference in average life satisfaction (full-time employed minus unemployed, and full-time employed minus retired). Make a separate scatterplot for each of these differences in life satisfaction, with average work ethic on the horizontal axis and difference in life satisfaction on the vertical axis. For each difference (employed vs unemployed, employed vs retired), calculate and interpret the correlation coefficient between average work ethic and difference in life satisfaction. So far we have described the data using tables and charts, but have not made any statements about whether what we observe is consistent with an assumption that work ethic has no bearing on differences in life satisfaction between different subgroups. In the next part, we will assess the relationship between employment status and life satisfaction and assess whether the observed data leads us to reject the above assumption. Part 8.3 Confidence intervals for difference in the mean Note You will need to have done Questions 1–5 in Part 8.1 before doing this part. Part 8.2 is not necessary but is helpful to get an idea of what the data looks like. Learning objectives for this part calculate and interpret confidence intervals for the difference in means between two groups. The aim of this project was to look at the empirical relationship between employment and life satisfaction. When we calculate differences between groups, we collect evidence which may or may not support a hypothesis that life satisfaction is identical between different subgroups. Economists often call this testing for statistical significance. In Part 6.2 of Empirical Project 6, we constructed 95% confidence intervals for means, and used a rule of thumb to decide whether we could maintain the assumption that the two subgroups were really identical. Now we will learn how to construct confidence intervals for the difference in two means, which allows us to make such a judgment on the basis of a single confidence interval. Remember that the width of a confidence interval depends on the standard deviation and number of observations. (Read Part 6.2 of Empirical Project 6 to understand why.) When making a confidence interval for a sample mean (such as the mean life satisfaction of the unemployed), we use the standard deviation and number of observations in that sample (unemployed people) to obtain the standard error of the sample mean. text{Standard error of sample mean (SE)} = frac{text{SD of sample data}}{sqrt{text{number of observations (n)}}} When we look at the difference in means (such as life satisfaction of employed minus unemployed), we are using data from two groups (the unemployed and the employed) to make one confidence interval, so the number of observations is the sum of observations across both groups. To calculate the standard deviation for the difference in means, we use the standard deviations (SD) of each group: text{SD (difference in means)} = sqrt{(text{SD of group 1})^2 + (text{SD of group 2})^2} This formula requires the two groups of data to be independent, meaning that the data from one group is not related, paired, or matched with data from the other group. This assumption is reasonable for the life satisfaction data we are using. However, if the two groups of data are not independent, for example if the same people generated both groups of data (as in the public goods experiments of Empirical Project 2), then we cannot use this formula. Once we have the new standard deviation and number of observations, we can calculate the width of the confidence interval (distance from the mean to one end of the interval) as before. The width of the confidence interval tells us how precisely the difference in means was estimated, and this precision tells us how confident we can be that the difference is not due to chance. If the value 0 falls outside the 95% confidence interval, this is strong evidence of a real difference in means. For example, if the estimated difference is positive, and the 95% confidence interval is not wide enough to include 0, we can say with 95% confidence that the true difference is positive too. In other words, it tells us that the p-value for the difference we have found is less than 0.05, so we would be very unlikely to find such a big difference by chance. In Figure 8.7, for Great Britain, we can say with 95% confidence that the true difference in average life satisfaction between the full-time employed and the unemployed is positive. However, for Spain, we do not have strong evidence of a real difference in mean life satisfaction. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-08-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''95% confidence intervals for the average difference in life satisfaction (full-time employed minus unemployed) in Great Britain and Spain. '' /> 95% confidence intervals for the average difference in life satisfaction (full-time employed minus unemployed) in Great Britain and Spain. Figure 8.7 95% confidence intervals for the average difference in life satisfaction (full-time employed minus unemployed) in Great Britain and Spain. We will apply this method to make confidence intervals for differences in life satisfaction. Choose three countries: one with an average work ethic in the top third of scores, one in the middle third, and one in the lower third. (See Google Sheets walk-through 6.4 for help on calculating confidence intervals and adding them to a chart.) Create a pivot table for these countries, showing the average life satisfaction score, standard deviation of life satisfaction, and number of observations, with country (S003) as the row variable, and employment status (full-time employed, retired, and unemployed only) as the column variable. Use your pivot table from 1(a) to calculate the difference in means (full-time employed minus unemployed, and full-time employed minus retired), the standard deviation of these differences, and the number of observations. Use Google Sheets’ CONFIDENCE.T function and the calculated values in 1(b) to determine the 95% confidence interval width (distance between the mean and one end of the interval) of the difference in means. Plot a column chart for your chosen countries showing the difference in life satisfaction (employed vs unemployed and employed vs retired) on the vertical axis, and country on the horizontal axis (sorted according to low, medium, and high work ethic). Add the confidence intervals from 1(c) to your chart. Interpret your findings from Question 1(d), commenting on the size of the observed differences in means, and the precision of your estimates. The method we used to compare life satisfaction relied on making comparisons between people with different employment statuses, but a person’s employment status is not entirely random. We cannot therefore make causal statements such as ‘being unemployed causes life satisfaction to decrease’. Describe how we could use the following methods to assess better the effect of being unemployed on life satisfaction, and make some statements about causality: a natural experiment panel data (data on the same individuals, taken at different points in time)."
});
index.addDoc({
    id: 46,
    title: "Doing Economics: Empirical Project 8 Solutions",
    content: "Empirical Project 8 Solutions These are not model answers. They are provided to help students, including those doing the project outside a formal class, to check their progress while working through the questions using the Excel, R, or Google Sheets walk-throughs. There are also brief notes for the more interpretive questions. Students taking courses using Doing Economics should follow the guidance of their instructors. Part 8.1 Cleaning and summarizing the data No solution is provided. One way to rename the variables is given in Solution figure 8.1. Variable New name Variable description S002EV EVS-wave EVS-wave S003 Country/region Country/region S006 Respondent number Original respondent number S009 Country abbreviation Country abbreviation A009 Health State of health (subjective) A170 Life satisfaction Satisfaction with your life C036 Work Q1 To develop talents you need to have a job C037 Work Q2 Humiliating to receive money without having to work for it C038 Work Q3 People who don’t work become lazy C039 Work Q4 Work is a duty towards society C041 Work Q5 Work should come first even if it means less spare time X001 Sex Sex X003 Age Age X007 Marital status Marital status X011_01 Number of children How many children you have—deceased children not included X025A Education Educational level respondent: ISCED-code one digit X028 Employment Employment status X047D Monthly household income Monthly household income (× 1,000), corrected for ppp in euros Completed data dictionary. Solution figure 8.1 Completed data dictionary. Examples of answers on how some of the variables were measured are provided: To use reported life satisfaction in interpersonal and cross-country comparisons, we need to assume that all individuals share similar presuppositions and answer the questions in similar environments and contexts. These assumptions are not entirely plausible. Perceived social norms, beliefs, culture, and many other characteristics vary across individuals and across countries, affecting individuals’ responses. As a result, differences in answers may reflect differences in these factors rather than the true level of satisfaction. There are a large number of environmental factors that can influence the answers. The day of the week, the weather, and major news stories are all examples of such factors. It is impossible to control for all these factors. Although the measure has limitations, it may be the best we can obtain. Employment status: Social norms, beliefs, culture, and other factors which vary across individuals and countries can affect people’s willingness to report truthfully on their employment status. Many people probably do not know accurately the technical definitions of the terms associated with employment status, resulting in misreports. Misreporting of employment status is therefore likely to be an issue. An example answer is given below. No-opinion responding: A tendency to select the response category that is most neutral in its meaning (for example ‘neither agree nor disagree’). Some people may consider the information being asked for as private and therefore will not want to give opinions. We can check for respondents who choose neutral answers for most questions. Primacy effects: A tendency to select one of the first response categories presented on a list. When people want to spend minimal time on a questionnaire, they may just choose the first category given for each question. We can check for respondents who choose the first category for most questions. We can include in the questionnaire similar questions with the categories ordered differently so that if individuals select the first categories even for similar questions, we would know that they are probably not paying attention. Socially desirable responding: Conscious or subconscious tendency to select response options more likely to conform with social norms or present the respondent in a good light. The answers reveal the values, attitudes, and beliefs of the respondents. Respondents may engage in socially desirable responding because they are afraid of the risk that their employers or potential employers may learn about their answers and penalize them. We can identify a set of answers complying with social norms in any given region and identify respondents who chose almost the same set of answers. (a)–(e) No solution is provided. For Excel users: Refer to Excel walk-through 8.1 for guidance on answering Question 3(a) and (e). For R users: Refer to R walk-through 8.2. For Google Sheets users: Refer to Google Sheets walk-through 8.1. No solution is provided. No solution is provided. The long right tail means the mean is skewed to the right. Using deviations from the average income in this case would underestimate the living standards of many people. No solution is provided. For Excel users: Refer to Excel walk-through 8.3 for guidance, and for an example of naming a new variable. For R users: Refer to R walk-through 8.4. For Google Sheets users: Refer to Google sheets walk-through 8.3. The table in Solution figure 8.2 shows the breakdown of each country’s population according to employment status. There are many possible features to comment on, for example: the differences in percentages of students surveyed the percentages in full-time employment, part-time employment, and self-employment, making a comparison between developing and developed countries variations in unemployment across countries. Country Full-time Housewife Other Part-time Retired Self-employed Students Unemployed Albania 29.42 7.42 1.50 5.50 9.08 22.08 7.33 17.67 Armenia 23.86 20.92 1.14 8.09 18.38 5.96 6.70 14.95 Austria 39.80 7.24 1.89 9.95 25.49 5.02 8.39 2.22 Belarus 57.88 2.43 1.21 6.95 18.59 3.40 6.87 2.67 Belgium 42.89 5.96 3.72 8.94 23.01 3.57 5.21 6.70 Bosnia Herzegovina 34.06 9.33 0.82 2.90 14.67 3.08 8.15 26.99 Bulgaria 46.32 2.62 0.76 2.79 31.28 5.58 2.37 8.28 Croatia 41.58 3.37 0.93 2.78 26.01 2.86 8.75 13.72 Cyprus 46.32 13.68 1.29 2.84 24.39 6.58 1.68 3.23 Czech Republic 46.56 3.06 4.66 1.68 31.27 3.82 5.43 3.52 Denmark 55.89 0.28 1.32 6.69 24.32 5.94 4.15 1.41 Estonia 50.35 4.08 2.20 5.11 28.52 3.61 3.38 2.75 Finland 52.34 1.38 3.94 5.11 22.77 6.17 3.72 4.57 France 46.83 5.59 1.94 6.04 28.78 2.76 3.13 4.92 Georgia 19.46 11.60 0.81 6.57 19.38 7.06 2.60 32.52 Germany 38.44 4.58 3.03 8.44 28.64 2.97 2.67 11.23 Great Britain 33.50 7.32 4.01 11.23 28.99 5.72 1.40 7.82 Greece 28.49 17.42 0.40 2.97 26.73 13.72 6.18 4.09 Hungary 46.39 1.20 7.21 2.00 24.04 3.53 6.57 9.05 Iceland 54.50 2.25 6.01 9.91 7.06 11.41 4.95 3.90 Ireland 41.87 19.84 1.59 9.72 13.89 4.96 1.59 6.55 Italy 32.88 8.33 0.46 9.13 23.06 13.70 7.08 5.37 Kosovo 19.57 11.65 0.52 5.23 5.83 9.41 18.00 29.80 Latvia 52.05 6.18 2.34 3.76 23.22 3.26 4.26 4.93 Lithuania 50.13 4.11 2.89 5.16 23.97 3.41 6.04 4.29 Luxembourg 51.33 9.36 1.12 7.38 15.36 3.00 9.87 2.58 Macedonia 35.74 4.34 1.24 1.71 16.74 3.72 8.53 27.98 Malta 33.84 32.33 0.68 3.84 23.42 2.33 0.55 3.01 Moldova 30.49 7.24 1.87 7.58 25.64 4.86 4.43 17.89 Montenegro 39.02 4.63 0.60 2.14 16.64 4.97 4.80 27.19 Netherlands 32.40 9.52 3.68 18.24 27.76 6.48 0.80 1.12 Northern Cyprus 31.19 19.55 2.23 5.20 8.91 8.91 13.61 10.40 Northern Ireland 30.10 10.36 4.53 8.74 29.45 3.56 1.29 11.97 Norway 53.23 2.22 6.55 9.48 12.60 8.17 7.06 0.71 Poland 41.81 6.00 0.10 3.14 28.00 5.81 7.62 7.52 Portugal 46.20 5.24 1.57 3.27 33.51 1.70 1.05 7.46 Romania 41.07 10.54 1.95 3.22 33.95 3.02 3.61 2.63 Russian Federation 54.36 5.81 2.72 5.08 23.77 1.27 2.72 4.26 Serbia 34.21 5.02 1.07 2.38 25.16 6.91 4.11 21.13 Slovakia 40.98 1.73 4.89 2.21 39.73 3.55 1.25 5.66 Slovenia 47.44 2.50 2.75 1.25 31.59 4.37 6.99 3.12 Spain 41.52 16.30 0.11 4.63 19.93 6.28 3.19 8.04 Sweden 54.82 0.38 6.60 7.36 15.36 7.23 4.06 4.19 Switzerland 48.50 6.42 3.21 14.03 21.31 2.89 1.39 2.25 Turkey 16.42 42.39 0.60 2.14 10.00 7.66 5.92 14.88 Ukraine 40.92 6.79 1.02 4.84 32.09 4.41 3.23 6.71 Self-reported employment status in each country (per cent of sample). Solution figure 8.2 Self-reported employment status in each country (per cent of sample). Solution figure 8.3 shows the summary table. Male Female Mean Standard deviation Mean Standard deviation Life satisfaction 6.98 2.30 6.98 2.30 Self-reported health 3.68 0.95 3.68 0.95 Work ethic 3.68 0.76 3.68 0.76 Age 47.10 17.42 47.09 17.42 Education 3.09 1.36 3.09 1.36 Number of children 1.63 1.41 1.63 1.41 A summary table for the EVS data. Solution figure 8.3 A summary table for the EVS data. Part 8.2 Visualizing the data Germany is used as an example. Solution figures 8.4 and 8.5 provide frequency tables for work ethic in Germany for Wave 3 and Wave 4. Range of work ethic score Frequency Percentage of individuals (%) 1.00 0 0.00 1.20 3 0.21 1.40 6 0.42 1.60 9 0.63 1.80 15 1.05 2.00 18 1.26 2.20 21 1.47 2.40 47 3.28 2.60 68 4.75 2.80 79 5.52 3.00 114 7.96 3.20 130 9.08 3.40 166 11.59 3.60 171 11.94 3.80 185 12.92 4.00 164 11.45 4.20 106 7.40 4.40 55 3.84 4.60 34 2.37 4.80 20 1.40 5.00 21 1.47 Frequency table for work ethic (Germany, Wave 3). Solution figure 8.4 Frequency table for work ethic (Germany, Wave 3). Range of work ethic score Frequency Percentage of individuals (%) 1.0 1 0.06 1.2 1 0.06 1.4 0 0.00 1.6 6 0.36 1.8 9 0.53 2.0 18 1.07 2.2 22 1.31 2.4 35 2.08 2.6 44 2.61 2.8 75 4.46 3.0 90 5.35 3.2 125 7.43 3.4 152 9.03 3.6 171 10.16 3.8 180 10.70 4.0 207 12.30 4.2 191 11.35 4.4 166 9.86 4.6 79 4.69 4.8 37 2.20 5.0 74 4.40 Frequency table for work ethic (Germany, Wave 4). Solution figure 8.5 Frequency table for work ethic (Germany, Wave 4). Solution figure 8.6 shows the column chart for Germany for Waves 3 and 4. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Distribution of work ethic score in Germany: Waves 3 and 4. '' /> Distribution of work ethic score in Germany: Waves 3 and 4. Solution figure 8.6 Distribution of work ethic score in Germany: Waves 3 and 4. The work ethic scores have increased over time, as shown by the rightward shift in the distribution in Wave 4. Solution figure 8.7 shows the average life satisfaction in Waves 1 to 4. Country Wave 1 Wave 2 Wave 3 Wave 4 Belgium 7.37 7.60 7.42 7.63 Denmark 8.21 8.17 8.31 8.41 France 6.71 6.77 6.98 7.05 Germany 7.22 7.03 7.43 6.77 Iceland 8.05 8.01 8.08 8.07 Italy 6.65 7.30 7.18 7.40 Netherlands 7.75 7.77 7.83 7.99 Northern Ireland 7.66 7.88 8.07 7.82 Spain 6.60 7.15 6.97 7.29 Sweden 8.03 7.99 7.62 7.68 Average life satisfaction across countries and survey waves. Solution figure 8.7 Average life satisfaction across countries and survey waves. Solution figure 8.8 provides the line chart of average life satisfaction across countries and survey waves. Note: When producing a chart with many lines, the key produced by Excel/R/Google Sheets is often inadequate in allowing the reader to distinguish between the series. You may need to add labels to the series. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-08.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-08-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-08-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-08-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-08.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Line chart of average life satisfaction (wellbeing) across countries and survey waves. '' /> Line chart of average life satisfaction (wellbeing) across countries and survey waves. Solution figure 8.8 Line chart of average life satisfaction (wellbeing) across countries and survey waves. Average life satisfaction increased slightly over the survey periods for most countries. Also, instead of looking at averages, we could look at deciles, which would allow us to assess whether the changes in average life satisfaction are due to a shift in the entire distribution or an increase in reporting of extreme values. The answer depends on your chosen country/countries. Solution figure 8.9 shows the correlation coefficients. Variable life satisfaction Work ethic Age –0.08 0.13 Education 0.09 –0.15 Full-time employment 0.18 –0.03 Gender (= 0 if male, = 1 if female) –0.02 –0.05 Self-reported health 0.38 –0.07 Income 0.24 –0.15 Number of children –0.02 0.09 Relative income 0.19 –0.05 Life satisfaction 1.00 –0.03 Work ethic –0.03 1.00 Correlation between life satisfaction, work ethic and other variables. Solution figure 8.9 Correlation between life satisfaction, work ethic and other variables. The correlation coefficient, ranged between –1 and 1, measures the strength and direction of a linear relationship between two variables. A negative coefficient means the two variables are negatively linearly correlated, and a positive coefficient means the two variables are positively linearly correlated. The coefficient of 0.18 between employ­ment status and life satisfaction, for example, means that people who are full-time employed tend to be more satisfied with their lives relative to those who are unemployed. The coefficients between life satisfaction and other variables have the expected sign. The results that work ethic is negatively linearly related to education, employment status, and relative income, however, are somewhat surprising. Solution figure 8.10 shows average life satisfaction according to employment status. Employed respondents are more satisfied with their lives compared with the unemployed. The retired, like the unemployed, do not work, but they do not suffer nearly as large a drop in life satisfaction. This suggests that social norms that discriminate against the unemployed is a plausible explanation for the relatively low life satisfaction of the unemployed. Average life satisfaction Employment status Country/Region Full-time Retired Unemployed Albania 6.63 5.81 6.07 Armenia 6.04 4.85 5.46 Austria 7.44 7.74 6.07 Belarus 6.10 5.62 5.61 Belgium 7.72 7.83 6.37 Bosnia Herzegovina 7.33 7.01 6.77 Bulgaria 6.18 4.97 4.69 Croatia 7.31 6.48 7.17 Cyprus 7.38 7.03 6.56 Czech Republic 7.30 6.89 6.07 Denmark 8.54 8.21 7.20 Estonia 6.93 6.25 4.97 Finland 7.82 8.02 5.77 France 7.20 6.97 6.24 Georgia 6.12 4.69 5.42 Germany 7.26 6.85 4.61 Great Britain 7.53 7.93 6.03 Greece 7.15 6.62 5.98 Hungary 6.65 5.89 4.86 Iceland 8.20 8.45 7.23 Ireland 7.90 7.83 7.18 Italy 7.43 7.44 6.60 Kosovo 6.30 6.04 6.78 Latvia 6.52 5.93 5.31 Lithuania 6.59 5.63 4.53 Luxembourg 7.87 8.24 5.50 Macedonia 7.19 6.67 6.61 Malta 7.70 7.76 5.95 Moldova 7.12 5.98 6.07 Montenegro 7.63 7.22 7.47 Netherlands 8.04 7.95 7.00 Northern Cyprus 6.74 6.44 5.64 Northern Ireland 7.68 7.77 7.54 Norway 8.19 8.26 8.00 Poland 7.46 6.57 7.06 Portugal 6.84 5.88 5.44 Romania 7.14 6.57 7.41 Russian Federation 6.86 5.70 6.40 Serbia 7.17 6.67 6.73 Slovakia 7.47 6.71 6.12 Slovenia 7.83 7.13 6.76 Spain 7.34 7.21 7.19 Sweden 7.88 8.17 6.52 Switzerland 8.04 8.12 5.76 Turkey 6.50 6.61 5.76 Ukraine 6.34 5.44 4.95 Average life satisfaction according to employment status and country. Solution figure 8.10 Average life satisfaction according to employment status and country. The calculation of the difference in average life satisfaction is provided in Solution figure 8.11. Country/Region Difference between full-time employed and unemployed Difference between full-time employed and retired Albania 0.57 0.83 Armenia 0.58 1.18 Austria 1.36 –0.31 Belarus 0.49 0.48 Belgium 1.35 –0.12 Bosnia Herzegovina 0.56 0.33 Bulgaria 1.48 1.20 Croatia 0.14 0.83 Cyprus 0.82 0.35 Czech Republic 1.24 0.42 Denmark 1.34 0.34 Estonia 1.95 0.67 Finland 2.05 –0.20 France 0.96 0.23 Georgia 0.70 1.43 Germany 2.65 0.41 Great Britain 1.51 –0.40 Greece 1.17 0.53 Hungary 1.79 0.76 Iceland 0.97 –0.24 Ireland 0.71 0.07 Italy 0.83 –0.01 Kosovo –0.49 0.26 Latvia 1.21 0.59 Lithuania 2.06 0.96 Luxembourg 2.37 –0.37 Macedonia 0.58 0.52 Malta 1.75 –0.06 Moldova 1.05 1.14 Montenegro 0.16 0.42 Netherlands 1.04 0.09 Northern Cyprus 1.10 0.29 Northern Ireland 0.14 –0.09 Norway 0.19 –0.07 Poland 0.40 0.89 Portugal 1.40 0.96 Romania –0.27 0.56 Russian Federation 0.46 1.16 Serbia 0.44 0.50 Slovakia 1.35 0.76 Slovenia 1.07 0.70 Spain 0.15 0.13 Sweden 1.36 –0.30 Switzerland 2.28 –0.08 Turkey 0.74 –0.11 Ukraine 1.40 0.90 Difference in average life satisfaction: full-time employed minus unemployed, and full-time employed minus retired. Solution figure 8.11 Difference in average life satisfaction: full-time employed minus unemployed, and full-time employed minus retired. Solution figures 8.12 and 8.13 provide column charts showing both differences in life satisfaction. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-12.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-12-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-12-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-12-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-12.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Difference in life satisfaction between the full-time employed and the unemployed vs average work ethic. '' /> Difference in life satisfaction between the full-time employed and the unemployed vs average work ethic. Solution figure 8.12 Difference in life satisfaction between the full-time employed and the unemployed vs average work ethic. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-13.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-13-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-13-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-13-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-13.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Difference in life satisfaction between the full-time employed and the retired vs average work ethic. '' /> Difference in life satisfaction between the full-time employed and the retired vs average work ethic. Solution figure 8.13 Difference in life satisfaction between the full-time employed and the retired vs average work ethic. The correlation coefficient for full-time employed vs unemployed is -0.158, which indicates a weak negative correlation. Looking at the scatterplot (Solution figure 8.12), it is difficult to see a clear relationship between the two variables. The correlation coefficient for full-time employed vs retired is 0.4843, which indicates a moderate positive correlation (the gap in life satisfaction between employed and retired is positive and wider in countries with higher average work ethic). Part 8.3 Confidence intervals for difference in the mean Choice of countries for this example: top third: Turkey middle third: Spain lower third: Great Britain. The table for our chosen countries is as shown in Solution figure 8.14. Full-time Retired Unemployed Country Average SD Count Average SD Count Average SD Count Great Britain 7.53 1.85 334 7.93 1.98 289 6.03 2.19 78 Spain 7.34 1.70 377 7.21 1.93 181 7.19 2.14 73 Turkey 6.50 2.55 330 6.61 2.60 201 5.76 3.13 299 Summary table of life satisfaction, by employment status. Solution figure 8.14 Summary table of life satisfaction, by employment status. The calculations are provided in Solution figures 8.15 and 8.16. Country Difference in means SD of difference in means Number of observations Great Britain –0.40 2.71 623 Spain 0.13 2.58 558 Turkey –0.11 3.64 531 Calculated values for differences in life satisfaction (full-time vs retired). Solution figure 8.15 Calculated values for differences in life satisfaction (full-time vs retired). Country Difference in means SD of difference in means Number of observations Great Britain 1.51 2.86 412 Spain 0.15 2.73 450 Turkey 0.74 4.04 629 Calculated values for differences in life satisfaction (full-time vs unemployed). Solution figure 8.16 Calculated values for differences in life satisfaction (full-time vs unemployed). The calculations are provided in Solution figures 8.17 and 8.18. Country Difference in means SD of difference in means Number of observations CI distance to sample mean Total width of CI Great Britain –0.40 2.71 623.00 0.21 0.43 Spain 0.13 2.58 558.00 0.21 0.43 Turkey –0.11 3.64 531.00 0.31 0.62 Calculated width of 95% confidence interval for differences in life satisfaction (full-time vs retired). Solution figure 8.17 Calculated width of 95% confidence interval for differences in life satisfaction (full-time vs retired). Country Difference in means SD of difference in means Number of observations CI distance to mean Total width of CI Great Britain 1.51 2.86 412.00 0.28 0.55 Spain 0.15 2.73 450.00 0.25 0.51 Turkey 0.74 4.04 629.00 0.32 0.63 Calculated width of 95% confidence interval for differences in life satisfaction (full-time vs unemployed). Solution figure 8.18 Calculated width of 95% confidence interval for differences in life satisfaction (full-time vs unemployed). Solution figures 8.19 and 8.20 provide column charts for the example countries showing the difference in life satisfaction and confidence intervals. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-19.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-19-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-19-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-19-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-19.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Difference in life satisfaction (wellbeing): full-time and retired. '' /> Difference in life satisfaction (wellbeing): full-time and retired. Solution figure 8.19 Difference in life satisfaction (wellbeing): full-time and retired. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-20.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-20-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-20-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-20-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-08-20.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Difference in life satisfaction (wellbeing): full-time and unemployed. '' /> Difference in life satisfaction (wellbeing): full-time and unemployed. Solution figure 8.20 Difference in life satisfaction (wellbeing): full-time and unemployed. For full-time and retired: In Great Britain, the retired have higher life satisfaction than full-time workers on average (0.4 points higher on a 1–5 scale), and this difference has been estimated with reasonable precision, so we can be fairly confident that this difference is unlikely to have occurred if really there was no difference in wellbeing between the two groups. In Turkey, the retired have slightly higher life satisfaction but this difference is both small (0.1 points) and not precisely estimated (the confidence interval is quite wide), so it is well possible that the difference we observed is consistent with there actually being no difference between the two groups. The same conclusions can be drawn for Spain, except that full-time workers have a slightly higher life satisfaction than the unemployed on average. For full-time and unemployed: In all three countries, full-time workers have higher life satisfaction than the unemployed on average. The difference is largest for Great Britain (1.5 points on a 1–5 scale, which is considerable), and is precisely estimated, so we can be fairly confident that this difference is unlikely to have occurred if really there was no difference in wellbeing between the two groups. While we find a smaller difference for Turkey, it is estimated with precision and we come to the same conclusion as for Great Britain. For Spain, the difference is small and not very precisely estimated, so it is well possible that the difference we observed is consistent with there actually being no difference between the two groups. A natural experiment can control for variables that affect both the dependent and independent variables. The method allows us to isolate the effect of employment status on life satisfaction. The ceteris paribus effect of employment status on life satisfaction, if obtained, has a causal interpretation. If we have data on the same individual over time, then we can calculate the difference in life satisfaction for the same person under different employment statuses, instead of comparing different individuals (as we did in this project)."
});
index.addDoc({
    id: 47,
    title: "Doing Economics: Empirical Project 9: Credit-excluded households in a developing country",
    content: "Empirical Project 9 Credit-excluded households in a developing country Learning objectives In this project you will: identify credit constrained and credit-excluded households using survey information (Part 9.1) create dummy (indicator) variables (Part 9.1) compare characteristics of successful borrowers, discouraged borrowers, credit constrained households, and credit-excluded households (Part 9.1) explain why selection bias is an important issue (Part 9.1) analyse the characteristics of loans obtained by successful borrowers (Part 9.2). Key concepts Concepts needed for this project: mean, standard deviation, range, percentile, correlation/correlation coefficient, confidence interval for the difference in means, dummy variable. Concepts introduced in this project: selection bias. Introduction CORE projects This empirical project is related to material in: Unit 9 of Economy, Society, and Public Policy Unit 10 of The Economy. Borrowing, either through formal or informal institutions, can help smooth consumption and also enable investment. However, the lenders cannot observe how hard the borrower is working to repay the loan, and cannot make the loan contract conditional on such effort. The relationship between the lender and the borrower, like that between the employer and the employer studied in Project 8, is called a principal–agent problem. Section 9.12 of Economy, Society, and Public Policy compares the labour market and the credit market. principal–agent relationshipThis is an asymmetrical relationship in which one party (the principal) benefits from some action or attribute of the other party (the agent) about which the principal’s information is not sufficient to enforce in a complete contract. See also: incomplete contract. Also known as: principal–agent problem.credit constrainedA description of individuals who are able to borrow only on unfavourable terms. See also: credit excluded.credit excludedA description of individuals who are unable to borrow on any terms. See also: credit constrained. Lenders can partially mitigate or eliminate the risk of default by requiring collateral, which can be repossessed and sold to repay the loan if the borrower defaults. People who are unable to provide this collateral often have to borrow under more unfavourable conditions (higher interest rates) or may be refused a loan entirely. We call the former group credit constrained, and the latter group credit excluded. Since the ability to borrow depends on a person’s wealth, such credit constraints and exclusion contribute to inequality, and some opportunities for economic growth are not realized. For example, a hard-working person without any assets may be refused a loan to start a business, which could contribute both to raising their income and to economic activity. To design policies that help credit markets function better, we first need to look at how widely borrowing conditions and available sources of finance differ according to household characteristics. Sometimes borrowers who are excluded from formal credit markets can still obtain loans through other lenders such as relatives or friends, so it is important to consider these sources when classifying people as credit constrained or excluded. In countries where formal credit markets are still developing, informal arrangements are important ways for communities to share resources and pool risks. For example, in Ethiopia, households may be part of a social support network called an ‘iddir’, a group of people who regularly pay cash into a common pool that is shared among group members who need it. We will be looking at data from an Ethiopian household survey (the Ethiopian Socioeconomic Survey) to investigate the borrowing conditions that different types of households face. Aside from credit constrained and credit-excluded households, we will also look at a third group of households that are ‘discouraged borrowers’, meaning that they did not apply for a loan because they thought they would be refused. Working in Excel Working in R Working in Google Sheets"
});
index.addDoc({
    id: 48,
    title: "Doing Economics: Empirical Project 9: Working in Excel",
    content: "Empirical Project 9 Working in Excel Excel-specific learning objectives In addition to the learning objectives for this project, in this section you will learn how to create and format time variables. Part 9.1 Households that did not get a loan Learning objectives for this part identify credit-constrained and credit-excluded households using survey information create dummy (indicator) variables compare characteristics of successful borrowers, discouraged borrowers, credit-constrained households, and credit-excluded households explain why selection bias is an important issue. The Ethiopian Socioeconomic Survey (ESS) data was collected in 2013–14 from a nationally representative sample of households. Households were asked about topics such as their housing conditions, assets, and access to credit. Download the ESS data and survey questionnaire: Download the ESS data.The Excel file contains three tabs (‘Data dictionary’, ‘All households’, and ‘Got loan’). Read the Data dictionary tab and make sure you know what each variable represents. (Later we will discuss exactly how some of these variables were constructed.) For the documentation, go to the data download site. Click on the ‘Documentation’ tab in the middle of the page. Under the heading ‘Questionnaires’, download the PDF file called ‘2013–2014 Ethiopian Socioeconomic Survey, Household Questionnaire’ by clicking the ‘Download’ button on the right-hand side of the page. You may find it helpful to refer to Section 14 of the questionnaire for the exact questions asked about credit and saving. The data is already in a format clean enough to use, so we will begin by summarizing the information in the ‘All households’ tab, starting with region and household characteristics. Create a pivot table showing the proportion of households that lived in each region and area type, with ‘region’ as the row variable and ‘rural’ as the column variable. (For help on creating pivot tables, see Excel walk-through 3.1.) dummy variable (indicator variable)A variable that takes the value 1 if a certain condition is met, and 0 otherwise. Use Excel’s IF function to create a variable that equals 1 if the household head was female, and 0 if the household head was male. Variables that are coded in this way are known as dummy variables (or indicator variables). What percentage of household heads were female? Create an appropriate summary table for the variables ‘hhsize’, ‘gender’, ‘age’, ‘young_children’, ‘working_age_adults’, ‘max_education’, and ‘number_assets’. (You may find it helpful to refer to Figure 2.9 in Empirical Project 2 for one possible format to use.) Write a short paragraph describing the information in your tables for 1(c). Now that we have an idea of what our data looks like, we will move on to identify households that are potentially excluded from the credit market or are credit constrained. The former are households who find it impossible to borrow and the latter are households who can only borrow on unfavourable terms (see Section 9.10 of Economy, Society, and Public Policy). The variables in our dataset that are related to this issue are ‘did_not_apply’ and ‘loan_rejected’. We will soon also look at the responses given in the variables ‘reason_not_apply1’ and ‘reason_not_apply2’. Using the ‘All households’ tab: Create a frequency table with ‘did_not_apply’ as the row variable and ‘loan_rejected’ as the column variable. Include the blanks as a separate row. Looking at these two variables, explain why some observations should be excluded and remove them from the dataset. Also remove all households with missing information for one or more of these variables. Of the non-excluded observations, what percentage of households applied for a loan over the past 12 months? Of those households, what percentage were successful? For the resulting categories in the frequency table, explain whether the households in that category can be described as credit constrained, credit excluded, or both. To create operational categories to use throughout this project, we will label households as either: ‘successful’: households that applied for a loan and were given the loan ‘denied’: households that applied but were not given the loan ‘did not apply’: households that did not apply for a loan. You should note that the ‘denied’ households are only a subset of the credit-excluded households as there will be households that are credit excluded and do not even apply for a loan. One could, for instance, reason that households who answered ‘Inadequate Collateral’ or ‘Do Not Know Any Lender’ are also likely to be credit excluded. Using the subset of data from Question 2(b): Create a new variable called ‘HH_status’ with the above categories. Create a new variable ‘discouraged_borrower’ that takes the value 1 if the household did not apply for a loan because it believed that it would not receive a loan (answered ‘Believe Would Be Refused’ in ‘reason_not_apply1’ or ‘reason_not_apply2’). How many households (and what percentage) are discouraged borrowers? (Note: this is a fairly narrow definition of ‘discouraged’ and one could easily argue that other criteria should also be considered under this label.) Note that arguably other answers are also indicative of being credit constrained, and so the criteria we use is definitely only a subset of all households that are credit constrained. For example, one could include households that have been denied a loan, and it is also indeed likely that some households that have been granted a loan are in fact credit constrained. Create a new variable ‘credit_constrained’ that takes the value 1 (or yes) for households that gave a reason for not applying other than ‘NA’, ‘Other’, or ‘Have Adequate Farm’ in either of the two questions ‘reason_not_apply1’ or ‘reason_not_apply2’, and 0 otherwise. For example, a household that answers ‘Have Adequate Farm’ in ‘reason_not_apply1’ and ‘Do Not Know Any Lender’ would not be classified as credit constrained. How many households (and what percentage) are credit constrained? Create a frequency table showing the most important reason for not applying for a loan, and another showing the second most important reason for not applying. What were the most common reasons for not applying? We will now analyse the stated reasons for wanting a loan, comparing those households that were successful (‘HH_status’ equal to ‘successful’) with those that were not successful (‘HH_status’ equal to ‘denied’). For both groups, create one table showing the proportion of households for each loan purpose. (You will realize that in the ‘All households’ tab, the reason for all successful loans is ‘Other’. For that reason, you should use the ‘Got loan’ tab to retrieve the reasons for loan information for successful loans.) Was the purpose of loans for denied and successful borrowers similar? (Hint: It may help to think about the broad categories of spending on consumption and investment). Using the information in the ‘All households’ tab and ‘Got loan’ tab, for each group of households: Create a table as shown in Figure 9.1 to compare the averages of the specified household characteristics. Household characteristic Successful Denied Age of household head Highest education in household Number of assets Household size Number of young children Number of working-age adults Characteristics of successful and denied borrowers. Figure 9.1 Characteristics of successful and denied borrowers. For each characteristic, explain how it may affect a household’s ability to get a loan (ceteris paribus). Looking at your table from Question 5(a), discuss whether you see this pattern in the data. (For example, are successful borrowers older/younger on average than denied borrowers?) conditional meanAn average of a variable, taken over a subgroup of observations that satisfy certain conditions, rather than all observations. Now try conditioning on the variable ‘rural’ or ‘region’ and discuss how (if at all) your results change. Using Figure 9.1, without conditioning on ‘rural’ or ‘region’, carry out the following: Calculate the difference in means (‘successful’ borrowers minus ‘denied’ borrowers). Calculate the 95% confidence interval for the difference in means between the two subgroups (‘successful’ minus ‘denied’). (Hint: Use Excel’s CONFIDENCE.T function and see Part 8.3 of Empirical Project 8 for help on how to do this.) Plot a column chart showing the differences on the vertical axis (sorted from smallest to largest), and household characteristics on the horizontal axis. Add the confidence intervals from Question 6(b) to the chart. Interpret your findings. Using the information in the ‘All households’ tab: Create a table similar to Figure 9.1, but with additional columns for discouraged borrowers and credit-constrained households. Compare the means across the four groups and discuss any similarities/differences you observe (you do not need to do any formal calculations). selection biasAn issue that occurs when the sample or data observed is not representative of the population of interest. For example, individuals with certain characteristics may be more likely to be part of the sample observed (such as students being more likely than CEOs to participate in computer lab experiments). A study on access to loans in Ethiopia looked at the relationship between loan amount and household characteristics. When doing so, they needed to account for selection bias, because we only observe positive loan amounts for successful borrowers. If we only had data for successful borrowers, then our sample would not be representative of the population of interest (all households), so we would have to interpret our results with caution. In our case, we have information about all households, so we can compare observable characteristics to see whether successful borrowers are similar to other households. An article by the Institute for Work and Health explains selection bias in more detail, and why it is a problem encountered by all areas of research. Think of another example where there might be selection bias, in other words, where the data we observe is not representative of the population of interest. Part 9.2 Households that got a loan Learning objectives for this part analyse the characteristics of loans obtained by successful borrowers. For households that successfully got a loan, we will look at: purpose of the loan duration of the loan(s) loan amount and interest rate charged who the household borrowed from. We will also see if there are any relationships between these loan characteristics and household characteristics. Now we will use the variables relating to the loan start and end dates to calculate the duration of the loan. Before using these variables, we need to check that the variable entries make sense. Some of this information could be recorded incorrectly (for example, the year is missing a digit, or the month is a number rather than a word). Using the ‘Got loan’ tab, do the following: Check the variables ‘loan_startmonth’, ‘loan_startyear’, ‘loan_endmonth’, and ‘loan_endyear’ and replace the entries that are recorded incorrectly with either the correct entry (if possible), or as blank (if not possible to infer the correct entry). (Note: Some entries are recorded as ‘Pagume’, which corresponds to early September in the Ethiopian calendar.) To calculate loan duration, we need to combine the month and year variables into one date variable. Use Excel’s CONCATENATE function to create new variables for the start and end date, and format them as date variables. (See Excel walk-through 9.1 for help on how to do this.) Some of the dates (months or years) are missing. Calculate the percentage of the data that is missing and explain whether you think missing data is a serious problem. Create a new variable containing the loan duration (end date minus start date), which will be measured in days. You will notice that some dates were recorded incorrectly, with the start date later than the end date. We could either treat these entries as missing or swap the start and end dates. Create two new variables for loan duration, one with all negative entries recorded as blank, and one with negative entries replaced as positive numbers. (Hint: Excel’s ABS function converts any number to a positive number.) For this project we will define a long-term loan as lasting more than a year (365 days), which we will use in later questions. For this definition use the loan length variable that converts negative loan lengths to positive ones. Create an indicator variable called ‘long_term’ that equals 1 if the loan was long term, and 0 otherwise. What percentage of loans were long term? Excel walk-through 9.1 Creating and formatting time variables Follow the walk-through in the CORE video, or in Figure 9.2, to find out how to create and format time variables in Excel. How to create and format time variables <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to create and format time variables using CONCATENATE. '' /> How to create and format time variables using CONCATENATE. Figure 9.2 How to create and format time variables using CONCATENATE. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : In this example, we will use the month and year variables in Columns O, P, R, and S to make new date variables in Columns V and W. Before doing this, we need to make sure all the months and years are coded correctly, otherwise Excel may not recognize these values as dates. Also, some cells are blank, so we need to make our cell formulas work regardless of missing data. '' /> The data In this example, we will use the month and year variables in Columns O, P, R, and S to make new date variables in Columns V and W. Before doing this, we need to make sure all the months and years are coded correctly, otherwise Excel may not recognize these values as dates. Also, some cells are blank, so we need to make our cell formulas work regardless of missing data. Figure 9.2a In this example, we will use the month and year variables in Columns O, P, R, and S to make new date variables in Columns V and W. Before doing this, we need to make sure all the months and years are coded correctly, otherwise Excel may not recognize these values as dates. Also, some cells are blank, so we need to make our cell formulas work regardless of missing data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Combine the month and year variables into one variable : Excel’s CONCATENATE function combines text in cells in the specified order. You can add punctuation and spaces by specifying them in quotation marks (“”). Here, we use the IF function so that Excel only fills in cells in rows where both start and end dates were non-missing (the AND function states these conditions). '' /> Combine the month and year variables into one variable Excel’s CONCATENATE function combines text in cells in the specified order. You can add punctuation and spaces by specifying them in quotation marks (“”). Here, we use the IF function so that Excel only fills in cells in rows where both start and end dates were non-missing (the AND function states these conditions). Figure 9.2b Excel’s CONCATENATE function combines text in cells in the specified order. You can add punctuation and spaces by specifying them in quotation marks (“”). Here, we use the IF function so that Excel only fills in cells in rows where both start and end dates were non-missing (the AND function states these conditions). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Reformat the cells as date variables : After step 4, you may not notice any visible changes to the text in cells, but Excel now recognizes them as dates and you can use them to make calculations, such as counting the number of days between two dates. '' /> Reformat the cells as date variables After step 4, you may not notice any visible changes to the text in cells, but Excel now recognizes them as dates and you can use them to make calculations, such as counting the number of days between two dates. Figure 9.2c After step 4, you may not notice any visible changes to the text in cells, but Excel now recognizes them as dates and you can use them to make calculations, such as counting the number of days between two dates. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use date variables to calculate duration : With the proper formatting, Excel can calculate the number of days between two dates. In this example, some of the start dates are later than the end dates, resulting in negative numbers. We will correct these values in the next step. '' /> Use date variables to calculate duration With the proper formatting, Excel can calculate the number of days between two dates. In this example, some of the start dates are later than the end dates, resulting in negative numbers. We will correct these values in the next step. Figure 9.2d With the proper formatting, Excel can calculate the number of days between two dates. In this example, some of the start dates are later than the end dates, resulting in negative numbers. We will correct these values in the next step. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-09-02-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Recode incorrect durations : Excel’s ABS function converts any value to its positive counterpart. After Step 8, the negative values in Column X are now recorded as positive numbers in Column Y. Again, we used the AND function so that Excel only did this for non-blank cells (in this case, the condition AND(X2&lt;&gt;“”) would give the same results). '' /> Recode incorrect durations Excel’s ABS function converts any value to its positive counterpart. After Step 8, the negative values in Column X are now recorded as positive numbers in Column Y. Again, we used the AND function so that Excel only did this for non-blank cells (in this case, the condition AND(X2<>“”) would give the same results). Figure 9.2e Excel’s ABS function converts any value to its positive counterpart. After Step 8, the negative values in Column X are now recorded as positive numbers in Column Y. Again, we used the AND function so that Excel only did this for non-blank cells (in this case, the condition AND(X2<>“”) would give the same results). Using the variables ‘loan_amount’ and ‘loan_interest’: Create summary tables to summarize the distribution of loan amount (mean, standard deviation, maximum, and minimum), one using the loan amount, the other using the total amount to repay (loan amount + interest). Make sure to exclude the one observation previously identified as having an extremely high interest rate. Remember to give your tables meaningful titles. Describe any features of the data that you find interesting. As mentioned earlier, the interest rate is a borrowing condition that can vary widely across households. Here we will take the interest rate to be the interest paid as a percentage of the loan amount. Calculate the interest rate for each loan in the data. (Exclude observations where the interest paid is not recorded.) Check for extreme values (interest rates that are either very large or zero). You may also want to create a scatterplot (with interest rate on the vertical axis and loan amount on the horizontal axis) to help you identify extreme (atypical) observations. Exclude the observation with the most extreme interest rate from further calculations. What percentage of the loans are zero interest? For help on making scatterplots and calculating correlation coefficients, see Excel walk-through 1.7. Make summary tables of the mean, maximum, minimum, and quartiles of the loan amount and interest rate, calculating these measures separately for long-term and short-term loans. Compare the distributions of interest rates for short-term and long-term loans. Create a table showing the correlation between the interest rate and household characteristics (you may want to refer to Figure 8.6 in Empirical Project 8 for an example). Interpreting the interest rate charged as a measure of default risk (inability to repay), explain whether the relationships implied by the coefficients are what you expected (for example, would you expect interest rates to be higher for households with less assets, more dependents etc.). Now we will look at sources of finance and how they are related to loan characteristics. Create a table showing the proportion of loans (in terms of the column variable) with source of finance (‘borrowed_from’) as the row variable and ‘rural’ as the column variable. Make a similar table but with ‘borrowed_from_other’ as the row variable instead. Does it look like rural households use different sources of finance from urban households? (Hint: It may help to think about sources of finance in terms of formal, informal, and other institutions such as microfinancers or NGOs.) For each of the variables below, create a table showing the average of that variable, with ‘borrowed_from’ as the row variable and ‘rural’ as the column variable. Comment on any similarities or differences between rows and columns that you find interesting, and suggest explanations for what you observe. duration of loan (using the variable in which negative durations were transformed to positive durations) loan amount interest rate Create a table showing the proportion of ‘gender’ (in terms of the row variable) with ‘borrowed_from’ as the row variable and ‘rural’ as the column variable. Describe any relationships you observe between the gender of household head, the place where he/she lives, and the types of finance used. What other variables are currently not in our dataset but could also be important for our analysis in Questions 2 and 3? In this project we have looked at patterns in borrowing and access to credit, but we are not able to make any causal statements such as ‘changes in X will cause households to be credit constrained’ or ‘characteristic Y causes improved access to credit’. Outline a policy intervention that could help improve households’ access to loans, and how to design the implementation so you can assess the causal effects of this policy."
});
index.addDoc({
    id: 49,
    title: "Doing Economics: Empirical Project 9: Working in R",
    content: "Empirical Project 9 Working in R Download the code To download the code chunks used in this project, right-click on the download link and select ‘Save Link As…’. You’ll need to save the code download to your working directory, and open it in RStudio. Don’t forget to also download the data into your working directory by following the steps in this project. Getting started in R For this project you will need the following packages: tidyverse, to help with data manipulation readxl, to import an Excel spreadsheet knitr, to format tables mosaic, to help create frequency tables. If you need to install these packages, run the following code: install.packages(c(''readxl'', ''tidyverse'', ''knitr'', ''mosaic'')) You can import the libraries now, or when they are used in the R walk-throughs below. library(readxl) library(tidyverse) library(knitr) library(mosaic) Part 9.1 Households that did not get a loan Learning objectives for this part identify credit-constrained and credit-excluded households using survey information create dummy (indicator) variables compare characteristics of successful borrowers, discouraged borrowers, credit-constrained households, and credit-excluded households explain why selection bias is an important issue. The Ethiopian Socioeconomic Survey (ESS) data was collected in 2013–14 from a nationally representative sample of households. Households were asked about topics such as their housing conditions, assets, and access to credit. Download the ESS data and survey questionnaire: Download the ESS data. The Excel file contains three tabs (‘Data dictionary’, ‘All households’, and ‘Got loan’). Read the ‘Data dictionary’ tab and make sure you know what each variable represents. For Part 9.1 we will use the data from the ‘All households’ tab. The ‘Got loan’ data will be used in Part 9.2. For the documentation, go to the data download site. Click on the ‘Documentation’ tab in the middle of the page. Under the heading ‘Questionnaires’, download the PDF file called ‘2013–2014 Ethiopian Socioeconomic Survey, Household Questionnaire’ by clicking the ‘Download’ button on the right-hand side of the page. You may find it helpful to refer to Section 14 of the questionnaire for the exact questions asked about credit and saving. R walk-through 9.1 Importing data into R Before importing the data, open it in Excel to look at its structure. You can see there are three tabs: ‘Data dictionary’, ‘All households’, and ‘Got loan’. We will import them into separate dataframes (DataDict, allHH, and gotL respectively). We import the ‘Data dictionary’ so that we do not have to return to the Excel spreadsheet. Also note that there are a lot of empty cells, which is how missing data is coded in Excel (but not in R). In the read_excel function we therefore use the na = '''' option so that R recognizes empty cells as missing data. library(tidyverse) library(readxl) # Set your working directory to the correct folder. # Insert your file path for 'YOURFILEPATH'. setwd(''YOURFILEPATH'') allHH <- read_excel(''Project 9 datafile.xlsx'', sheet = ''All households'', na = ''NA'') gotL <- read_excel(''Project 9 datafile.xlsx'', sheet = ''Got loan'', na = ''NA'') DataDict <- read_excel(''Project 9 datafile.xlsx'', sheet = ''Data dictionary'', na = ''NA'') Now let’s look at the variable types forallHH and gotL. To see the variable definitions, use the command view(DataDict). str(allHH) ## Classes 'tbl_df', 'tbl' and 'data.frame': 5262 obs. of 19 variables: ## $ household_id2 : num 1.01e+16 1.01e+16 1.01e+16 1.01e+16 1.01e+16 ... ## $ got_loan : chr ''No'' ''No'' ''No'' ''Yes'' ... ## $ rural : chr ''Rural'' ''Rural'' ''Rural'' ''Rural'' ... ## $ hhsize : num 8 8 1 3 4 4 5 5 6 7 ... ## $ region : chr ''Tigray'' ''Tigray'' ''Tigray'' ''Tigray'' ... ## $ gender : chr ''Female'' ''Male'' ''Female'' ''Female'' ... ## $ age : num 78 37 78 30 71 28 37 32 51 43 ... ## $ young_children : num 4 5 0 1 1 3 2 3 3 2 ... ## $ working_age_adults: num 3 3 0 2 1 1 3 2 3 6 ... ## $ max_education : num 3 0 0 4 8 4 9 6 4 5 ... ## $ number_assets : num 32 19 3 6 34 5 45 35 20 64 ... ## $ loan_rejected : chr ''No'' ''No'' ''No'' ''No'' ... ## $ rejection_source1 : chr NA NA NA NA ... ## $ rejection_source2 : chr NA NA NA NA ... ## $ loan_purpose : chr NA NA NA NA ... ## $ loan_purpose_other: chr NA NA NA NA ... ## $ did_not_apply : chr ''Did not apply'' ''Did not apply'' ''Did not apply'' ''Applied'' ... ## $ reason_not_apply1 : chr ''Too Expensive'' ''Believe Would Be Refused'' ''Too Expensive'' NA ... ## $ reason_not_apply2 : chr ''Fear Not Be Able To Pay'' NA ''Fear Not Be Able To Pay'' NA ... str(gotL) ## Classes 'tbl_df', 'tbl' and 'data.frame': 1480 obs. of 21 variables: ## $ household_id2 : num 1.01e+16 1.01e+16 1.01e+16 1.01e+16 1.01e+16 ... ## $ got_loan : chr ''Yes'' ''Yes'' ''Yes'' ''Yes'' ... ## $ rural : chr ''Rural'' ''Rural'' ''Rural'' ''Rural'' ... ## $ hhsize : num 3 4 5 9 7 7 8 7 10 8 ... ## $ region : chr ''Tigray'' ''Tigray'' ''Tigray'' ''Tigray'' ... ## $ gender : chr ''Female'' ''Male'' ''Male'' ''Male'' ... ## $ age : num 30 71 53 52 56 38 44 47 52 37 ... ## $ young_children : num 1 1 3 4 3 4 4 3 2 6 ... ## $ working_age_adults : num 2 1 2 6 3 3 4 4 8 2 ... ## $ max_education : num 4 8 5 8 7 7 8 6 10 1 ... ## $ number_assets : num 6 34 14 16 9 14 24 15 22 10 ... ## $ borrowed_from : chr ''Relative'' ''Relative'' ''Relative'' ''Other (specify)'' ... ## $ borrowed_from_other: chr NA NA NA ''Cooperatives'' ... ## $ loan_purpose : chr ''Purchase Agricultural Inputs for Food Crop'' ''Purchase Agricultural Inputs for Food Crop'' ''Purchase Agricultural Inputs for Food Crop'' ''Purchase Agricultural Inputs for Food Crop'' ... ## $ loan_startmonth : chr ''March'' ''November'' ''November'' ''June'' ... ## $ loan_startyear : num 2004 2006 2006 2005 2005 ... ## $ loan_repaid : chr ''Yes'' ''Yes'' ''No'' ''No'' ... ## $ loan_endmonth : chr NA NA ''June'' ''March'' ... ## $ loan_endyear : num NA NA 2006 2006 2006 ... ## $ loan_amount : num 1000 800 2500 2653 1600 ... ## $ loan_interest : num 300 0 1750 325 300 1750 290 300 300 268 ... It is important to ensure that all variables we expect to be numerical (numbers) are coded as num, and in this case, they are. You can see that there are many variables that are coded as character (chr) variables because they are text (for example gender or region), but since we can use these variables to group data by category, we will use as.factor to change them into categorical (factor) variables for later use. Instead of converting each character variable to a factor variable individually, say allHH$gender <- as.factor(allHH$gender), we use the piping operator (%>%) from the tidyverse package to do this step in one go. (For a more detailed introduction to piping, see the University of Manchester’s Econometric Computing Learning Resource). We take allHH and use the mutate_if function, which applies the as.factor function to character variables only. Then we do the same for gotL. allHH <- allHH %>% mutate_if(is.character, as.factor) gotL <- gotL %>% mutate_if(is.character, as.factor) Typing str(allHH) and str(gotL) will confirm that all character variables are now categorical variables. The data is already in a format clean enough to use, so we will begin by summarizing the information in the ‘All households’ tab, starting with region and household characteristics. Create a table showing the proportion of households that lived in each region and area type, with region as the row variable and rural as the column variable. (For help on creating tables, see R walk-through 3.3.) Use the Gender variable to find what percentage of household heads were female. Create an appropriate summary table for the variables hhsize, gender, age, young_children, working_age_adults, max_education, and number_assets. (You may find it helpful to refer to R walk-through 2.7 in Empirical Project 2 for one possible format to use.) Write a short paragraph describing the information in your tables for 1(c). R walk-through 9.2 Creating summary tables In order to get the proportions of households living in large towns, small towns, or rural areas (encoded in the variable rural), we use the table function. The counts (number) of households in the respective regions and area types are contained in stab1. Running this table through the prop.table() function changes the values from counts to proportions. The optional input (, 1) makes the proportions for each row add to one. (You could see what happens when you leave this option out, or if you choose the value 2.) To obtain detailed information, use the command ?prop.table . # Control how many digits are printed options(digits = 3) stab1 <- table(allHH$region, allHH$rural) prop.table(stab1, 1) ## ## Large town (urban) Rural Small town (urban) ## Addis Ababa 1.0000 0.0000 0.0000 ## Afar 0.0956 0.7574 0.1471 ## Amhara 0.2176 0.6692 0.1132 ## Benshagul Gumuz 0.0000 0.9040 0.0960 ## Diredwa 0.4685 0.5315 0.0000 ## Gambelia 0.1154 0.8000 0.0846 ## Harari 0.2727 0.7273 0.0000 ## Oromia 0.2831 0.6070 0.1098 ## SNNP 0.1826 0.7270 0.0905 ## Somalie 0.1552 0.7552 0.0897 ## Tigray 0.3670 0.5628 0.0701 Let’s use a similar approach to calculate the percentage of households with female heads (encoded in the variable gender). stab2 <- table(allHH$gender) prop.table(stab2) ## ## Female Male ## 0.304 0.696 As shown, 30.4% of households have a female head. We need to provide summary statistics for a range of variables. Most of these variables are numeric variables, but one, gender, is a factor variable. For the latter, we use the summary function. For the numeric variables we use the favstats function, which is part of the mosaic package. We could also use the summary function, but it does not provide the standard deviations that we need. The summary statistics for the numeric variables are generated using the lapply (list apply) function. Inside the function we specify the subset of variables we are interested in, subset(allHH, select = sel_q), and specify the function we want to apply to these variables, namely the favstats function. # Load the mosaic package library(mosaic) summary(allHH$gender) ## Female Male NA's ## 1599 3662 1 # Create a list of the numeric variable names sel_q <- c('hhsize', 'age', 'young_children', 'working_age_adults', 'max_education', 'number_assets') lapply(subset(allHH, select = sel_q), favstats) ## $hhsize ## min Q1 median Q3 max mean sd n missing ## 1 3 4 6 16 4.58 2.4 5260 2 ## ## $age ## min Q1 median Q3 max mean sd n missing ## 3 32 42 55 99 44.2 15.6 5253 9 ## ## $young_children ## min Q1 median Q3 max mean sd n missing ## 0 0 2 3 10 1.89 1.71 5262 0 ## ## $working_age_adults ## min Q1 median Q3 max mean sd n missing ## 0 2 2 3 10 2.58 1.52 5262 0 ## ## $max_education ## min Q1 median Q3 max mean sd n missing ## 0 2 6 10 30 7.53 7.28 5262 0 ## ## $number_assets ## min Q1 median Q3 max mean sd n missing ## 0 5 9 18 203 14.9 17.2 5262 0 Now that we have an idea of what our data looks like, we will move on to identifying households that are potentially excluded from the credit market or are credit constrained. The former are households that find it impossible to borrow, and the latter are households that can only borrow on unfavourable terms (see Section 9.10 of Economy, Society, and Public Policy). The variables in our dataset that are related to this issue are did_not_apply and loan_rejected. Later we will also look at the responses given in the variables ‘reason_not_apply1’ and ‘reason_not_apply2’. Using the ‘All households’ dataset: Create a frequency table with did_not_apply as the row variable and loan_rejected as the column variable. Include all ‘NA’ as a separate row. Looking at these two variables, explain why some observations should be excluded and remove them from the dataset. Also remove all households with missing information for one or more of these variables. Of the non-excluded observations, what percentage of households applied for a loan over the past 12 months? Of those households, what percentage were successful? For the resulting categories in the frequency table, explain whether the households in that category can be described as credit constrained, credit excluded, or both. R walk-through 9.3 Making frequency tables for loan applications and outcomes The easiest way to make a frequency table is to use the table function. Note that we nested the table () function in the addmargins function to obtain row and column totals. By default, the table function excludes missing (NA) values. Consulting the help function (type ?table in the command window) will show that the option useNA = ''always'' includes these values. stab3 <- addmargins(table( allHH$did_not_apply, allHH$loan_rejected, useNA = ''always'', dnn = c(''Applied?'', ''Rejected?''))) stab3 ## Rejected? ## Applied? No Yes <NA> Sum ## Applied 1363 201 1 1565 ## Did not apply 3632 24 2 3658 ## <NA> 37 2 0 39 ## Sum 5032 227 3 5262 Here, we decide to exclude the 24 households that indicated that they did not apply for a loan, but also indicated that they were refused a loan. This is a contestable decision, as it results in excluding more than 10% of households that indicated that they were refused a loan. We shall also remove all observations that have missing data for any of these two questions. Now we create the dataset with the non-missing data only (allHHc). # Remove NAs in did_not_apply allHHc <- subset(allHH, !is.na(allHH$did_not_apply)) allHHc <- subset(allHHc, !is.na(allHHc$loan_rejected)) # Show the number of observations nrow(allHHc) ## [1] 5220 At this stage we have dropped 42 observations. Now we delete the 24 observations that gave a nonsensical answer. allHHc <- subset(allHHc, !(allHHc$loan_rejected == ''Yes'' & allHHc$did_not_apply == ''Did not apply'')) # Show the number of observations nrow(allHHc) ## [1] 5196 We are left with 5,196 observations. Let’s recreate the frequency table, but this time adding the prop.table function to calculate proportions. (To obtain percentages, multiply the proportions by 100.) stab4 <- addmargins(prop.table(table( allHHc$did_not_apply, allHHc$loan_rejected, dnn = c(''Applied?'', ''Rejected?'')))) stab4 ## Rejected? ## Applied? No Yes Sum ## Applied 0.2623 0.0387 0.3010 ## Did not apply 0.6990 0.0000 0.6990 ## Sum 0.9613 0.0387 1.0000 To create operational categories to use throughout this project, we will label households as either: ‘successful’: households that applied for a loan and were given the loan ‘denied’: households that applied but were not given the loan ‘did not apply’: households that did not apply for a loan. You should note that the ‘denied’ households are only a subset of the credit-excluded households, as there will be households that are credit excluded and do not even apply for a loan. One could, for instance, reason that households who answered ‘Inadequate Collateral’ or ‘Do Not Know Any Lender’ are also likely to be credit excluded. Using the subset of data from Question 2(b): Create a new variable called HH_status with the above categories. Create a new variable discouraged_borrower that takes the value 1 if the household did not apply for a loan because it believed that it would not receive a loan (answered ‘Believe Would Be Refused’ in reason_not_apply1 or reason_not_apply2). How many households (and what percentage) are discouraged borrowers? (Note: This is a fairly narrow definition of ‘discouraged’ and one could easily argue that other criteria should also be considered under this label.) Note that arguably other answers are also indicative of being credit constrained, so the criteria we use is definitely only a subset of all households that are credit constrained. For example, one could include households that have been denied a loan, and it is also likely that some households that have been granted a loan are in fact credit constrained. Create a new variable credit_constrained that takes the value 1 (or yes) for households that gave a reason for not applying other than ‘NA’, ‘Other’, or ‘Have Adequate Farm’ in either of the two questions reason_not_apply1 or reason_not_apply2, and 0 otherwise. For example, a household that answers ‘Have Adequate Farm’ in reason_not_apply1 and ‘Do Not Know Any Lender’ would not be classified as credit constrained. How many households (and what percentage) are credit constrained? Create a frequency table showing the most important reason for not applying for a loan, and another showing the second most important reason for not applying. What were the most common reasons for not applying? R walk-through 9.4 Creating variables to classify households Let’s first create the HH_status variable. We set the values of HH_status to ”not applied”, then use logical indexing to change all entries where households applied for a loan (allHHc$did_not_apply == “Applied”) and were accepted (allHHC$loan_rejected == “No”) to “successful”, and households who were denied ((allHHC$loan_rejected == “Yes”) to ”denied”. # This is the default category. allHHc$HH_status <- ''not applied'' allHHc$HH_status[allHHc$did_not_apply == ''Applied'' & allHHc$loan_rejected == ''No''] <- ''successful'' allHHc$HH_status[allHHc$did_not_apply == ''Applied'' & allHHc$loan_rejected == ''Yes''] <- ''denied'' # Change from character to factor variable allHHc$HH_status <- factor(allHHc$HH_status) Typing summary(allHHc$HH_status) should give you numbers that correspond to the frequency table from Question 2. Let’s continue by using the same steps to make the discouraged_borrower variable. # This is the default category. allHHc$discouraged_borrower <- ''No'' allHHc$discouraged_borrower[allHHc$reason_not_apply1 == ''Believe Would Be Refused''] <- ''Yes'' allHHc$discouraged_borrower[allHHc$reason_not_apply2 == ''Believe Would Be Refused''] <- ''Yes'' # Change from character to factor variable allHHc$discouraged_borrower <- factor(allHHc$discouraged_borrower) summary(allHHc$discouraged_borrower) ## No Yes ## 4608 588 To make the credit_constrained variable, we use the levels function to check all the possible answers to the reason_not_apply1 variable. We store these answers in the object sel_ans. sel_ans <- levels(allHHc$reason_not_apply1) sel_ans ## [1] ''Believe Would Be Refused'' ''Do Not Know Any Lender'' ## [3] ''Do Not Like To Be In Debt'' ''Fear Not Be Able To Pay'' ## [5] ''Have Adequate Farm'' ''Inadequate Collateral'' ## [7] ''No Farm or Business'' ''Other (Specify)'' ## [9] ''Too Expensive'' ''Too Much Trouble'' Of these reasons, only reasons [5] and [8] do not lead to a conclusion that a household is credit constrained, so we remove them from sel_ans. # Remove reasons 5 and 8 sel_ans <- sel_ans[-c(5, 8)] # This is the default category, as households that did not # provide any reasons are classified as not credit # constrained. allHHc$credit_constrained <- ''No'' allHHc$credit_constrained[allHHc$reason_not_apply1 %in% sel_ans] <- ''Yes'' allHHc$credit_constrained[allHHc$reason_not_apply2 %in% sel_ans] <- ''Yes'' # Change from character to factor variable allHHc$credit_constrained <- factor(allHHc$credit_constrained) summary(allHHc$credit_constrained) ## No Yes ## 2184 3012 The use of %in% in the selection criterion allHHc$reason_not_apply1 %in% sel_ans is a very useful programming technique that you can use to select data according to a list of values/variables. In this case, sel_ans contains all the answers that we associate with a credit-constrained household. allHHc$reason_not_apply1 %in% sel_ans gives an outcome of TRUE if the answer to reason_not_apply1 is one of the answers in sel_ans, then sets the value of credit_constrained to ‘Yes’ for those observations. stab4 <- addmargins(prop.table(table( allHHc$credit_constrained, allHHc$discouraged_borrower, useNA = ''ifany'', dnn = c( ''Constrained?'', ''Discouraged?'')))) stab4 ## Discouraged? ## Constrained? No Yes Sum ## No 0.420 0.000 0.420 ## Yes 0.467 0.113 0.580 ## Sum 0.887 0.113 1.000 # Required for the use of the kable function library(knitr) print(''Reasons not to apply 1'') ## [1] ''Reasons not to apply 1'' # 'kable' is optional but formats tables more neatly. table5 <- prop.table(table(allHHc$reason_not_apply1)) kable(table5[rev(order(table5))]) Var1 Freq -------------------------- ------ Do Not Like To Be In Debt 0.191 Have Adequate Farm 0.185 Fear Not Be Able To Pay 0.170 Believe Would Be Refused 0.116 No Farm or Business 0.102 Do Not Know Any Lender 0.068 Too Expensive 0.049 Inadequate Collateral 0.045 Other (Specify) 0.041 Too Much Trouble 0.033 print(''Reasons not to apply 2'') ## [1] ''Reasons not to apply 2'' table6 <- prop.table(table(allHHc$reason_not_apply2)) kable(table6[rev(order(table6))]) Var1 Freq -------------------------- ------ Fear Not Be Able To Pay 0.277 Do Not Like To Be In Debt 0.243 Inadequate Collateral 0.087 Believe Would Be Refused 0.087 Too Expensive 0.068 Do Not Know Any Lender 0.064 Too Much Trouble 0.062 Have Adequate Farm 0.050 No Farm or Business 0.040 Other (Specify) 0.024 We will now analyse the stated reasons for wanting a loan, comparing those households that were successful (HH_status equal to ‘successful’) with those that were not successful (HH_status equal to ‘denied’). For both groups, create one table showing the proportion of households for each loan purpose. You will realize that in the ‘All households’ dataset, the reason for all ‘successful’ loans is ‘Other’. For that reason, you should use the ‘Got loan’ dataset to retrieve the reasons for loan information for successful loans. Was the purpose of loans for denied and successful borrowers similar? (Hint: It may help to think about the broad categories of spending on consumption and investment.) R walk-through 9.5 Making frequency tables to compare proportions Some of the data is in the allHH dataset, while the rest is in the gotL dataset, both of which we imported in R walk-through 9.1. We will combine that information into one new dataset called loan_data, which we then use to produce the table. sel_allHHc <- subset(allHHc, subset = ( allHHc$HH_status %in% c(''successful'', ''denied''))) # Removes the unused 'did not apply' level sel_allHHc <- droplevels(sel_allHHc) prop.table(table( sel_allHHc$loan_purpose, sel_allHHc$HH_status, dnn = c(''Loan Purpose'', ''Loan'')), 2) ## Loan ## Loan Purpose denied successful ## Business Start-up Capital 0.2615 0.0000 ## Expanding Business 0.1385 1.0000 ## Other (Specify) 0.2718 0.0000 ## Purchase Agricultural Inputs for Food Crop 0.2103 0.0000 ## Purchase House/Lease Land 0.0308 0.0000 ## Purchase Inputs for other Crops 0.0615 0.0000 ## Purchase Non-farm Inputs 0.0256 0.0000 This reveals a particular feature of the data, namely that for successful borrowers, the allHHc dataset does not contain all the useful information, as every successful household has ‘Other (Specify)’ in the loan_purpose variable. There is more useful information on loan purpose in the gotL data, so we will extract the loan_purpose variable for unsuccessful households from the allHHc dataset, and the equivalent information for successful loaners from the gotL dataset. # Select unsuccessful households from allHHc loan_no <- subset(allHHc, allHHc$HH_status == ''denied'', select = c(''loan_purpose'', ''HH_status'')) # Select loan purpose for successful households from gotL loan_yes <- subset(gotL, gotL$got_loan == ''Yes'', select = ''loan_purpose'') loan_yes$HH_status <- ''successful'' # Combine into one dataset loan_data <- rbind(loan_no, loan_yes) # Remove the unused 'did not apply' level loan_data <- droplevels(loan_data) kable(prop.table(table( loan_data$loan_purpose, loan_data$HH_status, dnn = c(''Loan Purpose'', ''Loan'')), 2)) denied successful ------------------------------------------- ------- ----------- Business Start-up Capital 0.262 0.154 Expanding Business 0.138 0.081 Other (Specify) 0.272 0.027 Purchase Agricultural Inputs for Food Crop 0.210 0.300 Purchase House/Lease Land 0.031 0.023 Purchase Inputs for other Crops 0.062 0.098 Purchase Non-farm Inputs 0.026 0.115 For consumption and personal expenses 0.000 0.201 Using the information in the ‘All households’ and ‘Got loan’ tab, for ‘successful’ and ‘denied’ households: Create a table as shown in Figure 9.1 to compare the averages of the specified household characteristics. Household characteristic Successful Denied Age of household head Highest education in household Number of assets Household size Number of young children Number of working-age adults Characteristics of successful and denied borrowers. Figure 9.1 Characteristics of successful and denied borrowers. For each characteristic, explain how it may affect a household’s ability to get a loan (ceteris paribus). conditional meanAn average of a variable, taken over a subgroup of observations that satisfy certain conditions, rather than all observations. Looking at your table from Question 5(a), discuss whether you see this pattern in the data. (For example, are successful borrowers older/younger on average than denied borrowers?) Now try conditioning on the variable rural or region and discuss how (if at all) your results change. R walk-through 9.6 Calculating differences in household characteristics Here we show how to get average characteristics conditional on HH_status using the mean function. With the mosaic package loaded (as we have done in R walk-through 9.2), we can use the conditioning symbol | in the mean function to condition according to HH_status. # Show the number of observations in each category summary(allHHc$HH_status) ## denied not applied successful ## 201 3632 1363 # Mean household size conditional on credit status mean(~hhsize|HH_status, data = allHHc, na.rm = TRUE) ## denied not applied successful ## 4.82 4.46 4.87 # Mean max_education of household head, by credit status mean(~max_education | HH_status, data = allHHc, na.rm = TRUE) ## denied not applied successful ## 8.00 7.62 7.26 Repeat the mean command above for all variables to complete the table. If we want to repeat this analysis by also splitting the data according to rural or region, we can use the group option (which is only available when mosaic has been loaded). # Show the number of observations in each category table(allHHc$rural, allHHc$HH_status) ## ## denied not applied successful ## Large town (urban) 56 1092 332 ## Rural 128 2236 903 ## Small town (urban) 17 304 128 # Mean HH size, by rural and credit status variables mean(~hhsize | HH_status, group = rural, data = allHHc, na.rm = TRUE) ## denied.Large town (urban) not applied.Large town (urban) ## 3.57 3.44 ## successful.Large town (urban) denied.Rural ## 3.84 5.47 ## not applied.Rural successful.Rural ## 4.98 5.35 ## denied.Small town (urban) not applied.Small town (urban) ## 4.06 4.28 ## successful.Small town (urban) Large town (urban) ## 4.12 3.54 ## Rural Small town (urban) ## 5.10 4.23 # Mean max_education of HH head, by rural and credit # status variables mean(~working_age_adults | HH_status, group = rural, data = allHHc, na.rm = TRUE) ## denied.Large town (urban) not applied.Large town (urban) ## 2.36 2.30 ## successful.Large town (urban) denied.Rural ## 2.42 2.88 ## not applied.Rural successful.Rural ## 2.58 2.91 ## denied.Small town (urban) not applied.Small town (urban) ## 3.18 2.75 ## successful.Small town (urban) Large town (urban) ## 2.54 2.33 ## Rural Small town (urban) ## 2.68 2.71 The same result can be also obtained using the piping operator (%>%) from the tidyverse package. stats4 <- allHHc %>% group_by(HH_status, region) %>% summarize(avg_hhsize = mean(hhsize, na.rm = TRUE)) %>% spread(HH_status, avg_hhsize) %>% print() ## # A tibble: 11 x 4 ## region denied not applied successful ## <fct> <dbl> <dbl> <dbl> ## 1 Addis Ababa 2.50 4.00 3.84 ## 2 Afar 7.00 4.97 5.69 ## 3 Amhara 4.42 3.86 4.50 ## 4 Benshagul Gumuz 5.00 4.70 5.41 ## 5 Diredwa 4.57 4.04 4.28 ## 6 Gambelia 5.22 5.04 5.50 ## 7 Harari 3.75 5.02 3.74 ## 8 Oromia 4.91 4.53 5.21 ## 9 SNNP 5.20 4.88 5.09 ## 10 Somalie 4.33 5.15 5.26 ## 11 Tigray 4.62 4.01 4.78 To understand what the spread command does, run the above code without it and see the difference. Using Figure 9.1, without conditioning on rural or region: Calculate the difference in means (‘successful’ borrowers minus ‘denied’ borrowers). Calculate the 95% confidence interval for the difference in means between the two subgroups (‘successful’ minus ‘denied’). (See Part 8.3 of Empirical Project 8 for help on how to do this.) Plot a column chart showing the differences on the vertical axis (sorted from smallest to largest), and household characteristics on the horizontal axis. Add the confidence intervals from Question 6(b) to the chart. Interpret your findings. R walk-through 9.7 Calculating confidence intervals and adding them to a chart To repeat the same set of calculations for a list of variables, we will use the piping operator (%>%). First we create a list of these variables (called sel_var). sel_var <- c(''age'', ''max_education'', ''number_assets'', ''hhsize'', ''young_children'', ''working_age_adults'') Now we use the age variable as an example. stats5 <- allHHc %>% # Filters out the 'did not apply' cases filter(HH_status %in% c(''denied'', ''successful'')) %>% group_by(HH_status) %>% summarize(avg_ = mean(age, na.rm = TRUE), sd_ = sd(age, na.rm = TRUE), n_ = sum(!is.na(age))) %>% print() ## # A tibble: 2 x 4 ## HH_status avg_ sd_ n_ ## <fct> <dbl> <dbl> <int> ## 1 denied 41.2 12.9 201 ## 2 successful 43.4 14.3 1361 Now we use the t.test function to calculate the difference between the successful group (sel_success) and the denied borrowers (sel_denied). # Select the age variable (sel_var[1]) for successful and # denied borrowers sel_success <- unlist(allHHc[ allHHc$HH_status == ''successful'', sel_var[1]]) sel_denied <- unlist(allHHc[ allHHc$HH_status == ''denied'', sel_var[1]]) # The unlist function is needed to get data as a vector # instead of a dataframe/tibble. temp <- t.test(sel_success, sel_denied, conf.level = 0.95) The output of this test provides us with the details required. Note that the conf.level = 0.95 option is actually not necessary here, as 0.95 is the default level. We will now do this for all required variables and save the difference in means and the confidence interval values in a dataframe so we can plot this information. # Create the dataframe to save the data used for the chart temp_plot <- data.frame(name = sel_var, dmean = NA, yhigh = NA, ylow = NA) for (i in sel_var){ sel_success <- unlist(allHHc[ allHHc$HH_status == ''successful'', i]) sel_denied <- unlist(allHHc[ allHHc$HH_status == ''denied'', i]) temp <- t.test(sel_success, sel_denied, conf.level = 0.95) # Mean difference temp_plot$dmean[temp_plot$name == i] <- temp$estimate[1] - temp$estimate[2] # Lower limit of the confidence interval temp_plot$ylow[temp_plot$name == i] <- temp$conf.int[1] # Upper limit of the confidence interval temp_plot$yhigh[temp_plot$name == i] <- temp$conf.int[2] } ggplot(temp_plot, aes(x = name, y = dmean)) + geom_bar(stat = ''identity'') + geom_errorbar(aes(ymin = ylow, ymax = yhigh), width = .2) + ylab(''Difference in means'') + xlab(''Variable'') + theme_bw() + ggtitle(''Difference in HH characteristics (successful and denied borrowers)'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-02.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-02-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-02-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-02-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-02.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Column chart showing difference in HH characteristics for successful and denied borrowers. '' /> Column chart showing difference in HH characteristics for successful and denied borrowers. Figure 9.2 Column chart showing difference in HH characteristics for successful and denied borrowers. Using the information in the ‘All households’ dataset: Create a table similar to Figure 9.1, but with additional columns for discouraged borrowers and credit-constrained households. Compare the means across the four groups and discuss any similarities/differences you observe (you do not need to do any formal calculations). R walk-through 9.8 Calculating conditional means We are interested in the means of a range of variables for different subgroups. Two subgroups are mutually exclusive (HH_status == ''successful'' and HH_status == ''denied''), while the others ( credit_constrained == ''yes'' and discouraged_borrower == ''yes'') are partially overlapping subgroups of the data. Our strategy is to create a temporary dataframe (sel_allHHc) that only contains the relevant observations and the relevant variables. Then we can use the colMeans function to calculate the required means. # List variables we are interested in el_var <- c(''age'', ''max_education'', ''number_assets'', ''hhsize'', ''young_children'', ''working_age_adults'') sel_allHHc <- allHHc[ allHHc$HH_status==''successful'', sel_var] paste(''successful (n = '', nrow(sel_allHHc), '')'') ## [1] ''successful (n = 1363 )'' colMeans(sel_allHHc, na.rm = TRUE) ## age max_education number_assets ## 43.37 7.26 15.88 ## hhsize young_children working_age_adults ## 4.87 2.09 2.75 sel_allHHc <- allHHc[allHHc$HH_status == ''denied'', sel_var] paste(''denied (n = '', nrow(sel_allHHc), '')'') ## [1] ''denied (n = 201 )'' colMeans(sel_allHHc, na.rm = TRUE) ## age max_education number_assets ## 41.21 8.00 14.46 ## hhsize young_children working_age_adults ## 4.82 2.22 2.76 sel_allHHc <- allHHc[ allHHc$discouraged_borrower == ''Yes'', sel_var] paste(''discouraged (n = '', nrow(sel_allHHc), '')'') ## [1] ''discouraged (n = 588 )'' colMeans(sel_allHHc, na.rm = TRUE) ## age max_education number_assets ## 43.28 6.50 10.16 ## hhsize young_children working_age_adults ## 4.65 2.03 2.49 sel_allHHc <- allHHc[ allHHc$credit_constrained == ''Yes'', sel_var] paste(''constrained (n = '', nrow(sel_allHHc), '')'') ## [1] ''constrained (n = 3012 )'' colMeans(sel_allHHc, na.rm = TRUE) ## age max_education number_assets ## 44.84 7.14 13.54 ## hhsize young_children working_age_adults ## 4.44 1.81 2.49 selection biasAn issue that occurs when the sample or data observed is not representative of the population of interest. For example, individuals with certain characteristics may be more likely to be part of the sample observed (such as students being more likely than CEOs to participate in computer lab experiments). A study on access to loans in Ethiopia looked at the relationship between loan amount and household characteristics. When doing so, they needed to account for selection bias, because we only observe positive loan amounts for successful borrowers. If we only had data for successful borrowers, then our sample would not be representative of the population of interest (all households), so we would have to interpret our results with caution. In our case, we have information about all households, so we can compare observable characteristics to see whether successful borrowers are similar to other households. An article by the Institute for Work and Health explains selection bias in more detail, and why it is a problem encountered by all areas of research. Think of another example where there might be selection bias, in other words, where the data we observe is not representative of the population of interest. Part 9.2 Households that got a loan Learning objectives for this part analyse the characteristics of loans obtained by successful borrowers. For households that successfully got a loan, we will look at: purpose of the loan duration of the loan(s) loan amount and interest rate charged who the household borrowed from. We will also see if there are any relationships between these loan characteristics and household characteristics. Now we will use the variables relating to the loan start and end dates to calculate the duration of the loan. Before using these variables, we need to check that the variable entries make sense. Some of this information could be recorded incorrectly (for example, the year is missing a digit, or the month is a number rather than a word). Using the ‘Got loan’ dataset: Check the variables loan_startmonth, loan_startyear, loan_endmonth, and loan_endyear, and replace the entries that are recorded incorrectly with either the correct entry (if possible), or as blank (if not possible to infer the correct entry). (Note: Some entries are recorded as ‘Pagume’, which corresponds to early September in the Ethiopian calendar.) To calculate loan duration, combine the month and year variables into one date variable and format them as date variables. Some of the dates (months or years) are missing. Calculate the percentage of the data that is missing and explain whether you think missing data is a serious problem. Create a new variable containing the loan duration (end date minus start date), which will be measured in days. You will notice that some dates were recorded incorrectly, with the start date later than the end date. We could either treat these entries as missing or swap the start and end dates. Create two new variables for loan duration, one with all negative entries recorded as blank, and one with negative entries replaced as positive numbers. For this project we will define a long-term loan as lasting more than a year (365 days), which we will use in later questions. For this definition, use the loan_length variable that converts negative loan lengths to positive ones (see Question 1(e) above). Create an indicator variable called long_term that equals 1 if the loan was long term, and 0 otherwise. What percentage of loans were long term? R walk-through 9.9 Data cleaning and summarizing loan characteristics We start by cleaning up the loan dates. We have information on start month and year as well as end month and year. Let’s look at these in turn. The structure of the dataframe (str(gotL)) indicates that the start and end year are numeric variables, but the months are factor variables with month names (for example ‘April’). Let’s first look at the years by creating a scatterplot. ggplot(gotL, aes(x = loan_endyear, y = loan_startyear)) + geom_point(size = 2, shape = 23, fill = ''blue'') + theme_bw() + ggtitle(''Loan start and end year'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Scatterplot showing loan start and end year. '' /> Scatterplot showing loan start and end year. Figure 9.3 Scatterplot showing loan start and end year. We can see that there are three observations that have very low (< 500) start or end year values, which does not make sense. We will replace these with ‘NA’, but leave the original data untouched and create a new dataset called gotLc, where the ‘c’ indicates cleaned data. gotLc <- gotL gotLc$loan_startyear[gotLc$loan_startyear < 500] <- NA gotLc$loan_endyear[gotLc$loan_endyear < 500] <- NA ggplot(gotLc, aes(x = loan_endyear, y = loan_startyear)) + geom_point(size = 2, shape = 23, fill = ''blue'') + theme_bw() + ggtitle(''Loan start and end year'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Revised scatterplot showing loan start and end year without outliers. '' /> Revised scatterplot showing loan start and end year without outliers. Figure 9.4 Revised scatterplot showing loan start and end year without outliers. In the top left corner, there is a loan with the start year (2006) after the end year (2003). Clearly this is incorrect, so we should remove this observation when analysing loan periods. However, we wait until we have combined the years with the months as there may be more observations with this issue. Also, we can only see a small number of points because there are many identical observations (for example startyear of 2006 and endyear of 2006). To see these points you could replacegeom_point with geom_jitter in the command above ggplot command . Use ?geom_jitter to understand what this option does. Now let’s look at the values in start_month. summary(gotLc$loan_startmonth) ## April August December February January July June ## 95 63 133 145 176 115 156 ## March May November October September NA's ## 85 141 115 106 146 4 There is no particular issue with the start months. What about the end months? summary(gotLc$loan_endmonth) ## April August December February January July June ## 136 53 49 170 52 34 94 ## March May November October Pagume September NA's ## 155 89 33 39 5 37 534 Two things are noteworthy here: there are now many ‘NA’ entries, and there is an entry called ‘Pagume’. As described in the task, ‘Pagume’ can be approximated by September. Let’s recode that. gotLc$loan_endmonth[gotLc$loan_endmonth == ''Pagume''] <- ''September'' Another call of summary(gotLc$loan_endmonth) would confirm that there are no observations with ‘Pagume’ left. Now we want to calculate the length of the loan, in other words, the number of days between start and end day. As we only have months and not days, this will be an approximation. We will create a new variable combining months and years using the paste function, assuming that all loan start and end dates are on the first day of each month. # We assume that all loan start and end dates are on the # 1st of the month. gotLc$loan_startdate <- paste(''1'', gotLc$loan_startmonth, gotLc$loan_startyear) gotLc$loan_enddate <- paste(''1'', gotLc$loan_endmonth, gotLc$loan_endyear) For observations with an unknown end date (recall we had more than 500 of these), we will code as missing. Currently these are recorded as ‘1 NA NA’. First we need to find which observations have missing data elements for either the loan_startdate or loan_enddate variable. R has very powerful tools to identify text patterns such as this, which you can learn about by searching the Internet for help (you could search for ‘R test whether character contains string’). For example, a useful short introduction to using such tools contains one solution to our problem. See the first 20 observations of loan_enddate. gotLc$loan_enddate[1:20] ## [1] ''1 NA NA'' ''1 NA NA'' ''1 June 2006'' ## [4] ''1 March 2006'' ''1 February 2006'' ''1 September 2007'' ## [7] ''1 March 2006'' ''1 March 2006'' ''1 March 2006'' ## [10] ''1 March 2006'' ''1 February 2006'' ''1 NA NA'' ## [13] ''1 August 2007'' ''1 August 2007'' ''1 February 2006'' ## [16] ''1 June 2007'' ''1 November 2007'' ''1 March 2006'' ## [19] ''1 April 2006'' ''1 NA NA'' You can find NAs in observations 1, 2, 12, and 20. The command grep will identify these rows automatically. # Identify the position of the observations that contain # NAs selNA <- grep('NA', gotLc$loan_enddate) selNA[1:5] ## [1] 1 2 12 20 26 As you can see, grep identifies the first four instances correctly, and we can see that the next missing end date is in observation 26. Now we will replace all these observations as missing values and then convert the non-missing observations to dates, using the as.Date function. gotLc$loan_enddate[selNA] <- NA gotLc$loan_enddate <- as.Date(gotLc$loan_enddate, format = ''%d %B %Y'') The option format = ''%d %B %Y'' specifies the format that dates are recorded as (for example ‘1 June 2006’), where %B stands for full months (view this page for examples of other date formatting options). Now we repeat the same steps for the start date. # Identify the position of the observations that contain # NAs selNA <- grep('NA', gotLc$loan_startdate) gotLc$loan_startdate[selNA] <- NA gotLc$loan_startdate <- as.Date(gotLc$loan_startdate, format = ''%d %B %Y'') Let’s use the is.na function to find out the percentage of observations with missing values for start and/or end date. Here, we used the paste function to print the output as both a number and a percentage. # Missing start dates paste(sum(is.na(gotLc$loan_startdate)), ''('', 100 * round(mean(is.na(gotLc$loan_startdate)), 4), ''% )'') ## [1] ''6 ( 0.41 % )'' # Missing end dates paste(sum(is.na(gotLc$loan_enddate)), ''('', 100 * round(mean(is.na(gotLc$loan_enddate)), 4), ''% )'') ## [1] ''535 ( 36.15 % )'' Now we will add a new variable indicating the length of the loan period. As R knows that loan_startdate and loan_enddate are dates, it recognizes automatically that the difference between two dates should be expressed as the number of days. gotLc$loan_length <- gotLc$loan_enddate-gotLc$loan_startdate Let’s look at the first few observations. gotLc[1:5, c(''loan_startdate'', ''loan_enddate'', ''loan_length'')] ## # A tibble: 5 x 3 ## loan_startdate loan_enddate loan_length ## <date> <date> <time> ## 1 2004-03-01 NA <NA> ## 2 2006-11-01 NA <NA> ## 3 2006-11-01 2006-06-01 -153 ## 4 2005-06-01 2006-03-01 273 ## 5 2005-06-01 2006-02-01 245 Notice the following: Where any of the two dates is missing, the length is missing as well. Some loan lengths are negative (for example observation 3), because the recorded end date is before the start date. It could be that the two dates were switched when the data was entered into the system. This is unfortunate, but is a common feature of real-life data, and you will have to be on the lookout for such occasions. As required in Question 1, we will create two variants of the loan_length variable: one where we assign missing values to all observations that have negative loan_length, and one where we assume that the problem was the switching of start and end date, so we transform all loan lengths to positive values. gotLc$loan_length_NA <- gotLc$loan_length # Assign NA to negative loan lengths gotLc$loan_length_NA[gotLc$loan_length_NA<0] <- NA gotLc$loan_length_abs <- abs(gotLc$loan_length) Now we can create the long_term variable and look at the number of long-term loans. gotLc$long_term <- (gotLc$loan_length_abs>365) summary(gotLc$long_term) ## Mode FALSE TRUE NA's ## logical 728 215 537 We therefore have about 23% long-term loans (only looking at loans for which we do have date information). Using the variables loan_amount and loan_interest: Create summary tables to summarize the distribution of loan amount (mean, standard deviation, maximum, and minimum): one using the loan amount, the other using the total amount to repay (loan amount + interest). Make sure to exclude the one observation previously identified as having an extremely high interest rate. Remember to give your tables meaningful titles. Describe any features of the data that you find interesting. As mentioned earlier, the interest rate is a borrowing condition that can vary widely across households. Here we will take the interest rate to be the interest paid as a percentage of the loan amount. Calculate the interest rate for each loan in the data. (Exclude observations where the interest paid is not recorded.) Check for extreme values (interest rates that are either very large or zero). You may also want to create a scatterplot (with interest rate on the vertical axis and loan amount on the horizontal axis) to help you identify extreme (atypical) observations. Exclude the observation with the most extreme interest rate from further calculations. What percentage of the loans are zero interest? Make summary tables of the mean, maximum, minimum, and quartiles of the loan amount and interest rate, calculating these measures separately for long-term and short-term loans. Compare the distributions of interest rates for short-term and long-term loans. Create a table showing the correlation between the interest rate and household characteristics (you may want to refer to Figure 8.4 in Empirical Project 8 for an example). Interpreting the interest rate charged as a measure of default risk (inability to repay), explain whether the relationships implied by the coefficients are what you expected (for example, would you expect interest rates to be higher for households with less assets, more dependents, etc.). R walk-through 9.10 Making summary tables and calculating correlations To make summary tables, we use the favstats function from the mosaic package. # loan_amount favstats(~loan_amount, data = gotLc) ## min Q1 median Q3 max mean sd n missing ## 1 400 1200 3490 3e+07 26896 783587 1479 1 # loan_amount + loan_interest favstats(~(loan_amount + loan_interest), data = gotLc) ## min Q1 median Q3 max mean sd n missing ## 20 500 1400 3780 31260000 29223 827144 1445 35 It is best to look at loan amounts and interest rate graphically, for example in a scatterplot. ggplot(gotLc, aes(x = loan_amount, y = loan_interest)) + geom_point(size = 2, shape = 23, fill = ''blue'') + theme_bw() + ggtitle(''Loan amounts and interest payments'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Scatterplot showing loan amounts and interest payments. '' /> Scatterplot showing loan amounts and interest payments. Figure 9.5 Scatterplot showing loan amounts and interest payments. One large loan (top right corner) dominates this graph. Let’s exclude observations with a loan amount larger than 200,000 from the graph. ggplot(gotLc, aes(x = loan_amount, y = loan_interest)) + geom_point(size = 2, shape = 23, fill = ''blue'') + # Set horizontal axis limits xlim(0, 200000) + # Set vertical axis limits ylim(0, 30000) + theme_bw() + ggtitle(''Loan amounts and interest payments'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Revised scatterplot showing loan amounts and interest payments without outliers. '' /> Revised scatterplot showing loan amounts and interest payments without outliers. Figure 9.6 Revised scatterplot showing loan amounts and interest payments without outliers. Interestingly we can see many zero interest loans. Now we will calculate the interest rate as loan_interest/loan_amount. gotLc$interest_rate <- gotLc$loan_interest / gotLc$loan_amount favstats(~interest_rate, data = gotLc) ## min Q1 median Q3 max mean sd n missing ## 0 0 0 0.167 200 0.257 5.26 1445 35 The maximum interest rate is 200 (in other words 20,000%), which does not make sense and could be due to a data entry error. Making another scatterplot can also identify extreme values for loan amounts. ggplot(gotLc, aes(x = loan_amount, y = interest_rate)) + geom_point(size = 2, shape = 23, fill = ''blue'') + theme_bw() + ggtitle(''Loan amounts and interest rates'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Scatterplot identifying extreme values for loan amounts. '' /> Scatterplot identifying extreme values for loan amounts. Figure 9.7 Scatterplot identifying extreme values for loan amounts. Let’s make another scatterplot, excluding the observation with the extremely high interest rate and only looking at small loan amounts (< 1,000). ggplot(subset(gotLc, interest_rate < 50), aes(x = loan_amount, y = interest_rate)) + geom_point(size = 2, shape = 23, fill = ''blue'') + # Set horizontal axis limits xlim(0, 1000) + # Set vertical axis limits ylim(0, 5) + theme_bw() + ggtitle(''Loan amount and interest rates'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-08.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-08-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-08-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-08-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-09-08.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Scatterplot excluding extremely high interest rate and including only small loan amounts. '' /> Scatterplot excluding extremely high interest rate and including only small loan amounts. Figure 9.8 Scatterplot excluding extremely high interest rate and including only small loan amounts. Again we can see that there are many zero interest loans. From the summary statistics above, we can see that the median interest rate is 0, which implies that at least 50% of loans have a zero interest rate. The following code calculates that percentage precisely. temp_all <- gotLc$interest_rate[ !is.na(gotLc$interest_rate)] temp_0 <- temp_all[temp_all == 0] paste(''Percentage of zero interest rate loans: '', round(100 * length(temp_0) / length(temp_all), 2), ''%'') ## [1] ''Percentage of zero interest rate loans: 50.52 %'' Now let’s calculate statistics conditional on whether a loan is long term or not. Before we do this, we will remove the observation with the very extreme interest rate (20,000%) from our ‘gotLc’ dataset (but not from the original ‘gotL’ dataset). That observation has a loan amount of 1 and an interest payment of 200, which is probably a data entry mistake. There is another extreme observation (with a loan amount of 30,000,000), but there is no indication that this observation is misrecorded as there is a significant interest payment for this loan. gotLc <- subset(gotLc, interest_rate < 200) favstats(~interest_rate | long_term, data = gotLc) ## long_term min Q1 median Q3 max mean sd n missing ## 1 FALSE 0 0 0.050 0.171 1.12 0.110 0.173 717 0 ## 2 TRUE 0 0 0.141 0.245 2.24 0.189 0.268 211 0 Both the mean and median interest rate are higher for long-term loans. You can adapt the code above to calculate statistics for the loan_amount variable. We now calculate correlations between interest rates and household characteristics. Below we use piping operations (%>%) to select the relevant data (as in Project 8). We store the correlation coefficients in a matrix (array of rows and columns) called M. gotLc %>% # Only select observations with interest rate information subset(!is.na(interest_rate)) %>% select(age, max_education, number_assets, hhsize, young_children, working_age_adults, interest_rate) %>% cor(., use = ''pairwise.complete.obs'') -> M M[, c(''interest_rate'')] ## age max_education number_assets ## 0.0258 -0.0841 -0.0474 ## hhsize young_children working_age_adults ## 0.1050 0.1022 0.0466 ## interest_rate ## 1.0000 Now we will look at sources of finance and how they are related to loan characteristics. Create a table showing the proportion of loans (in terms of the column variable) with source of finance (borrowed_from) as the row variable and rural as the column variable. Make a similar table but with borrowed_from_other as the row variable instead. Does it look like rural households use different sources of finance from urban households? (Hint: It may help to think about sources of finance in terms of formal, informal, and other institutions such as microfinancers or NGOs.) For each of the variables below, create a table showing the average of that variable, with borrowed_from as the row variable and rural as the column variable. Comment on any similarities or differences between rows and columns that you find interesting, and suggest explanations for what you observe. duration of loan (using the variable in which negative durations were transformed to positive durations) loan amount interest rate Create a table showing the proportion of gender (in terms of the row variable) with borrowed_from as the row variable and rural as the column variable. Describe any relationships you observe between the gender of household head, the place where he/she lives, and the types of finance used. What other variables are currently not in our dataset but could also be important for our analysis in Questions 2 and 3? R walk-through 9.11 Creating summary tables of means First we use the table function to create the table with the variable borrowed_from. stab10 <- table(gotLc$borrowed_from, gotLc$rural) addmargins(prop.table(stab10, 2), 1) ## ## Large town (urban) Rural Small town (urban) ## Bank (commercial) 0.01881 0.00393 0.00000 ## Employer 0.04075 0.00196 0.01031 ## Grocery/Local Merchant 0.08150 0.04711 0.10309 ## Microfinance Institution 0.19122 0.27969 0.26804 ## Money Lender (Katapila) 0.00313 0.04809 0.02062 ## NGO 0.01254 0.04711 0.05155 ## Neighbour 0.10658 0.11580 0.07216 ## Other (specify) 0.05643 0.12071 0.04124 ## Relative 0.48589 0.31501 0.43299 ## Religious Institution 0.00313 0.02061 0.00000 ## Sum 1.00000 1.00000 1.00000 Note that in all settings, most loans come from relatives. To create the table with borrowed_from_other, substitute this variable name in the above command. When creating a table with categorical (factor) variables in the rows and columns, but with the cells reporting a statistic based on a third variable such as the average duration of a loan (rather than counts or proportions), we use piping operations (%>%). tab10 <- gotLc %>% group_by(borrowed_from, rural) %>% summarize(mean_duration = round(mean(loan_length_abs, na.rm = TRUE), 0)) %>% spread(rural, mean_duration) %>% print() ## # A tibble: 11 x 4 ## # Groups: borrowed_from [11] ## borrowed_from `Large town (urban)` Rural `Small town (urba~ ## <fct> <time> <time> <time> ## 1 Bank (commercial) 1814 619 <NA> ## 2 Employer 602 290 NaN ## 3 Grocery/Local Merchant 166 259 176 ## 4 Microfinance Institution 712 411 510 ## 5 Money Lender (Katapila) 365 332 365 ## 6 Neighbour 125 187 296 ## 7 NGO 372 395 236 ## 8 Other (specify) 274 372 806 ## 9 Relative 237 217 393 ## 10 Religious Institution 1461 343 <NA> ## 11 <NA> 1096 289 151 To get the tables for the loan amount and interest rate, change the variable name in the mean() calculation above. Extension Investigating sources of finance associated with zero interest loans We previously saw that a large percentage of loans have a zero interest rate. Here we investigate whether particular sources of finance are responsible for these interest rates. The code we use is very similar to the code above, but instead of calculating the mean of a variable, we calculate the mean of a boolean (true/false) variable ((interest_rate==0)). This will deliver the proportion of ‘true’ observations, in other words, loans where the interest rate was equal to zero. tab10 <- gotLc %>% group_by(borrowed_from, rural) %>% summarize(prop_0_interest = mean((interest_rate == 0), na.rm = TRUE)) %>% spread(rural, prop_0_interest) %>% print() ## # A tibble: 11 x 4 ## # Groups: borrowed_from [11] ## borrowed_from `Large town (urban)` Rural `Small town (urba~ ## <fct> <dbl> <dbl> <dbl> ## 1 Bank (commercial) 0. 0. NA ## 2 Employer 0.692 0.500 1.00 ## 3 Grocery/Local Merchant 1.00 0.812 1.00 ## 4 Microfinance Institution 0.0820 0.0351 0.0385 ## 5 Money Lender (Katapila) 0. 0.0204 0.500 ## 6 NGO 0.250 0.0833 0.400 ## 7 Neighbour 1.00 0.763 1.00 ## 8 Other (specify) 0.222 0.171 0. ## 9 Relative 0.961 0.819 0.976 ## 10 Religious Institution 1.00 0.190 NA ## 11 <NA> 0. 0.571 1.00 You can see that in both urban and rural settings, a high proportion of loans granted by local merchants, neighbours, and relatives are zero interest (possibly because these people have a close relationship with the borrower so there is a lower chance of default). We will use exactly the same technique to determine the proportion of loans that go to households with female heads. tab11 <- gotLc %>% group_by(borrowed_from, rural) %>% summarize(prop_female = mean((gender == ''Female''), na.rm = TRUE)) %>% spread(rural, prop_female) %>% print() ## # A tibble: 11 x 4 ## # Groups: borrowed_from [11] ## borrowed_from `Large town (urban)` Rural `Small town (urban~ ## <fct> <dbl> <dbl> <dbl> ## 1 Bank (commercial) 0. 0.500 NA ## 2 Employer 0.308 0. 0. ## 3 Grocery/Local Merchant 0.385 0.188 0.500 ## 4 Microfinance Institution 0.410 0.130 0.308 ## 5 Money Lender (Katapila) 0. 0.265 0.500 ## 6 NGO 0.250 0.312 0.200 ## 7 Neighbour 0.353 0.254 0.429 ## 8 Other (specify) 0.389 0.187 0.250 ## 9 Relative 0.400 0.206 0.286 ## 10 Religious Institution 1.00 0.238 NA ## 11 <NA> 1.00 0.429 1.00 In this project we have looked at patterns in borrowing and access to credit, but we are not able to make any causal statements such as ‘changes in X will cause households to be credit constrained’ or ‘characteristic Y causes improved access to credit’. Outline a policy intervention that could help improve households’ access to loans, and how to design the implementation so you can assess the causal effects of this policy."
});
index.addDoc({
    id: 50,
    title: "Doing Economics: Empirical Project 9: Working in Google Sheets",
    content: "Empirical Project 9 Working in Google Sheets Google Sheets-specific learning objectives In addition to the learning objectives for this project, in this section you will learn how to create and format time variables. Part 9.1 Households that did not get a loan Learning objectives for this part identify credit-constrained and credit-excluded households using survey information create dummy (indicator) variables compare characteristics of successful borrowers, discouraged borrowers, credit-constrained households, and credit-excluded households explain why selection bias is an important issue. The Ethiopian Socioeconomic Survey (ESS) data was collected in 2013–14 from a nationally representative sample of households. Households were asked about topics such as their housing conditions, assets, and access to credit. Download the ESS data and survey questionnaire: Download the ESS data. The Excel file contains three tabs (‘Data dictionary’, ‘All households’, and ‘Got loan’). Read the Data dictionary tab and make sure you know what each variable represents. (Later we will discuss exactly how some of these variables were constructed.) For the documentation, go to the data download site. Click on the ‘Documentation’ tab in the middle of the page. Under the heading ‘Questionnaires’, download the PDF file called ‘2013–2014 Ethiopian Socioeconomic Survey, Household Questionnaire’ by clicking the ‘Download’ button on the right-hand side of the page. You may find it helpful to refer to Section 14 of the questionnaire for the exact questions asked about credit and saving. The data is already in a format clean enough to use, so we will begin by summarizing the information in the ‘All households’ tab, starting with region and household characteristics. Create a pivot table showing the proportion of households that lived in each region and area type, with ‘region’ as the row variable and ‘rural’ as the column variable. (For help on creating pivot tables, see Google Sheets walk-through 3.1.) dummy variable (indicator variable)A variable that takes the value 1 if a certain condition is met, and 0 otherwise. Use Google Sheets’ IF function to create a variable that equals 1 if the household head was female, and 0 if the household head was male. Variables that are coded in this way are known as dummy variables (or indicator variables). What percentage of household heads were female? Create an appropriate summary table for the variables ‘hhsize’, ‘gender’, ‘age’, ‘young_children’, ‘working_age_adults’, ‘max_education’, and ‘number_assets’. (You may find it helpful to refer to Figure 2.9 in Empirical Project 2 for one possible format to use.) Write a short paragraph describing the information in your tables for 1(c). Now that we have an idea of what our data looks like, we will move on to identify households that are potentially excluded from the credit market or are credit constrained. The former are households who find it impossible to borrow and the latter are households who can only borrow on unfavourable terms (see Section 9.10 of Economy, Society, and Public Policy). The variables in our dataset that are related to this issue are ‘did_not_apply’ and ‘loan_rejected’. We will soon also look at the responses given in the variables ‘reason_not_apply1’ and ‘reason_not_apply2’. Using the ‘All households’ tab: Create a frequency table with ‘did_not_apply’ as the row variable and ‘loan_rejected’ as the column variable. Include the blanks as a separate row. Looking at these two variables, explain why some observations should be excluded and remove them from the dataset. Also remove all households with missing information for one or more of these variables. Of the non-excluded observations, what percentage of households applied for a loan over the past 12 months? Of those households, what percentage were successful? For the resulting categories in the frequency table, explain whether the households in that category can be described as credit constrained, credit excluded, or both. To create operational categories to use throughout this project, we will label households as either: ‘successful’: households that applied for a loan and were given the loan ‘denied’: households that applied but were not given the loan ‘did not apply’: households that did not apply for a loan. You should note that the ‘denied’ households are only a subset of the credit-excluded households as there will be households that are credit excluded and do not even apply for a loan. One could, for instance, reason that households who answered ‘Inadequate Collateral’ or ‘Do Not Know Any Lender’ are also likely to be credit excluded. Using the subset of data from Question 2(b): Create a new variable called ‘HH_status’ with the above categories. Create a new variable ‘discouraged_borrower’ that takes the value 1 if the household did not apply for a loan because it believed that it would not receive a loan (answered ‘Believe Would Be Refused’ in ‘reason_not_apply1’ or ‘reason_not_apply2’). How many households (and what percentage) are discouraged borrowers? (Note: this is a fairly narrow definition of ‘discouraged’ and one could easily argue that other criteria should also be considered under this label.) Note that arguably other answers are also indicative of being credit constrained, and so the criteria we use is definitely only a subset of all households that are credit constrained. For example, one could include households that have been denied a loan, and it is also indeed likely that some households that have been granted a loan are in fact credit constrained. Create a new variable ‘credit_constrained’ that takes the value 1 (or yes) for households that gave a reason for not applying other than ‘NA’, ‘Other’, or ‘Have Adequate Farm’ in either of the two questions ‘reason_not_apply1’ or ‘reason_not_apply2’, and 0 otherwise. For example, a household that answers ‘Have Adequate Farm’ in ‘reason_not_apply1’ and ‘Do Not Know Any Lender’ would not be classified as credit constrained. How many households (and what percentage) are credit constrained? Create a frequency table showing the most important reason for not applying for a loan, and another showing the second most important reason for not applying. What were the most common reasons for not applying? We will now analyse the stated reasons for wanting a loan, comparing those households that were successful (‘HH_status’ equal to ‘successful’) with those that were not successful (‘HH_status’ equal to ‘denied’). For both groups, create one table showing the proportion of households for each loan purpose. (You will realize that in the ‘All households’ tab, the reason for all successful loans is ‘Other’. For that reason, you should use the ‘Got loan’ tab to retrieve the reasons for loan information for successful loans.) Was the purpose of loans for denied and successful borrowers similar? (Hint: It may help to think about the broad categories of spending on consumption and investment). Using the information in the ‘All households’ tab and ‘Got loan’ tab, for each group of households: Create a table as shown in Figure 9.1 to compare the averages of the specified household characteristics. Household characteristic Successful Denied Age of household head Highest education in household Number of assets Household size Number of young children Number of working-age adults Characteristics of successful and denied borrowers. Figure 9.1 Characteristics of successful and denied borrowers. For each characteristic, explain how it may affect a household’s ability to get a loan (ceteris paribus). Looking at your table from Question 5(a), discuss whether you see this pattern in the data. (For example, are successful borrowers older/younger on average than denied borrowers?) conditional meanAn average of a variable, taken over a subgroup of observations that satisfy certain conditions, rather than all observations. Now try conditioning on the variable ‘rural’ or ‘region’ and discuss how (if at all) your results change. Using Figure 9.1, without conditioning on ‘rural’ or ‘region’, carry out the following: Calculate the difference in means (‘successful’ borrowers minus ‘denied’ borrowers). Calculate the 95% confidence interval for the difference in means between the two subgroups (‘successful’ minus ‘denied’). (Hint: Use Google Sheets’ CONFIDENCE.T function and see Part 8.3 of Empirical Project 8 for help on how to do this.) Plot a column chart showing the differences on the vertical axis (sorted from smallest to largest), and household characteristics on the horizontal axis. Add the confidence intervals from Question 6(b) to the chart. Interpret your findings. Using the information in the ‘All households’ tab: Create a table similar to Figure 9.1, but with additional columns for discouraged borrowers and credit-constrained households. Compare the means across the four groups and discuss any similarities/differences you observe (you do not need to do any formal calculations). selection biasAn issue that occurs when the sample or data observed is not representative of the population of interest. For example, individuals with certain characteristics may be more likely to be part of the sample observed (such as students being more likely than CEOs to participate in computer lab experiments). A study on access to loans in Ethiopia looked at the relationship between loan amount and household characteristics. When doing so, they needed to account for selection bias, because we only observe positive loan amounts for successful borrowers. If we only had data for successful borrowers, then our sample would not be representative of the population of interest (all households), so we would have to interpret our results with caution. In our case, we have information about all households, so we can compare observable characteristics to see whether successful borrowers are similar to other households. An article by the Institute for Work and Health explains selection bias in more detail, and why it is a problem encountered by all areas of research. Think of another example where there might be selection bias, in other words, where the data we observe is not representative of the population of interest. Part 9.2 Households that got a loan Learning objectives for this part analyse the characteristics of loans obtained by successful borrowers. For households that successfully got a loan, we will look at: purpose of the loan duration of the loan(s) loan amount and interest rate charged who the household borrowed from. We will also see if there are any relationships between these loan characteristics and household characteristics. Now we will use the variables relating to the loan start and end dates to calculate the duration of the loan. Before using these variables, we need to check that the variable entries make sense. Some of this information could be recorded incorrectly (for example, the year is missing a digit, or the month is a number rather than a word). Using the ‘Got loan’ tab, do the following: Check the variables ‘loan_startmonth’, ‘loan_startyear’, ‘loan_endmonth’, and ‘loan_endyear’ and replace the entries that are recorded incorrectly with either the correct entry (if possible), or as blank (if not possible to infer the correct entry). (Note: Some entries are recorded as ‘Pagume’, which corresponds to early September in the Ethiopian calendar.) To calculate loan duration, we need to combine the month and year variables into one date variable. Use Google Sheets’ CONCATENATE function to create new variables for the start and end date, and format them as date variables. (See Google Sheets walk-through 9.1 for help on how to do this.) Some of the dates (months or years) are missing. Calculate the percentage of the data that is missing and explain whether you think missing data is a serious problem. Create a new variable containing the loan duration (end date minus start date), which will be measured in days. You will notice that some dates were recorded incorrectly, with the start date later than the end date. We could either treat these entries as missing or swap the start and end dates. Create two new variables for loan duration, one with all negative entries recorded as blank, and one with negative entries replaced as positive numbers. (Hint: Google Sheets’ ABS function converts any number to a positive number.) For this project we will define a long-term loan as lasting more than a year (365 days), which we will use in later questions. For this definition use the loan length variable that converts negative loan lengths to positive ones. Create an indicator variable called ‘long_term’ that equals 1 if the loan was long term, and 0 otherwise. What percentage of loans were long term? Google Sheets walk-through 9.1 Creating and formatting time variables <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to create and format time variables using CONCATENATE. '' /> How to create and format time variables using CONCATENATE. Figure 9.2 How to create and format time variables using CONCATENATE. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : In this example, we will use the month and year variables in Columns O, P, R, and S to make new date variables in Columns V and W. Before doing this, we need to make sure all the months and years are coded correctly, otherwise Google Sheets may not recognize these values as dates. Also, some cells are blank, so we need to make our cell formulas work regardless of missing data. '' /> The data In this example, we will use the month and year variables in Columns O, P, R, and S to make new date variables in Columns V and W. Before doing this, we need to make sure all the months and years are coded correctly, otherwise Google Sheets may not recognize these values as dates. Also, some cells are blank, so we need to make our cell formulas work regardless of missing data. Figure 9.2a In this example, we will use the month and year variables in Columns O, P, R, and S to make new date variables in Columns V and W. Before doing this, we need to make sure all the months and years are coded correctly, otherwise Google Sheets may not recognize these values as dates. Also, some cells are blank, so we need to make our cell formulas work regardless of missing data. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Combine the month and year variables into one variable : The CONCATENATE function combines text in cells in the specified order. You can add punctuation and spaces by specifying them in quotation marks (“”). Here, we use the IF function so that Google Sheets only fills in cells in rows where both start and end dates were non-missing (the AND function states these conditions). '' /> Combine the month and year variables into one variable The CONCATENATE function combines text in cells in the specified order. You can add punctuation and spaces by specifying them in quotation marks (“”). Here, we use the IF function so that Google Sheets only fills in cells in rows where both start and end dates were non-missing (the AND function states these conditions). Figure 9.2b The CONCATENATE function combines text in cells in the specified order. You can add punctuation and spaces by specifying them in quotation marks (“”). Here, we use the IF function so that Google Sheets only fills in cells in rows where both start and end dates were non-missing (the AND function states these conditions). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Reformat the cells as date variables : After step 5, you may not notice any visible changes to the text in cells, but Google Sheets now recognizes them as dates and you can use them to make calculations, such as counting the number of days between two dates. '' /> Reformat the cells as date variables After step 5, you may not notice any visible changes to the text in cells, but Google Sheets now recognizes them as dates and you can use them to make calculations, such as counting the number of days between two dates. Figure 9.2c After step 5, you may not notice any visible changes to the text in cells, but Google Sheets now recognizes them as dates and you can use them to make calculations, such as counting the number of days between two dates. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Use date variables to calculate duration : With the proper formatting, Google Sheets can calculate the number of days between two dates. In this example, some of the start dates are later than the end dates, resulting in negative numbers. We will correct these values in the next step. '' /> Use date variables to calculate duration With the proper formatting, Google Sheets can calculate the number of days between two dates. In this example, some of the start dates are later than the end dates, resulting in negative numbers. We will correct these values in the next step. Figure 9.2d With the proper formatting, Google Sheets can calculate the number of days between two dates. In this example, some of the start dates are later than the end dates, resulting in negative numbers. We will correct these values in the next step. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-09-02-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Recode incorrect durations : The ABS function converts any value to its positive counterpart. After step 9, the negative values in Column X are now recorded as positive numbers in Column Y. Again, we used the AND function so that Google Sheets only did this for non-blank cells (in this case, the condition AND(X2&lt;&gt;“”) would give the same results). '' /> Recode incorrect durations The ABS function converts any value to its positive counterpart. After step 9, the negative values in Column X are now recorded as positive numbers in Column Y. Again, we used the AND function so that Google Sheets only did this for non-blank cells (in this case, the condition AND(X2<>“”) would give the same results). Figure 9.2e The ABS function converts any value to its positive counterpart. After step 9, the negative values in Column X are now recorded as positive numbers in Column Y. Again, we used the AND function so that Google Sheets only did this for non-blank cells (in this case, the condition AND(X2<>“”) would give the same results). Using the variables ‘loan_amount’ and ‘loan_interest’: Create summary tables to summarize the distribution of loan amount (mean, standard deviation, maximum, and minimum), one using the loan amount, the other using the total amount to repay (loan amount + interest). Make sure to exclude the one observation previously identified as having an extremely high interest rate. Remember to give your tables meaningful titles. Describe any features of the data that you find interesting. As mentioned earlier, the interest rate is a borrowing condition that can vary widely across households. Here we will take the interest rate to be the interest paid as a percentage of the loan amount. Calculate the interest rate for each loan in the data. (Exclude observations where the interest paid is not recorded.) Check for extreme values (interest rates that are either very large or zero). You may also want to create a scatterplot (with interest rate on the vertical axis and loan amount on the horizontal axis) to help you identify extreme (atypical) observations. Exclude the observation with the most extreme interest rate from further calculations. What percentage of the loans are zero interest? Make summary tables of the mean, maximum, minimum, and quartiles of the loan amount and interest rate, calculating these measures separately for long-term and short-term loans. Compare the distributions of interest rates for short-term and long-term loans. Create a table showing the correlation between the interest rate and household characteristics (you may want to refer to Figure 8.6 in Empirical Project 8 for an example). Interpreting the interest rate charged as a measure of default risk (inability to repay), explain whether the relationships implied by the coefficients are what you expected (for example, would you expect interest rates to be higher for households with less assets, more dependents etc.). For help on making scatterplots and calculating correlation coefficients, see Google Sheets walk-through 1.7. Now we will look at sources of finance and how they are related to loan characteristics. Create a table showing the proportion of loans (in terms of the column variable) with source of finance (‘borrowed_from’) as the row variable and ‘rural’ as the column variable. Make a similar table but with ‘borrowed_from_other’ as the row variable instead. Does it look like rural households use different sources of finance from urban households? (Hint: It may help to think about sources of finance in terms of formal, informal, and other institutions such as microfinancers or NGOs.) For each of the variables below, create a table showing the average of that variable, with ‘borrowed_from’ as the row variable and ‘rural’ as the column variable. Comment on any similarities or differences between rows and columns that you find interesting, and suggest explanations for what you observe. duration of loan (using the variable in which negative durations were transformed to positive durations) loan amount interest rate Create a table showing the proportion of ‘gender’ (in terms of the row variable) with ‘borrowed_from’ as the row variable and ‘rural’ as the column variable. Describe any relationships you observe between the gender of household head, the place where he/she lives, and the types of finance used. What other variables are currently not in our dataset but could also be important for our analysis in Questions 2 and 3? In this project we have looked at patterns in borrowing and access to credit, but we are not able to make any causal statements such as ‘changes in X will cause households to be credit constrained’ or ‘characteristic Y causes improved access to credit’. Outline a policy intervention that could help improve households’ access to loans, and how to design the implementation so you can assess the causal effects of this policy."
});
index.addDoc({
    id: 51,
    title: "Doing Economics: Empirical Project 9 Solutions",
    content: "Empirical Project 9 Solutions These are not model answers. They are provided to help students, including those doing the project outside a formal class, to check their progress while working through the questions using the Excel, R, or Google Sheets walk-throughs. There are also brief notes for the more interpretive questions. Students taking courses using Doing Economics should follow the guidance of their instructors. Part 9.1 Households that did not get a loan Solution figure 9.1 indicates the proportion of households that lived in each region and area type. Region Proportion in the region living in small towns Proportion of regional households living in large towns Proportion of regional households living in rural areas Proportion of all households living in the region Addis Ababa 0 1.00 0 0.06 Afar 0.15 0.10 0.76 0.03 Amhara 0.11 0.22 0.67 0.20 Benshagul Gumuz 0.10 0.00 0.90 0.02 Diredwa 0.00 0.47 0.53 0.04 Gambelia 0.08 0.12 0.80 0.02 Harari 0.00 0.27 0.73 0.03 Oromia 0.11 0.28 0.61 0.20 SNNP 0.09 0.18 0.73 0.23 Somalie 0.09 0.16 0.76 0.06 Tigray 0.07 0.37 0.56 0.12 Grand total 0.09 0.28 0.63 1.00 Proportion of sample living in towns vs rural areas, by ‘region’. (Note that numbers may not add up to 1 due to rounding.) Solution figure 9.1 Proportion of sample living in towns vs rural areas, by ‘region’. (Note that numbers may not add up to 1 due to rounding.) 30.40% of households have a female household head. Solution figure 9.2 provides the summary table for the variables (‘SD’ refers to standard deviation). Mean SD Min Max Household size 4.58 2.40 1.00 16.00 Gender 0.30 0.46 0.00 1.00 Age 44.18 15.61 3.00 99.00 Young children 1.89 1.71 0.00 10.00 Working-age adults 2.58 1.52 0.00 10.00 Max education 7.53 7.28 0.00 30.00 Number of assets 14.90 17.23 0.00 203.00 Summary table for household demographics (all households). Solution figure 9.2 Summary table for household demographics (all households). The average household size is 4.58 (or 5 people, rounding to the nearest person), with around 1–2 children and 2–3 working-age adults. On average, the most educated person in the household has had 7-8 years of schooling. Households have around 15 assets on average. Note that only households with information for all three variables are included. Not rejected Rejected Blank or NA Sum Applied for loan 1,363 201 1 1,565 Did not apply for loan 3,632 24 2 3,658 Blank or NA 37 2 0 39 Grand total 5,032 227 3 5,262 Loan applications and approvals. Solution figure 9.3 Loan applications and approvals. There are some responses that do not make sense. For example, 1,363 households applied for a loan and received the loan. Two households did not answer whether they applied for a loan but at the same time indicated that they were rejected a loan. In fact, 24 households indicated that they did not apply for a loan, yet they indicated that they were refused a loan. As the questions about loan application and loan rejection refer to the same 12-month period, these observations clearly make no sense, so we decided to exclude them from the data set analysed. This decision is contestable, since we exclude more than 10% of households that indicate that they were refused a loan. Thus, it is important to be transparent about this decision. We shall also remove all observations that have missing data for any of these two questions. This leaves us with 1,363 + 3,632 + 201 = 5,196 observations (those highlighted in green in Solution figure 9.3). Of these observations, 30.10% of households applied for a loan. 96.13% of these were successful. The 26.23% of households that applied for a loan and received one are clearly not credit excluded. However, they may still be credit constrained, as the mere fact that they received a loan says nothing about whether the terms were favourable or not (‘favourable’ being a criterion which may be somewhat difficult to pin down). The 69.90% of households that did not apply for a loan could be credit excluded, credit constrained, or neither as they may not have a need for a loan. It is here where additional information on the reasons for not applying will become important. The 3.87% of households that did apply but were refused a loan can be described as credit excluded. Create the ‘HH_status’ variable in your software. After creating this variable in your software, you should find that 588 (11.32%) households were discouraged borrowers (4,608 were not discouraged). 3,012 (57.97%) households were credit constrained (2,184 were not). Solution figures 9.4 and 9.5 provide frequency tables (with values expressed in proportions) showing the most important and the second most important reasons for not applying for a loan. Across the two tables it is clear that there is a substantial portion of households that do not want to be in debt. Others indicate that they do not believe they can afford a loan (‘Fear not to be able to pay’ or ‘Believe would be refused’). These households would certainly be considered credit constrained, or even credit excluded. Almost 20% of households also state that they do not have any need for a loan (‘Have adequate farm’). Reason Proportion Do Not Like To Be In Debt 0.19 Have Adequate Farm 0.19 Fear Not Be Able To Pay 0.17 Believe Would Be Refused 0.12 No Farm or Business 0.10 Do Not Know Any Lender 0.07 Too Expensive 0.05 Inadequate Collateral 0.05 Other (Specify) 0.04 Too Much Trouble 0.03 Most important reason for not applying for a loan. Solution figure 9.4 Most important reason for not applying for a loan. Reason Proportion Fear Not Be Able To Pay 0.28 Do Not Like To Be In Debt 0.24 Inadequate Collateral 0.09 Believe Would Be Refused 0.09 Too Expensive 0.07 Do Not Know Any Lender 0.06 Too Much Trouble 0.06 Have Adequate Farm 0.05 No Farm or Business 0.04 Other (Specify) 0.02 Second most important reason for not applying for a loan. Solution figure 9.5 Second most important reason for not applying for a loan. Solution figure 9.6 compares the loan purposes of ‘successful’ and ‘denied’ borrowers. Compared with all households, a greater proportion of households that got loans indicated the purpose for the loan as consumption, and a smaller proportion of households that got loans indicated the purpose as for investment. Note: The entry ‘10’ is likely due to incorrect recording of responses, since ‘10’ is not a valid category in the original survey. Purpose Successful Denied 10 0.00 0.02 Business start-up capital 0.15 0.26 Expanding business 0.08 0.14 Other (specify) 0.03 0.26 Purchase agricultural inputs for food crop 0.30 0.21 Purchase house/lease land 0.02 0.03 Purchase inputs for other crops 0.01 0.06 Purchase non-farm inputs 0.12 0.03 For consumption and personal expenses 0.20 0.00 Loan purpose for successful and denied (credit-excluded) borrowers. Solution figure 9.6 Loan purpose for successful and denied (credit-excluded) borrowers. Note Although most categories are the same, the two tables are not exactly comparable because ‘Consumption and personal expenses’ was a separate category from ‘Other’ for successful borrowers, but was considered as one of the responses in ‘Other (specify)’ for credit-excluded households. Looking at the variable loan_purpose_other in the ‘All households’ tab, we can see that reasons related to consumption and personal expenses (such as medical, school, transport expenses) apply to around 20 or so households. Solution figure 9.7 compares the averages of the specified household characteristics. Household characteristic Successful Denied Number of observations 1,363 201 Age of household head 43.40 41.20 Highest education in household 7.26 8.00 Number of assets 15.90 14.50 Household size 4.87 4.82 Number of young children 2.09 2.22 Number of working-age adults 2.75 2.76 Comparison of household characteristics (successful and denied borrowers). Solution figure 9.7 Comparison of household characteristics (successful and denied borrowers). Explanations of how characteristics may affect a household’s ability to get a loan: Younger households are more likely to be credit excluded compared to older people because they are more likely to have less assets and unstable employment. Less educated households are less likely to get higher paid jobs and hence less likely to repay the loans. We can thus expect more loans given to more educated people or people with a higher-educated household head. Assets can serve as collateral. We can expect households with less collateral to receive fewer loans because the risks involved for the lenders are higher. Households with more members to share the repayment burden are more likely to be able to repay loans. Having a greater number of children improves the prospects of repayments as the children will eventually grow up and become productive members of the family. Governments may provide benefits to families with more children. Having a greater number of working-age adults means greater repayment capability. The data reveals patterns consistent with our expectations, except in the pattern for the relation between education and loans. The data shows that credit-excluded people tend to have better education. One possible explanation is that people with more educated family members may be more likely to apply for loans because they are more likely to be aware of how to apply for loans and be more confident of their abilities to get a loan and repay. However, it is also possible that education is also correlated with other variable(s) that are significantly correlated with the likelihood of receiving a loan. There are many possibilities for conditioning on the variables ‘rural’ or ‘region’. The example in Solution figure 9.8 shows means conditioned on household status (‘successful’ or ‘denied’) and the ‘rural’ variable. Some patterns remain the same, though we can now see that in rural areas, successful borrowers have on average more education. In terms of numbers of children and working-age adults, the conditioning on the ‘rural’ variable changed the values. Now we can see that successful borrowers tend to have on average more children and more working-age adults. However, we ought to interpret these differences carefully as some of the groups have very small numbers of observations. Rural Small town (urban) Large town (urban) Successful Denied Successful Denied Successful Denied Number of observations 903 128 128 17 332 56 Age of household head 45.10 43.40 42.10 37.50 39.20 37.30 Highest education in household 5.00 4.55 9.59 13.12 12.51 14.3 Number of assets 13.60 12.10 15.70 16.70 22.10 19.20 Household size 5.35 5.47 4.12 4.06 3.84 3.57 Number of young children 2.49 2.84 1.56 1.41 1.20 1.05 Number of working-age adults 2.91 2.88 2.54 2.33 2.42 2.36 Comparison of household characteristics, conditioning on the ‘rural’ variable. Solution figure 9.8 Comparison of household characteristics, conditioning on the ‘rural’ variable. The difference in means is shown in the table in Solution figure 9.9. The standard deviation (SD) for the difference in means, and the number of observations in both groups, are shown in the table below. Successful borrowers Denied borrowers Difference in means Household characteristic Mean SD N Mean SD N Difference in means CI lower CI upper Age of household head 43.40 14.30 1,361 41.20 12.90 201 2.15 0.21 4.09 Highest education in household 7.26 6.74 1,361 8.00 7.90 201 −0.73 −1.89 0.42 Number of assets 15.90 19.00 1,361 14.50 16.80 201 1.42 −1.13 3.97 Household size 4.87 2.35 1,361 4.82 2.35 201 0.05 −0.30 0.40 Number of young children 2.09 1.68 1,361 2.22 1.80 201 −0.14 −0.40 0.13 Number of working-age adults 2.75 1.49 1,361 2.76 1.42 201 −0.003 −0.22 0.21 Calculating 95% confidence interval for difference in means between ‘successful’ and ‘denied’ borrowers. Solution figure 9.9 Calculating 95% confidence interval for difference in means between ‘successful’ and ‘denied’ borrowers. Solution figure 9.10 shows the differences in household characteristics. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-09-10.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-09-10-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-09-10-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-09-10-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-09-10.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Difference in means between ‘successful’ and ‘denied’ borrowers by household characteristics, with 95% confidence intervals. '' /> Difference in means between ‘successful’ and ‘denied’ borrowers by household characteristics, with 95% confidence intervals. Solution figure 9.10 Difference in means between ‘successful’ and ‘denied’ borrowers by household characteristics, with 95% confidence intervals. On average, successful borrowers have an older household head (2 years older) and 1–2 more assets than denied borrowers, though these differences are quite imprecisely estimated (as with the differences for all other characteristics), so it’s likely that these differences are due to chance. See Solution figure 9.11. Household characteristic Successful borrowers Denied borrowers Discouraged borrowers Constrained borrowers Number of observations 1,363 201 588 3,012 Age 43.37 41.21 43.28 44.84 Highest education in household 7.26 8.00 6.50 7.14 Number of assets 15.88 14.46 10.16 13.54 Household size 4.87 4.82 4.65 4.44 Number of young children 2.09 2.22 2.03 1.81 Number of working-age adults 2.75 2.76 2.49 2.49 Comparison of household characteristics by borrower type. Solution figure 9.11 Comparison of household characteristics by borrower type. We can see that, in addition to what we analysed before, discouraged borrowers have less education and fewer assets. Constrained borrowers have smaller household sizes. One example of selection bias is the study of the determinants of prosperity. To do this, we can compare developing with developed countries. However, only developing countries with adequate resources and willingness collect and submit their data. The set of developing countries for which we have data is thus not representative of the population of developing countries. Part 9.2 Households that got a loan No solution is provided. There are a total of 1,480 observations for these variables. There are six blanks for the start date (0.41%) and 535 (36.15%) for the end date. Hence for more than a third of observations we have no end date. No solution is provided. No solution is provided. No solution is provided. Percentage of the loans that were long term: 22.80% if using the measure of loan duration with negatives replaced by absolute values; and ignoring blanks (NAs). Numbers are rounded to the nearest whole number, and shown in Solution figure 9.12. Mean SD Min Max Loan amount (principal) 26,896 783,587 1 30,000,000 Total amount 29,223 827,144 20 31,260,000 Summary measures of loan amount (principal) and total amount. Solution figure 9.12 Summary measures of loan amount (principal) and total amount. No solution is provided. 50.52% of the loans are zero interest. There is one loan for which the interest rate is 200%. It may not be evident on a column chart (histogram), but you can also identify the extreme value(s) with a scatterplot, as shown in Solution figure 9.13. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-09-13.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-09-13-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-09-13-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-09-13-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-09-13.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Loan amounts and interest rates. '' /> Loan amounts and interest rates. Solution figure 9.13 Loan amounts and interest rates. Solution figure 9.14 shows summary tables of measures of the loan amount and the interest rate. (Note: It is good practice to show the number of observations in a particular group.) Short-term loans are, on average, for smaller amounts. A greater proportion of short-term loans offer very low interest rates. Smaller proportions of short-term loans offer high interest rates. The spread of interest rates and loan amounts is much larger for long-term loans than for short-term loans. Loan amount N Mean SD Min Max 1st quartile 2nd quartile 3rd quartile Long term 211 172,718 2,072,736 20 30,000,000 1,000 3,700 8,000 Short term 717 3,017 8,398 40 150,000 480 1,500 3,500 Interest rate N Mean SD Min Max 1st quartile 2nd quartile 3rd quartile Long term 211 0.19 0.27 0.00 2.24 0.00 0.14 0.25 Short term 717 0.11 0.17 0.00 1.12 0.00 0.05 0.17 Comparison of distribution of long-term and short-term loans. Solution figure 9.14 Comparison of distribution of long-term and short-term loans. Solution figure 9.15 shows the correlation between the interest rate and household characteristics. Here, we recoded gender as a dummy variable (1 = Female, 0 = Male). The correlations are all fairly weak. We would expect that lenders would charge lower interest rates to households with more working-age adults and more members because these households are better able to repay, and the risks associated with lending to them are therefore lower. This is not what we observe in the data. The rest of the correlations have the expected sign (positive/negative). Household characteristics Interest rate Household size 0.11 Gender –0.02 Age 0.03 Young children 0.10 Working-age adults 0.05 Max education −0.08 Number of assets –0.05 Correlations between interest rate and household characteristics. Solution figure 9.15 Correlations between interest rate and household characteristics. Solution figures 9.16 and 9.17 show the proportion of loans with sources of finance ‘borrowed_from’ and ‘borrowed_from_other’. (Numbers are quoted to 3 decimal places due to small values). Compared to rural households, urban households are more likely to borrow from banks and employers. Rural households are more likely to borrow from microfinance institutions and governments. Proportion of 'borrowed_from' Source of finance Large town (urban) Rural Small town (urban) Total Bank (commercial) 0.02 0.00 0.00 0.01 Employer 0.04 0.00 0.01 0.01 Grocery/Local Merchant 0.08 0.05 0.10 0.06 Microfinance Institution 0.19 0.28 0.27 0.26 Money Lender (Katapila) 0.00 0.05 0.02 0.04 Neighbour 0.11 0.12 0.07 0.11 NGO 0.01 0.05 0.05 0.04 Other (specify) 0.06 0.12 0.04 0.10 Relative 0.49 0.32 0.43 0.37 Religious Institution 0.00 0.02 0.00 0.02 Source of loan, by variable ‘rural’. Solution figure 9.16 Source of loan, by variable ‘rural’. Proportion of borrowed_from_other Source of finance Large town (urban) Rural Small town (urban) Total Bank 0.00 0.01 0.00 0.01 Cooperatives 0.42 0.43 0.40 0.43 Equib 0.05 0.01 0.00 0.01 From government 0.05 0.30 0.00 0.26 From individuals 0.05 0.01 0.00 0.01 From private business 0.00 0.02 0.00 0.02 From relatives 0.05 0.00 0.20 0.01 From women association 0.00 0.01 0.00 0.01 From Youth Association 0.00 0.01 0.00 0.01 HAB project 0.00 0.02 0.00 0.01 Iddir 0.16 0.15 0.00 0.14 Micro and small enterprise 0.11 0.00 0.00 0.01 Micro finance 0.00 0.04 0.40 0.05 Mobile 0.05 0.00 0.00 0.01 NGO 0.05 0.00 0.00 0.01 Source of loan, by variable ‘rural’ (‘Other’ category only). Solution figure 9.17 Source of loan, by variable ‘rural’ (‘Other’ category only). Solution figures 9.18, 9.19, and 9.20 provide tables for each of the variables. There are many possible comparisons to make, for example: Duration of loan: In large towns and rural areas, the average loan duration of banks is the longest. Across all types of areas, informal sources (such as relatives or neighbours) tend to lend for shorter durations on average. Loan amount: Banks lend larger amounts on average. Interest rate: The interest rate from informal sources is not necessarily higher (on average) than interest rates from formal sources such as banks. Lending from people that know the household (employer, relatives, neighbours) incurs lower average interest rates, possibly because the principal–agent problem is less severe. Source of finance Large town (urban) Rural Small town (urban) Bank (commercial) 1,814 619 0 Employer 602 290 0 Grocery/Local Merchant 166 259 176 Microfinance Institution 712 411 510 Money Lender (Katapila) 365 332 365 Neighbour 125 187 296 NGO 373 395 236 Other (specify) 274 372 806 Relative 237 217 393 Religious Institution 1,461 343 0 (blank) 1,096 289 151 Duration of loan (rounded to nearest day). Solution figure 9.18 Duration of loan (rounded to nearest day). Source of finance Large town (urban) Rural Small town (urban) Bank (commercial) 5,755,833 3,575 0 Employer 5,915 1,350 1,000 Grocery/Local Merchant 1,370 1,429 1,599 Microfinance Institution 14,525 3,852 7,543 Money Lender (Katapila) 350 1,360 5,150 Neighbour 602 837 686 NGO 5,532 1,4475 2,300 Other (specify) 3,912 1,842 3,829 Relative 7,872 1,576 6,802 Religious Institution 50,000 910 0 Loan amount (rounded to nearest whole number). Solution figure 9.19 Loan amount (rounded to nearest whole number). Source of finance Large town (urban) Rural Small town (urban) Bank (commercial) 0.14 0.26 0.00 Employer 0.04 0.06 0.00 Grocery/Local Merchant 0.00 0.09 0.00 Microfinance Institution 0.16 0.18 0.15 Money Lender (Katapila) 1.00 0.43 0.09 Neighbour 0.00 0.12 0.00 NGO 0.06 0.12 0.05 Other (specify) 0.13 0.16 0.22 Relative 0.01 0.08 0.00 Religious Institution 0.00 0.20 0.00 Interest rate (rounded to two decimal places). Solution figure 9.20 Interest rate (rounded to two decimal places). The following example uses a dummy variable called ‘Gender numerical’ which is equal to 1 for ‘Female’ and 0 for ‘Male’. The averages of this variable across categories are equivalent to the proportions. Households with female heads are relatively more likely to borrow from grocery/local merchants, neighbours, NGOs, and relatives. Source of finance Large town (urban) Rural Small town (urban) Bank (commercial) 0.00 0.50 0.00 Employer 0.31 0.00 0.00 Grocery/Local Merchant 0.39 0.19 0.50 Microfinance Institution 0.41 0.13 0.31 Money Lender (Katapila) 0.00 0.27 0.50 Neighbour 0.35 0.25 0.43 NGO 0.25 0.31 0.20 Other (specify) 0.39 0.19 0.25 Relative 0.40 0.21 0.29 Religious Institution 1.00 0.24 0.00 (blank) 1.00 0.43 1.00 Proportion of households with a female head, according to source of finance. Solution figure 9.21 Proportion of households with a female head, according to source of finance. Government policies such as tax reliefs and benefit, distance and/or access to various sources of financing, and stability of employment are all examples of variables that can be important. One hypothesis is that lack of knowledge of loan availability affects access by households to lending. The government could fund education programmes aimed at improving the poor’s understanding of financial services. The government could identify two poor regions with similar characteristics that are distant from each other. One region would serve as the control group while the other as the treatment group. The government could then randomly select and educate individuals from the treatment region. The causal effects of the policy could be assessed by comparing outcomes in the two regions after the treatment. During the study period, the government should avoid implementing other policies in the regions, especially those that can affect the regions differently. The two regions should be chosen such that they would evolve in similar ways without the policy, and that treatments on one region cannot indirectly affect outcomes in the other region."
});
index.addDoc({
    id: 52,
    title: "Doing Economics: Empirical Project 10: Characteristics of banking systems around the world",
    content: "Empirical project 10 Characteristics of banking systems around the world Learning objectives In this project you will: compare characteristics of banking systems around the world and across time use box and whisker plots to summarize distributions and identify outliers (Part 10.1) calculate weighted averages and explain the differences between weighted and simple averages (Part 10.1) use confidence intervals to assess changes in the stability of financial institutions before and after the 2008 global financial crisis (Part 10.2). Key concepts Concepts needed for this project: mean, median, standard deviation, box and whisker plot, correlation/correlation coefficient, and confidence interval (for difference in means). Concepts introduced in this project: weighted average. Introduction CORE projects This empirical project is related to material in: Unit 10 of Economy, Society, and Public Policy Unit 10 and Unit 17 of The Economy. If you are not familiar with banking systems and how they function, you are strongly encouraged to read one of these units before starting the project. Credit, money, and banks create opportunities for mutual gain, allowing firms and individuals to rearrange the timing of their spending by borrowing, lending, investing, and saving. When banking systems work well, they can allocate resources efficiently to firms and households, contributing to economic development and fostering economic wellbeing. However, if banking systems function poorly, they can hamper economic growth and make economies unstable, which in turn prevent households and firms from accessing or using the financial resources they need. A recent example of poorly functioning banking systems was the 2008 global financial crisis, which was largely due to banks increasing their borrowing to extend more loans for housing (often to riskier borrowers) and buying more financial assets based on bundles of these housing loans. (You can read more about the causes and effects of the global financial crisis in Section 17.8 of The Economy.) The crisis highlighted important issues in data collection and measurement, as policymakers lacked good quality, cross-country, and cross-time (so-called ‘time series’) data on financial systems. In response to this need for better data, researchers at the World Bank put together the Global Financial Development Database, which contains a variety of measures that policymakers can use to compare financial systems across countries and time. While stability is a key measure of interest after the 2008 global financial crisis, the Database also has information about other dimensions such as the size of financial systems and the degree to which individuals and firms can access financial services. We will be using this database to explore the following questions: How do banking systems around the world differ in size and in the access that they provide to financial services? Have banking systems become more stable since the 2008 global financial crisis? Working in Excel Working in R Working in Google Sheets"
});
index.addDoc({
    id: 53,
    title: "Doing Economics: Empirical Project 10: Working in Excel",
    content: "Empirical Project 10 Working in Excel Part 10.1 Summarizing the data Learning objectives for this part compare characteristics of banking systems around the world and across time use box and whisker plots to summarize distributions and identify outliers calculate weighted averages and explain the differences between weighted and simple averages. We will be using the World Bank’s Global Financial Development Database. Download the data and documentation: Go to the Global Financial Development Database. At the bottom of the page, click the ‘June 2017 Version’ to download the Excel file. (You can download a later version of the data, though your results may be slightly different from those shown here.) The paper ‘Benchmarking financial systems around the world’ gives an overview of the data. You may find it helpful to read Section 3 (pages 7–9) for a summary of the framework used to measure financial systems. The World Bank’s Global Financial Development Database contains information about four categories: financial depth: the size of financial institutions and markets access: the degree to which individuals are able to use financial services stability of financial institutions and markets efficiency of financial intermediaries and markets in facilitating financial transactions. We will be looking at the first three categories, focusing particularly on measures of stability before and after the 2008 global financial crisis. Each category is measured by a number of indicators. Figure 10.1 shows the indicators we will be using in this project. (Note that in other versions of the dataset, the indicators may be in lowercase instead of uppercase.) Category Indicator name Indicator code Depth Private credit by deposit money banks to GDP (%) GFDD.DI.01 Deposit money banks’ assets to GDP (%) GFDD.DI.02 Access Bank accounts per 1,000 adults GFDD.AI.01 Bank branches per 100,000 adults GFDD.AI.02 Firms with a bank loan or line of credit (%) GFDD.AI.03 Small firms with a bank loan or line of credit (%) GFDD.AI.04 Stability Bank Z-score GFDD.SI.01 Bank regulatory capital to risk-weighted assets (%) GFDD.SI.05 Indicators used in this project. Figure 10.1 Indicators used in this project. The ‘Definitions and Sources’ tab in your spreadsheet contains a description of all indicators in the Database. Use the information provided in the ‘Short Description’ column to explain briefly why each of the indicators listed in Figure 10.1 may be a good measure of that category, or may give misleading information about that category. (You may find it helpful to conduct some research on these measures, especially if the explanation contains technical terms). The ‘Data – June 2016’ tab contains the values of each indicator over time (1960–2014) for various countries around the world, though data may be missing for some countries and years. box and whisker plotA graphic display of the range and quartiles of a distribution, where the first and third quartile form the ‘box’ and the maximum and minimum values form the ‘whiskers’. To get an idea of what the distribution of values for each variable looks like, we will use box and whisker plots. Box and whisker plots are useful for looking at the distribution of a single variable and checking if there are many extreme values (either very large or very small, relative to the rest of the values). Make a separate box and whisker plot for each indicator, with the outliers displayed (see Excel walk-through 6.3 for help on how to do this). Comment on the overall distribution, the number of outliers, and suggest why there may be many outliers. (In Question 5, we will look at one way to handle extreme values if there is a concern that one or a few very extreme values will significantly affect the average.) Now we will use Excel’s PivotTable option to make summary tables of some indicators and look at how they have changed over time. Each country belongs to a particular region and income group. Choose one indicator in Depth and one indicator in Access: Make tables showing the average of those indicators and number of observations (count), with ‘Region’ or ‘Income Group’ as the column variable(s) and ‘Year’ (2000–2014 only) as the row variable. (Make a separate table for region and income group.) For each indicator chosen, make a line chart with the average indicator value (either for region or income group) as the vertical axis variable, and year as the horizontal axis variable. Comment on any patterns you see across regions/income groups and time. So far, we have been looking at simple averages where each observation is given the same weight (importance), so we simply add up all the numbers and divide by the number of observations. However, when we take averages across regions or income groups, we may want to account for the fact that countries differ in size (population or GDP). For example, if one country is far larger than another, we may want that country to have a larger influence on the average. See the box below for more about weighted averages. weighted averageA type of average that assigns greater importance (weight) to some components than to others, in contrast with a simple average, which weights each component equally. Components with a larger weight can have a larger influence on the average. Weighted averages An example of weighted averages that you have probably experienced is in calculating course grades. Usually, course grades are not calculated by simply summing up the scores in all components and dividing by the number of components. Instead, certain components such as the final exam are given more importance (influence over the overall grade) than the midterm exam or course assignments. To calculate the weighted average, we first determine the weight of each component (these are fractions or proportions that sum to 1). Then we multiply each component by its respective weight, and then sum over all components. Using the course grade as an example, suppose the final exam is worth 70% of the final grade and the midterm exam is worth 30%, with both being scored out of 100. Then the weighted average would be: % <![CDATA[ begin{align*} text{Overall grade (weighted average)} &=  (0.7timestext{Final exam score}) &+ (0.3 timestext{Midterm exam score}) end{align*} %]]> In comparison, the simple average would give both components equal weight: % <![CDATA[ begin{align*} text{Overall grade (simple average)} &=  (0.5timestext{Final exam score}) &+ (0.5timestext{Midterm exam score}) end{align*} %]]> To develop your intuition for this concept, you can experiment by choosing values for the final exam score and midterm exam score and seeing how a change in one of the scores affects the weighted and simple averages. The indicator ‘Bank regulatory capital to risk-weighted assets (%)’ in the Database also uses weights to account for the fact that some assets are riskier than others, and should therefore not be considered equally. We will practice calculating weighted averages for the indicator ‘Bank accounts per 1,000 adults’, weighting according to total population in each region (so countries with a larger population will have a larger influence on the average). Since data is missing for some countries, we will calculate the total population in each region as the total population for countries with non-missing data. For each region and for the years 2004–2014: In the ‘Data – June 2016’ tab, create a new variable for the weight, which is the ratio of ‘SP.POP.TOTL’ and the sum of ‘SP.POP.TOTL’ for each country within the relevant region and year. Only use observations that have non-missing values for the ‘GFDD.AI.01’ indicator variable (Bank accounts per 1,000 adults). (Hint: Use Excel’s IF function to separate data with non-missing values, then use Excel’s SUMIFS function, conditioning on ‘Region’ and ‘Year’.) Check that your answer is correct by filtering the data for a particular region and year and verifying that the weights sum to 1. Now multiply the ‘Bank accounts per 1,000 adults’ indicator by the weights in Question 4(a) and sum up the resulting values according to region to get the weighted average. (Hint: Excel’s PivotTable option can help you sum the values and put them in a table.) Compare your answers to Question 4(c) with the corresponding simple averages in Question 3(a) and comment on any similarities or differences. Extension Using Winsorization to handle extreme values If we are interested in combining indicators into a single index (as in Empirical Project 4), we may be concerned about extreme values, but still want to include these countries in the index (rather than excluding them from the calculations). When calculating summary statistics, we can deal with these extreme values by using the median instead of the mean. On page 19 of the paper ‘Benchmarking financial systems around the world’, the authors discuss Winsorization (replacing extreme values with either the 95th or the 5th percentile value) as one way to handle these extreme values. Sometimes the extreme values are due to peculiar features of a single country, so we might want to adjust the data to make the values ‘less extreme’. For an indicator you have used in Questions 3 and 4 and for the year 2010: Calculate the 95th and 5th percentile value of that indicator, across all countries. (Hint: Use Excel’s PERCENTILE.INC function.) Replace any value larger than the 95th percentile value with the 95th percentile value, and replace any value smaller than the 5th percentile value with the 5th percentile value. (Hint: Use Excel’s IF function.) Use your ‘Winsorized’ values from Question 5(b) to calculate the average values of the indicator, by region and income group (separately). Compare these values to the simple averages from Question 3(a). Part 10.2 Comparing financial stability before and after the 2008 global financial crisis Learning objectives for this part use confidence intervals to assess changes in the stability of financial institutions before and after the 2008 global financial crisis. Now we will assess whether financial stability (measured by the two indicators in Figure 10.1) has changed since the 2008 global financial crisis. For both indicators of stability in Figure 10.1, explain what effect the post-crisis banking regulations are likely to have on the value of the indicator (for example, would the value increase or decrease?), and why. You may find it helpful to research the regulations that were implemented as a result of the 2008 global financial crisis. For the years 2007 and 2014: Use Excel’s PivotTable option to make tables showing the average of those indicators, with ‘Region’ or ‘Income Group’ as the row variable(s) and ‘year’ (2000–2014 only) as the column variable. (Make a separate table for region and income group.) Add a column showing the difference in means (2014 minus 2007). Add four extra columns containing the standard deviation and number of observations for each year. Calculate the standard deviation for the difference in means, and the number of observations in both years. Use Excel’s CONFIDENCE.T function to calculate the 95% confidence interval ‘width’ of the difference in means (the distance from one end of the interval to the mean). (See Part 8.3 of Empirical Project 8 for help on how to do this.) For each indicator: Plot column charts (one for regions, one for income groups) showing the differences on the vertical axis and indicator on the horizontal axis. Add the confidence intervals from Question 2(d) to your charts. leverage ratio (for banks or households)The value of assets divided by the equity stake (capital contributed by owners and shareholders) in those assets. Interpret your findings. Is there evidence that stability has increased since the 2008 global financial crisis? (Note that ‘Bank regulatory capital to risk-weighted assets’ is inversely related to the leverage ratio. High leverage ratios were common in the lead-up to the 2008 crisis, and contributed to financial instability.)"
});
index.addDoc({
    id: 54,
    title: "Doing Economics: Empirical Project 10: Working in R",
    content: "Empirical Project 10 Working in R Download the code To download the code chunks used in this project, right-click on the download link and select ‘Save Link As…’. You’ll need to save the code download to your working directory, and open it in RStudio. Don’t forget to also download the data into your working directory by following the steps in this project. Getting started in R For this project you will need the following packages: tidyverse, to help with data manipulation readxl, to import an Excel spreadsheet knitr, to format tables. If you need to install any of these packages, run the following code: install.packages(c(''tidyverse'', ''readxl'', ''knitr'')) You can import the libraries now, or when they are used in the R walk-throughs below. library(tidyverse) library(readxl) library(knitr) Part 10.1 Summarizing the data Learning objectives for this part compare characteristics of banking systems around the world and across time use box and whisker plots to summarize distributions and identify outliers calculate weighted averages and explain the differences between weighted and simple averages. We will be using the World Bank’s Global Financial Development Database. First, download the data and documentation: Go to the Global Financial Development Database. At the bottom of the page, click the ‘June 2017 Version’ to download the Excel file. (You can download a later version of the data, though your results will be slightly different from those shown here, and you will need to adjust parts of the code e.g. file names.) The paper ‘Benchmarking financial systems around the world’ gives an overview of the data. You may find it helpful to read Section 3 (pages 7–9) for a summary of the framework used to measure financial systems. The World Bank’s Global Financial Development Database contains information about four categories: financial depth: the size of financial institutions and markets access: the degree to which individuals are able to use financial services stability of financial institutions and markets efficiency of financial intermediaries and markets in facilitating financial transactions. We will be looking at the first three categories, focusing particularly on measures of stability before and after the 2008 global financial crisis. Each category is measured by a number of indicators. Figure 10.1 shows the indicators we will be using in this project. (Note that in other versions of the dataset, the indicators may be in lowercase instead of uppercase.) Category Indicator name Indicator code Depth Private credit by deposit money banks to GDP (%) GFDD.DI.01 Deposit money banks’ assets to GDP (%) GFDD.DI.02 Access Bank accounts per 1,000 adults GFDD.AI.01 Bank branches per 100,000 adults GFDD.AI.02 Firms with a bank loan or line of credit (%) GFDD.AI.03 Small firms with a bank loan or line of credit (%) GFDD.AI.04 Stability Bank Z-score GFDD.SI.01 Bank regulatory capital to risk-weighted assets (%) GFDD.SI.05 Indicators used in this project. Figure 10.1 Indicators used in this project. The ‘Definition and Sources’ tab in the Excel spreadsheet contains a description of all indicators in the Database. Use the information provided in the ‘Short Description’ column to explain briefly why each of the indicators listed in Figure 10.1 may be a good measure of that category, or may give misleading information about that category. (You may find it helpful to conduct some research on these measures, especially if the explanation contains technical terms). The ‘Data – June 2016’ tab contains the values of each indicator over time (1960–2014) for various countries around the world, though data may be missing for some countries and years. R walk-through 10.1 Importing an Excel spreadsheet into R Before loading an Excel spreadsheet into R, open it in Excel to understand the structure of the spreadsheet and the data it contains. In this case we can see that detailed descriptions of all variables are in the first tab (‘Definitions and Sources’). Make sure to read the definitions for the indicators listed in Figure 10.1. The spreadsheet contains a number of other worksheets, but the data that we need is in the tab called ‘Data – June 2016’. You can see that the variable names are all in the first row and missing values are simply empty cells. We can therefore proceed to import the data into R using the read_excel function without any additional options. library(tidyverse) library(readxl) library(knitr) # Set your working directory to the correct folder. # Insert your file path for 'YOURFILEPATH'. setwd(''YOURFILEPATH'') GFDD <- read_excel( ''GlobalFinancialDevelopmentDatabaseJune2017.xlsx'', sheet = ''Data - June 2016'') box and whisker plotA graphic display of the range and quartiles of a distribution, where the first and third quartile form the ‘box’ and the maximum and minimum values form the ‘whiskers’. To get an idea of what the distribution of values for each variable looks like, we will use box and whisker plots. Box and whisker plots are useful for looking at the distribution of a single variable and checking if there are many extreme values (either very large or very small, relative to the rest of the values). Make a separate box and whisker plot for each indicator, with the outliers displayed (see Empirical Project 6 for help on how to do this). Comment on the overall distribution, the number of outliers, and suggest why there may be many outliers. (In Question 5, we will look at one way to handle extreme values if there is a concern that one or a few very extreme values will heavily affect the average.) R walk-through 10.2 Making box and whisker plots Box and whisker plots were introduced in Empirical Project 6. We can use the same process here, after ensuring that the data is in the correct format. ggplot expects the data to be in ‘long’ format (each row is a value for a single variable and year), whereas our data is in ‘wide’ format (each row contains a single variable but multiple years). We transform the data from wide to long format using the gather function. Further details on the difference between long and wide format data can be found in ‘The Wide and Long Data Format for Repeated Measures Data’. We also have to remove any missing values using the na.omit function, otherwise the ggplot function will not plot the chart. So before creating our box plots, we will select the relevant variables (saving them in a list called indicators), reshape the data (gather) and remove missing values (na.omit), and finally save it as a temporary dataframe called df.new. We can run this sequence of commands in one go using the piping operator (%>%) from the tidyverse package. (For a more detailed introduction to piping, see the University of Manchester’s Econometric Computing Learning Resource.) So for the Depth indicators: # Rename and specify the variables to plot names(GFDD)[names(GFDD) == ''GFDD.DI.01''] <- ''private.credit'' names(GFDD)[names(GFDD) == ''GFDD.DI.02''] <- ''bank.assets'' # Select the correct variables and remove missing values indicators <- c(''private.credit'', ''bank.assets'') df.new <- GFDD[, indicators] %>% # Reshape wide to long format gather(., Indicator, value) %>% # Remove missing values na.omit() # Plot the data # The Indicator variable determines the plot's 'color'. ggplot(df.new) + geom_boxplot(aes(x = Indicator, y = value, color = Indicator), lwd = 1) + ylab(''Value'') + theme_bw() <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Box and whisker plot for ‘Private credit by deposit money banks to GDP (%)’ (private.credit) and ‘Deposit money banks’ assets to GDP (%)’ (bank.assets). '' /> Box and whisker plot for ‘Private credit by deposit money banks to GDP (%)’ (private.credit) and ‘Deposit money banks’ assets to GDP (%)’ (bank.assets). Figure 10.2 Box and whisker plot for ‘Private credit by deposit money banks to GDP (%)’ (private.credit) and ‘Deposit money banks’ assets to GDP (%)’ (bank.assets). We could repeat the process for each topic and plot all indicators together. However, the range for the GFDD.AI.01 variable (Bank accounts per 1,000 adults) is far larger than the other variables in this group, so it makes sense to plot this separately. We use the same process as before, as shown below. names(GFDD)[names(GFDD) == ''GFDD.AI.01''] <- ''bank.accounts'' indicators <- c(''bank.accounts'') df.new <- GFDD[, indicators] %>% gather(., Indicator, value) %>% na.omit() ggplot(df.new) + geom_boxplot(aes(x = Indicator, y = value, color = Indicator), lwd = 1) + ylab(''Value'') + theme_bw() <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Box and whisker plot for ‘Bank accounts per 1,000 adults’ (bank.accounts). '' /> Box and whisker plot for ‘Bank accounts per 1,000 adults’ (bank.accounts). Figure 10.3 Box and whisker plot for ‘Bank accounts per 1,000 adults’ (bank.accounts). names(GFDD)[names(GFDD) == ''GFDD.AI.02''] <- ''bank.branches'' names(GFDD)[names(GFDD) == ''GFDD.AI.03''] <- ''firms.credit'' names(GFDD)[names(GFDD) == ''GFDD.AI.04''] <- ''small.firms.credit'' indicators <- c(''bank.branches'', ''firms.credit'', ''small.firms.credit'') df.new <- GFDD[, indicators] %>% gather(., Indicator, value) %>% na.omit() ggplot(df.new) + geom_boxplot(aes(x = Indicator, y = value, color = Indicator), lwd = 1) + ylab(''Value'') + theme_bw() <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Box and whisker plot for ‘Bank branches per 100,000 adults’ (bank.branches), ‘Firms with a bank loan or line of credit (%)’ (firms.credit), and ‘Small firms with a bank loan or line of credit (%)’ (small.firms.credit). '' /> Box and whisker plot for ‘Bank branches per 100,000 adults’ (bank.branches), ‘Firms with a bank loan or line of credit (%)’ (firms.credit), and ‘Small firms with a bank loan or line of credit (%)’ (small.firms.credit). Figure 10.4 Box and whisker plot for ‘Bank branches per 100,000 adults’ (bank.branches), ‘Firms with a bank loan or line of credit (%)’ (firms.credit), and ‘Small firms with a bank loan or line of credit (%)’ (small.firms.credit). You can repeat the process for the indicators on bank stability by copying the above code and replacing the indicator variable names accordingly. We will now create summary tables of some indicators and look at how they have changed over time. Each country belongs to a particular region and income group. Choose one indicator in Depth and one indicator in Access: Make tables showing the average of those indicators and number of observations (count), with Region or Income Group as the column variable(s) and Year (2000–2014 only) as the row variable. (Make a separate table for region and income group.) For each indicator chosen, make a line chart with the average indicator value (either for region or income group) as the vertical axis variable, and year as the horizontal axis variable. Comment on any patterns you see across regions/income groups over time. R walk-through 10.3 Tabulating and visualizing time trends In this walk-through we will use the indicators for ‘Deposit money banks’ assets to GDP (%)’ and ‘Bank accounts per 1,000 adults’ as examples (bank.assets and bank.accounts respectively). Obtaining the average indicator value for each year and region is straightforward using the group_by and summarize functions, but again we have to select the relevant years (using filter) and remove any observations that have a missing value for the indicator being analysed (using is.na). We save the final output as deposit_region. # Save into new dataframe deposit_region <- GFDD %>% # We only want observations from 2000 to 2014. subset(Year > 1999 & Year < 2015) %>% # We need to average by year and region. group_by(Year, Region) %>% # Remove observations with missing values filter(!is.na(bank.assets)) %>% # Get the mean (rounded to 2 decimal places) # and number of observations summarize(mean = round(mean(bank.assets), 2), n = n()) At this stage the summary data is stored in long format. head(deposit_region) ## # A tibble: 6 x 4 ## # Groups: Year [1] ## Year Region mean n ## <dbl> <chr> <dbl> <int> ## 1 2000 East Asia & Pacific 67.6 25 ## 2 2000 Europe & Central Asia 58.6 45 ## 3 2000 Latin America & Caribbean 46.9 33 ## 4 2000 Middle East & North Africa 60.9 17 ## 5 2000 North America 68.5 2 ## 6 2000 South Asia 28.1 7 This format is useful for plotting the data, but to produce the required table (with Region as the column variable and Year as the row variable), we need to reshape the data into wide format. While we previously used gather to move from wide to long, we can use the spread function to achieve the opposite and transform the data from long to wide. Trying to produce complicated tables with multiple values (average and counts) for each year and region can be cumbersome. In this example, we can combine the mean and count into a single entry (with the count in brackets) using the paste function. This will help with the formatting of the table. deposit_region_table <- deposit_region %>% # Create a new variable for formatting purposes mutate(new = paste(mean, ''('', n, '')'')) %>% # Drop the old average and count variables select(-n, -mean) %>% # Reshape the data from long to wide spread(Region, new) At this point you could just print or view the data, however using the kable function from the knitr package produces output that is visually easier to read and can be copied and pasted into your analysis document. kable(deposit_region_table, format=''markdown'', align = ''r'', digits = 2) | Year| East Asia & Pacific| Europe & Central Asia| Latin America & Caribbean| Middle East & North Africa| North America| South Asia| Sub-Saharan Africa| |----:|-------------------:|---------------------:|-------------------------:|--------------------------:|-------------:|-----------:|------------------:| | 2000| 67.56 ( 25 )| 58.61 ( 45 )| 46.93 ( 33 )| 60.92 ( 17 )| 68.46 ( 2 )| 28.12 ( 7 )| 20.64 ( 43 )| | 2001| 67.25 ( 25 )| 58.1 ( 46 )| 48.8 ( 33 )| 62.51 ( 18 )| 83.1 ( 2 )| 29.39 ( 7 )| 20.53 ( 44 )| | 2002| 62.32 ( 26 )| 58.67 ( 47 )| 49.1 ( 33 )| 61.92 ( 19 )| 94.57 ( 2 )| 32.06 ( 7 )| 20.5 ( 45 )| | 2003| 61.24 ( 26 )| 60.47 ( 47 )| 48.25 ( 33 )| 59.39 ( 19 )| 92.08 ( 2 )| 33.15 ( 7 )| 21.43 ( 45 )| | 2004| 60.62 ( 26 )| 62.51 ( 47 )| 46.69 ( 33 )| 57.02 ( 20 )| 91.04 ( 2 )| 36.02 ( 7 )| 20.99 ( 45 )| | 2005| 62.69 ( 25 )| 67.64 ( 46 )| 47.28 ( 33 )| 56.14 ( 20 )| 94.11 ( 2 )| 38.77 ( 7 )| 21.48 ( 46 )| | 2006| 63.32 ( 25 )| 73.5 ( 47 )| 47.17 ( 33 )| 56.31 ( 20 )| 100.84 ( 2 )| 34.78 ( 8 )| 22.12 ( 44 )| | 2007| 64.1 ( 25 )| 79.04 ( 46 )| 48.88 ( 33 )| 55.71 ( 21 )| 101.98 ( 2 )| 37.7 ( 8 )| 22.78 ( 44 )| | 2008| 68.48 ( 25 )| 85.71 ( 46 )| 51.88 ( 33 )| 57.94 ( 21 )| 104.46 ( 2 )| 41.53 ( 8 )| 24.38 ( 44 )| | 2009| 74.95 ( 25 )| 92.9 ( 44 )| 55.29 ( 33 )| 65.27 ( 21 )| 66.73 ( 1 )| 43.88 ( 8 )| 26.32 ( 42 )| | 2010| 75.56 ( 25 )| 91.18 ( 46 )| 54.3 ( 32 )| 64.12 ( 21 )| 60.5 ( 1 )| 45.36 ( 8 )| 26.15 ( 43 )| | 2011| 75.97 ( 23 )| 90.08 ( 46 )| 54.39 ( 32 )| 65.47 ( 20 )| 59.34 ( 1 )| 45.63 ( 8 )| 26.94 ( 43 )| | 2012| 75.11 ( 24 )| 89.88 ( 46 )| 55.87 ( 32 )| 65.36 ( 20 )| 58.29 ( 1 )| 45.79 ( 8 )| 27.45 ( 41 )| | 2013| 79.25 ( 24 )| 88.83 ( 46 )| 56.44 ( 32 )| 66.47 ( 20 )| 58.08 ( 1 )| 46.44 ( 8 )| 28.39 ( 40 )| | 2014| 86.66 ( 23 )| 87.06 ( 46 )| 57.33 ( 31 )| 75.13 ( 18 )| 60.28 ( 1 )| 45.5 ( 8 )| 29.01 ( 38 )| We can use ggplot to plot a line chart using the long format data (deposit_region), with year on the horizontal axis (formatted as a factor variable so the data will be plotted in chronological order). We specify color = Region and group = Region to group the data by region. # Plot a line chart ggplot(deposit_region, aes(x = factor(Year), y = mean, color = Region)) + geom_line(aes(group = Region), size = 1) + xlab(''Year'') + ggtitle(''Deposit money banks' assets to GDP (%), 2000-2014, by region'') + ylab(''Mean deposit, % of GDP'') + theme_bw() <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Line chart of ‘Deposit money banks’ assets to GDP (%)’, 2000–2014, by region. '' /> Line chart of ‘Deposit money banks’ assets to GDP (%)’, 2000–2014, by region. Figure 10.5 Line chart of ‘Deposit money banks’ assets to GDP (%)’, 2000–2014, by region. The process can be repeated for income group rather than region. However, be careful when specifying the variable Income Group, which needs backtick quotes (i.e. `Income Group`) because it contains a space in the variable name. deposit_income <- GFDD %>% subset(Year > 1999) %>% # Note variable name in backticks group_by(Year, `Income Group`) %>% filter(!is.na(bank.assets)) %>% summarize(mean = round(mean(bank.assets), 2), n = n()) deposit_income %>% # Create a new variable for formatting purposes mutate(new = paste(mean, ''('', n, '')'')) %>% select(-n, -mean) %>% spread(`Income Group`, new) %>% kable(., format = ''markdown'', align = ''r'', digits = 2) | Year| High income: nonOECD| High income: OECD| Low income| Lower middle income| Upper middle income| |----:|--------------------:|-----------------:|------------:|-------------------:|-------------------:| | 2000| 63.8 ( 25 )| 89.67 ( 32 )| 15.26 ( 26 )| 28.1 ( 45 )| 45.6 ( 44 )| | 2001| 67.17 ( 25 )| 90.06 ( 32 )| 14.96 ( 26 )| 27.85 ( 47 )| 46.79 ( 45 )| | 2002| 68.69 ( 26 )| 91.16 ( 32 )| 14.98 ( 27 )| 27.61 ( 48 )| 45.17 ( 46 )| | 2003| 67.29 ( 26 )| 92.75 ( 32 )| 15.71 ( 27 )| 27.76 ( 48 )| 44.8 ( 46 )| | 2004| 63.35 ( 27 )| 94.17 ( 32 )| 15.2 ( 27 )| 28.61 ( 48 )| 44.92 ( 46 )| | 2005| 62.19 ( 27 )| 98.79 ( 32 )| 15.25 ( 27 )| 30.36 ( 48 )| 46.68 ( 45 )| | 2006| 62.77 ( 27 )| 105.78 ( 32 )| 15.86 ( 26 )| 30.35 ( 48 )| 48.37 ( 46 )| | 2007| 65.25 ( 27 )| 110.92 ( 31 )| 16.57 ( 26 )| 32.02 ( 48 )| 50.24 ( 47 )| | 2008| 68.73 ( 27 )| 117.73 ( 31 )| 18.28 ( 26 )| 34.73 ( 48 )| 54.25 ( 47 )| | 2009| 79.37 ( 25 )| 123.16 ( 30 )| 19.11 ( 25 )| 37.65 ( 47 )| 58.59 ( 47 )| | 2010| 78.77 ( 26 )| 120.75 ( 30 )| 20.3 ( 25 )| 37.23 ( 48 )| 58.52 ( 47 )| | 2011| 78 ( 26 )| 118.81 ( 29 )| 21.58 ( 25 )| 37.88 ( 46 )| 58.91 ( 47 )| | 2012| 78.85 ( 26 )| 117.64 ( 29 )| 21.19 ( 23 )| 38.63 ( 47 )| 59.95 ( 47 )| | 2013| 80.12 ( 26 )| 115.07 ( 29 )| 22.87 ( 23 )| 40.28 ( 46 )| 61.48 ( 47 )| | 2014| 83.81 ( 25 )| 112.49 ( 29 )| 23.56 ( 22 )| 42.46 ( 44 )| 64.68 ( 45 )| ggplot(deposit_income, aes(x = factor(Year), y = mean, color = `Income Group`)) + geom_line(aes(group = `Income Group`), size = 1) + xlab(''Year'') + ggtitle(''Deposit money banks' assets to GDP (%), 2000-2014, by income group'') + ylab(''Mean deposit, % of GDP'') + theme_bw() <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Line chart of ‘Deposit money banks’ assets to GDP (%)’, 2000–2014, by income group. '' /> Line chart of ‘Deposit money banks’ assets to GDP (%)’, 2000–2014, by income group. Figure 10.6 Line chart of ‘Deposit money banks’ assets to GDP (%)’, 2000–2014, by income group. We can repeat the process for the indicator ‘Bank accounts per 1,000 adults’ by replacing the variable name bank.assets with bank.accounts in the above code, again by region and then by income group. So far, we have been looking at simple averages, where each observation is given the same weight (importance), so we simply add up all the numbers and divide by the number of observations. However, when we take averages across regions or income groups, we might want to account for the fact that countries differ in size (population or GDP). For example, if one country is much larger than another, we might want that country to have a larger influence on the average. See the box below for more about weighted averages. weighted averageA type of average that assigns greater importance (weight) to some components than to others, in contrast with a simple average, which weights each component equally. Components with a larger weight can have a larger influence on the average. Weighted averages An example of weighted averages that you have probably experienced is in calculating course grades. Usually, course grades are not calculated by simply summing up the scores in all components and dividing by the number of components. Instead, certain components such as the final exam are given more importance (influence over the overall grade) than the midterm exam or course assignments. To calculate the weighted average, we first determine the weight of each component (these are fractions or proportions that sum to 1). Then we multiply each component by its respective weight, and then sum over all components. Using the course grade as an example, suppose the final exam is worth 70% of the final grade and the midterm exam is worth 30%, with both being scored out of 100. Then the weighted average would be: % <![CDATA[ begin{align*} text{Overall grade (weighted average)} &=  (0.7~times~text{Final exam score}) &+ (0.3 ~times~text{Midterm exam score}) end{align*} %]]> In comparison, the simple average would give both components equal weight: % <![CDATA[ begin{align*} text{Overall grade (simple average)} &=  (0.5~times~text{Final exam score}) &+ (0.5~times~text{Midterm exam score}) end{align*} %]]> To develop your intuition for this concept, you can experiment by choosing values for the final exam score and midterm exam score and seeing how a change in one of the scores affects the weighted and simple averages. The indicator ‘Bank regulatory capital to risk-weighted assets (%)’ in the Database also uses weights to account for the fact that some assets are riskier than others, and should therefore not be considered equally. We will practice calculating weighted averages for the indicator ‘Bank accounts per 1,000 adults’, weighting according to total population in each region (so countries with a larger population will have a larger influence on the average). Since data is missing for some countries, we will calculate the total population in each region as the total population for countries with non-missing data. For each region and the years 2004–2014: Create a new variable for the weight that is the ratio of SP.POP.TOTL and the sum of SP.POP.TOTL for each country within the relevant region and year. Only use observations that have non-missing values for the GFDD.AI.01 (Bank accounts per 1,000 adults) indicator variable. Check that your calculations are correct by filtering the data for a particular region and year and verifying that the weights sum to 1. Now multiply the ‘Bank accounts per 1,000 adults’ indicator by the weights in Question 4(b) and sum up the resulting values according to region to get the weighted average. Compare your answers to Question 4(c) with the corresponding simple averages in Question 3(a) and comment on any similarities or differences. R walk-through 10.4 Creating weighted averages As we only require the weighted averages for the years 2004–2014, we will create a new dataframe (called weighted_GFDD) to save our results in. The weights are required for each country within each region for each year, but only if there is a value for the GFDD.AI.01 (bank.accounts) indicator, so we group by year and then region (using group_by). We can then generate the weight for each country by dividing the population of each country by the sum of populations of all countries within a region (and year). weighted_GFDD <- GFDD %>% # Select years of interest subset(Year > 2003 & Year < 2015) %>% # Select variables of interest select(''Year'', ''Country'', ''Region'', ''bank.accounts'', ''SP.POP.TOTL'') %>% # Only keep observations with non-missing data filter(!is.na(bank.accounts)) %>% # Group by year and then country group_by(Year, Region) %>% # Generate country weights mutate(weight = SP.POP.TOTL / sum(SP.POP.TOTL)) After transforming and manipulating data, stop and check that you have obtained the desired result. In this case, the sum of the weights for a given region in a particular year should be 1, so let’s check whether that is the case. weighted_GFDD %>% group_by(Year, Region) %>% summarize(total = sum(weight)) %>% head() ## # A tibble: 6 x 3 ## # Groups: Year [1] ## Year Region total ## <dbl> <chr> <dbl> ## 1 2004 East Asia & Pacific 1 ## 2 2004 Europe & Central Asia 1 ## 3 2004 Latin America & Caribbean 1 ## 4 2004 Middle East & North Africa 1 ## 5 2004 South Asia 1 ## 6 2004 Sub-Saharan Africa 1 This is correct, so we can proceed to calculate the required weighted indicator values by year and region. We start by creating a new variable with the weighted indicator value (bank.accounts.weighted), and then sum up the weighted indicator values by year and region. Recall that when calculating the weighted average, you sum all of the weighted observations rather than taking the mean (which would calculate the simple average instead). Again, our data will be in long format so it needs to be converted to wide format before printing out. weighted_GFDD %>% # Apply the weighting mutate(bank.accounts.weighted = bank.accounts * weight) %>% group_by(Year, Region) %>% # Round the summed weights to 1 decimal place summarize(weighted_mean = round(sum(bank.accounts.weighted), 2)) %>% # Reshape the data spread(Region, weighted_mean) %>% kable(., format = ''markdown'', align = ''r'', digits = 2) | Year| East Asia & Pacific| Europe & Central Asia| Latin America & Caribbean| Middle East & North Africa| South Asia| Sub-Saharan Africa| |----:|-------------------:|---------------------:|-------------------------:|--------------------------:|----------:|------------------:| | 2004| 253.86| 637.40| 504.48| 301.03| 529.82| 64.48| | 2005| 336.63| 1262.14| 471.72| 324.82| 531.27| 76.57| | 2006| 81.43| 1324.44| 484.82| 351.77| 549.06| 79.62| | 2007| 85.34| 1380.43| 520.49| 393.35| 573.51| 132.21| | 2008| 87.54| 1525.15| 561.82| 418.40| 614.92| 149.09| | 2009| 90.62| 1501.88| 633.35| 436.77| 227.03| 195.23| | 2010| 94.60| 1371.62| 686.92| 431.48| 267.47| 220.34| | 2011| 105.83| 1419.89| 688.62| 420.16| 334.88| 236.72| | 2012| 96.85| 1269.52| 736.57| 426.43| 373.07| 277.92| | 2013| 101.48| 1043.82| 755.75| 427.68| 399.85| 309.17| | 2014| 106.33| 1059.82| 780.49| 524.85| 411.35| 350.80| As usual, there are several ways to achieve the same thing in R. You may want to work out how to get the same results with the weighted.mean function in R. Extension Using Winsorization to handle extreme values If we are interested in combining indicators into a single index (as in Empirical Project 4), we may be concerned about extreme values, but still want to include these countries in the index (rather than excluding them from the calculations). When calculating summary statistics, we can deal with these extreme values by using the median instead of the mean. On page 19 of the paper ‘Benchmarking financial systems around the world’, the authors discuss Winsorization (replacing extreme values with either the 95th or the 5th percentile value) as one way to handle these extreme values. Sometimes the extreme values are due to peculiar features of a single country, so we might want to adjust the data to make the values ‘less extreme’. For an indicator you have used in Questions 3 and 4 and for the year 2010: Calculate the 95th and 5th percentile value of that indicator, across all countries. Replace any value larger than the 95th percentile value with the 95th percentile value, and replace any value smaller than the 5th percentile value with the 5th percentile value. Use your ‘Winsorized’ values from Question 5(b) to calculate the average values of the indicator, by region and income group (separately). Compare these values to the simple averages from Question 3(a). Extension R walk-through 10.5 Dealing with extreme values In this example we use ‘Bank accounts per 1,000 adults’ (bank.accounts). The 95th and 5th percentiles can be obtained using the quantiles function. We save the output into a tibble (similar to a dataframe) so we can refer to the values in later calculations. q_5_95 <- GFDD %>% subset(Year == 2010) %>% filter(!is.na(bank.accounts)) %>% summarize( lower = quantile(bank.accounts, probs = c(0.05)), upper = quantile(bank.accounts, probs = c(0.95))) %>% print() ## # A tibble: 1 x 2 ## lower upper ## <dbl> <dbl> ## 1 27.6 1605. We can compare the value of the indicator with these upper and lower bounds using the ifelse function. An example of using this function to nest two conditions was used in R walk-through 8.8. Here, we use a similar syntax, first replacing all values below the 5th percentile with the value for the 5th percentile, and then replacing all values above the 95th percentile with the value for the 95th percentile. bank_2010 <- GFDD %>% subset(Year == 2010) %>% mutate(bank.accounts = ifelse(bank.accounts < q_5_95$lower, q_5_95$lower, ifelse(bank.accounts > q_5_95$upper, q_5_95$upper, bank.accounts))) Next we can obtain our summary statistics and print out the ‘Winsorized’ averages. bank_2010 %>% subset(Year == 2010) %>% group_by(`Income Group`) %>% filter(!is.na(bank.accounts)) %>% summarize(`2010 average` = mean(bank.accounts)) %>% kable(., format = ''markdown'', align = ''r'', digits = 2) | Income Group| 2010 average| |--------------------:|------------:| | High income: nonOECD| 916.90| | High income: OECD| 1356.47| | Low income| 123.06| | Lower middle income| 422.65| | Upper middle income| 635.71| Part 10.2 Comparing financial stability before and after the 2008 global financial crisis Learning objectives for this part use confidence intervals to assess changes in the stability of financial institutions before and after the 2008 global financial crisis. Now we will assess whether financial stability (measured by the two indicators in Figure 10.1) has changed since the 2008 global financial crisis. For both indicators of stability in Figure 10.1, explain what effect the post-global financial crisis banking regulations are likely to have on the value of the indicator (for example, would the value increase or decrease?), and why. You may find it helpful to research the regulations that were implemented as a result of the 2008 global financial crisis. For the years 2007 and 2014, using the t.test function, create tables with Region or Income Group as the row variables(s), and the difference in the average of those indicators between the two years, the 95% confidence interval lower and upper values, and the CI width, as column variables. R Walk-Through 10.6 Calculating confidence intervals In R walk-throughs 3.6 and 8.10 we used the t.test function to obtain differences in means and confidence intervals (CIs) for two groups of data. Here we need to obtain these statistics for the GFDD.SI.05 indicator (renamed as risk.weighted.assets) between 2007 and 2014 for each region. As we need to find the confidence intervals for a number of regions, we can use a for loop to perform the same calculation for each region in turn. We get the full list of regions using the levels function. We start by defining an empty dataframe (df.ttest), and for each region, we will apply the t.test function. We can then extract the confidence interval upper and lower limits, and compute the difference in the means (as the midpoint) and the CI width. names(GFDD)[ names(GFDD) == ''GFDD.SI.05''] <- ''risk.weighted.assets'' df.ttest <- data.frame(region = factor(), mean = double(), lower = double(), upper = double(), width = double()) for (i in levels(as.factor(GFDD$`Region`))) { t <- t.test( GFDD$risk.weighted.assets[ GFDD$`Region` == i & GFDD$Year == 2014], GFDD$risk.weighted.assets[ GFDD$`Region` == i & GFDD$Year == 2007]) df.ttest <- rbind(df.ttest, data.frame( region = i, mean = (t$conf.int[1] + t$conf.int[2]) / 2, lower = t$conf.int[1], upper = t$conf.int[2], width =(t$conf.int[2] - t$conf.int[1]) / 2)) } print(df.ttest) ## region mean lower upper width ## 1 East Asia & Pacific 1.86333333 -2.0207518 5.747418 3.884085 ## 2 Europe & Central Asia 2.73065657 0.7143307 4.746982 2.016326 ## 3 Latin America & Caribbean 0.35294118 -1.1443531 1.850235 1.497294 ## 4 Middle East & North Africa 0.05666667 -2.9481918 3.061525 3.004858 ## 5 North America 0.50000000 -11.6928842 12.692884 12.192884 ## 6 South Asia 3.06000000 -1.3680600 7.488060 4.428060 ## 7 Sub-Saharan Africa 1.29263158 -2.5820985 5.167362 3.874730 The same process can be repeated for income groups and for the indicator GFDD.SI.01 (Bank Z-score). For each indicator: Plot column charts (one for regions and one for income groups) showing the differences on the vertical axis and indicator on the horizontal axis. Add the confidence intervals from Question 2 to your charts. leverage ratio (for banks or households)The value of assets divided by the equity stake (capital contributed by owners and shareholders) in those assets. Interpret your findings. Is there evidence that stability has increased since the 2008 global financial crisis? (Note that Bank regulatory capital to risk-weighted assets is inversely related to the leverage ratio. High leverage ratios were common in the lead-up to the 2008 crisis, and contributed to financial instability.) R Walk-Through 10.7 Plotting column charts with error bars Again we use the GFDD.SI.05 indicator (risk.weighted.assets) for Region as an example. You can repeat the following steps by region and for the risk.weighted.assets variable by changing the variable name(s) in R walk-through 10.6 accordingly, then running the code below. The data required to plot a column chart of the difference in means for each income group (using ggplot and geom_bar) was obtained in Question 2. To add confidence intervals to the chart we use the geom_errorbar option (see R walk-through 6.4 for more details on how you can use this option). As the labels for each income group are quite long (wider than the columns) they will overlap on the horizontal axis. To prevent this, we rotate the labels using the themes option. (As with many problems in R, we found the solution by searching the internet for ‘ggplot rotate axis labels’ and adapting the code from the first result.) ggplot(df.ttest, aes(y = mean, x = region)) + geom_bar(stat = ''identity'', position = ''identity'', fill = ''grey'') + geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, lwd = 1) + theme_bw() + theme(axis.text.x = element_text( angle = 30, hjust = 1)) + ylab(''Difference'') + xlab(''Region'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-08.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-08-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-08-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-08-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-10-08.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Column chart with error bars for ‘Bank regulatory capital to risk-weighted assets (%)’ (risk.weighted.assets). '' /> Column chart with error bars for ‘Bank regulatory capital to risk-weighted assets (%)’ (risk.weighted.assets). Figure 10.7 Column chart with error bars for ‘Bank regulatory capital to risk-weighted assets (%)’ (risk.weighted.assets)."
});
index.addDoc({
    id: 55,
    title: "Doing Economics: Empirical Project 10: Working in Google Sheets",
    content: "Empirical Project 10 Working in Google Sheets Part 10.1 Summarizing the data Learning objectives for this part compare characteristics of banking systems around the world and across time use box and whisker plots to summarize distributions and identify outliers calculate weighted averages and explain the differences between weighted and simple averages. We will be using the World Bank’s Global Financial Development Database. Download the data and documentation: Go to the Global Financial Development Database. At the bottom of the page, click the ‘June 2017 Version’ to download the Excel file. (You can download a later version of the data, though your results may be slightly different from those shown here.) The paper ‘Benchmarking financial systems around the world’ gives an overview of the data. You may find it helpful to read Section 3 (pages 7–9) for a summary of the framework used to measure financial systems. The World Bank’s Global Financial Development Database contains information about four categories: financial depth: the size of financial institutions and markets access: the degree to which individuals are able to use financial services stability of financial institutions and markets efficiency of financial intermediaries and markets in facilitating financial transactions. We will be looking at the first three categories, focusing particularly on measures of stability before and after the 2008 global financial crisis. Each category is measured by a number of indicators. Figure 10.1 shows the indicators we will be using in this project. (Note that in other versions of the dataset, the indicators may be in lowercase instead of uppercase.) Category Indicator name Indicator code Depth Private credit by deposit money banks to GDP (%) GFDD.DI.01 Deposit money banks’ assets to GDP (%) GFDD.DI.02 Access Bank accounts per 1,000 adults GFDD.AI.01 Bank branches per 100,000 adults GFDD.AI.02 Firms with a bank loan or line of credit (%) GFDD.AI.03 Small firms with a bank loan or line of credit (%) GFDD.AI.04 Stability Bank Z-score GFDD.SI.01 Bank regulatory capital to risk-weighted assets (%) GFDD.SI.05 Indicators used in this project. Figure 10.1 Indicators used in this project. The ‘Definitions and Sources’ tab in your spreadsheet contains a description of all indicators in the Database. Use the information provided in the ‘Short Description’ column to explain briefly why each of the indicators listed in Figure 10.1 may be a good measure of that category, or may give misleading information about that category. (You may find it helpful to conduct some research on these measures, especially if the explanation contains technical terms). The ‘Data – June 2016’ tab contains the values of each indicator over time (1960–2014) for various countries around the world, though data may be missing for some countries and years. box and whisker plotA graphic display of the range and quartiles of a distribution, where the first and third quartile form the ‘box’ and the maximum and minimum values form the ‘whiskers’. To get an idea of what the distribution of values for each variable looks like, we will use box and whisker plots. Box and whisker plots are useful for looking at the distribution of a single variable and checking if there are many extreme values (either very large or very small, relative to the rest of the values). Make a separate box and whisker plot for each indicator, with the outliers displayed (see Google Sheets walk-through 6.3 for help on how to do this). Comment on the overall distribution, the number of outliers, and suggest why there may be many outliers. (In Question 5, we will look at one way to handle extreme values if there is a concern that one or a few very extreme values will significantly affect the average.) Now we will use Google Sheets’ PivotTable option to make summary tables of some indicators and look at how they have changed over time. Each country belongs to a particular region and income group. Choose one indicator in Depth and one indicator in Access: Make tables showing the average of those indicators and number of observations (count), with ‘Region’ or ‘Income Group’ as the column variable(s) and ‘Year’ (2000–2014 only) as the row variable. (Make a separate table for region and income group.) For each indicator chosen, make a line chart with the average indicator value (either for region or income group) as the vertical axis variable, and year as the horizontal axis variable. Comment on any patterns you see across regions/income groups and time. So far, we have been looking at simple averages where each observation is given the same weight (importance), so we simply add up all the numbers and divide by the number of observations. However, when we take averages across regions or income groups, we may want to account for the fact that countries differ in size (population or GDP). For example, if one country is far larger than another, we may want that country to have a larger influence on the average. See the box below for more about weighted averages. weighted averageA type of average that assigns greater importance (weight) to some components than to others, in contrast with a simple average, which weights each component equally. Components with a larger weight can have a larger influence on the average. Weighted averages An example of weighted averages that you have probably experienced is in calculating course grades. Usually, course grades are not calculated by simply summing up the scores in all components and dividing by the number of components. Instead, certain components such as the final exam are given more importance (influence over the overall grade) than the midterm exam or course assignments. To calculate the weighted average, we first determine the weight of each component (these are fractions or proportions that sum to 1). Then we multiply each component by its respective weight, and then sum over all components. Using the course grade as an example, suppose the final exam is worth 70% of the final grade and the midterm exam is worth 30%, with both being scored out of 100. Then the weighted average would be: % <![CDATA[ begin{align*} text{Overall grade (weighted average)} &=  (0.7timestext{Final exam score}) &+ (0.3 timestext{Midterm exam score}) end{align*} %]]> In comparison, the simple average would give both components equal weight: % <![CDATA[ begin{align*} text{Overall grade (simple average)} &=  (0.5timestext{Final exam score}) &+ (0.5timestext{Midterm exam score}) end{align*} %]]> To develop your intuition for this concept, you can experiment by choosing values for the final exam score and midterm exam score and seeing how a change in one of the scores affects the weighted and simple averages. The indicator ‘Bank regulatory capital to risk-weighted assets (%)’ in the Database also uses weights to account for the fact that some assets are riskier than others, and should therefore not be considered equally. We will practice calculating weighted averages for the indicator ‘Bank accounts per 1,000 adults’, weighting according to total population in each region (so countries with a larger population will have a larger influence on the average). Since data is missing for some countries, we will calculate the total population in each region as the total population for countries with non-missing data. For each region and for the years 2004–2014: In the ‘Data – June 2016’ tab, create a new variable for the weight, which is the ratio of ‘SP.POP.TOTL’ and the sum of ‘SP.POP.TOTL’ for each country within the relevant region and year. Only use observations that have non-missing values for the ‘GFDD.AI.01’ indicator variable (Bank accounts per 1,000 adults). (Hint: Use Google Sheets’ IF function to separate data with non-missing values, then use Google Sheets’ SUMIFS function, conditioning on ‘Region’ and ‘Year’.) Check that your answer is correct by filtering the data for a particular region and year and verifying that the weights sum to 1. Now multiply the ‘Bank accounts per 1,000 adults’ indicator by the weights in Question 4(a) and sum up the resulting values according to region to get the weighted average. (Hint: Google Sheets’ PivotTable option can help you sum the values and put them in a table.) Compare your answers to Question 4(c) with the corresponding simple averages in Question 3(a) and comment on any similarities or differences. Extension Using Winsorization to handle extreme values If we are interested in combining indicators into a single index (as in Empirical Project 4), we may be concerned about extreme values, but still want to include these countries in the index (rather than excluding them from the calculations). When calculating summary statistics, we can deal with these extreme values by using the median instead of the mean. On page 19 of the paper ‘Benchmarking financial systems around the world’, the authors discuss Winsorization (replacing extreme values with either the 95th or the 5th percentile value) as one way to handle these extreme values. Sometimes the extreme values are due to peculiar features of a single country, so we might want to adjust the data to make the values ‘less extreme’. For an indicator you have used in Questions 3 and 4 and for the year 2010: Calculate the 95th and 5th percentile value of that indicator, across all countries. (Hint: Use Google Sheets’ PERCENTILE.INC function.) Replace any value larger than the 95th percentile value with the 95th percentile value, and replace any value smaller than the 5th percentile value with the 5th percentile value. (Hint: Use Google Sheets’ IF function.) Use your ‘Winsorized’ values from Question 5(b) to calculate the average values of the indicator, by region and income group (separately). Compare these values to the simple averages from Question 3(a). Part 10.2 Comparing financial stability before and after the 2008 global financial crisis Learning objectives for this part use confidence intervals to assess changes in the stability of financial institutions before and after the 2008 global financial crisis. Now we will assess whether financial stability (measured by the two indicators in Figure 10.1) has changed since the 2008 global financial crisis. For both indicators of stability in Figure 10.1, explain what effect the post-crisis banking regulations are likely to have on the value of the indicator (for example, would the value increase or decrease?), and why. You may find it helpful to research the regulations that were implemented as a result of the 2008 global financial crisis. For the years 2007 and 2014: Use Google Sheets’ PivotTable option to make tables showing the average of those indicators, with ‘Region’ or ‘Income Group’ as the row variable(s) and ‘year’ (2000–2014 only) as the column variable. (Make a separate table for region and income group.) Add a column showing the difference in means (2014 minus 2007). Add four extra columns containing the standard deviation and number of observations for each year. Calculate the standard deviation for the difference in means, and the number of observations in both years. Use Google Sheets’ CONFIDENCE.T function to calculate the 95% confidence interval ‘width’ of the difference in means (the distance from one end of the interval to the mean). (See Part 8.3 of Empirical Project 8 for help on how to do this.) For each indicator: Plot column charts (one for regions, one for income groups) showing the differences on the vertical axis and indicator on the horizontal axis. Add the confidence intervals from Question 2(d) to your charts. leverage ratio (for banks or households)The value of assets divided by the equity stake (capital contributed by owners and shareholders) in those assets. Interpret your findings. Is there evidence that stability has increased since the 2008 global financial crisis? (Note that ‘Bank regulatory capital to risk-weighted assets’ is inversely related to the leverage ratio. High leverage ratios were common in the lead-up to the 2008 crisis, and contributed to financial instability.)"
});
index.addDoc({
    id: 56,
    title: "Doing Economics: Empirical Project 10 Solutions",
    content: "Empirical Project 10 Solutions These are not model answers. They are provided to help students, including those doing the project outside a formal class, to check their progress while working through the questions using the Excel, R, or Google Sheets walk-throughs. There are also brief notes for the more interpretive questions. Students taking courses using Doing Economics should follow the guidance of their instructors. Note These solutions are based on the June 2017 version. Your solutions will differ slightly if using other versions. Part 10.1 Summarizing the data Explanations of why each indicator may be a good measure or may give misleading information of that category follow: Depth Private credit by deposit money banks to GDP (%). The amount of outstanding credit extended by banks to the non-financial private sector by deposit money banks measured relative to a country’s GDP is a measure of the size of the financial sector. (Deposit money banks are resident banks, called commercial banks in Unit 10 of Economy, Society, and Public Policy, which have liabilities in the form of deposits payable on demand, transferable by electronic transfer, cheque, or otherwise usable for making payments.) However, countries vary in terms of the government’s role in financial service provision and in the extent of ownership of enterprises by the government rather than the private sector. This measure may therefore underestimate financial depth in countries where the government is more dominant. Deposit money banks’ assets to GDP (%). This is a broader measure and whilst it includes lending to government-owned enterprises, for example, it also includes other assets (such as government bonds) that are not directly related to bank lending in the economy. Access Bank accounts per 1,000 adults. Countries with more bank accounts per 1,000 adults would have better access, ceteris paribus. However, countries differ in terms of the preference for accessing formal sources of finance, the credibility of the institutions, and the nature of the services. The cross-country differences in this measure may reflect differences in these factors rather than in access. Also, this measure does not account for the possibility that one person can have multiple bank accounts. Bank branches per 100,000 adults. Bank branches provide an easy, efficient and trustworthy platform for people to access financial services. The number of branches reflects financial institutions’ dedication to and presence in a country. If the additional branches are located in different areas, more people would be able to physically reach them and access the services. However, if the additional branches are all concentrated in the same area, then the effect on access would be small. Given the variation across countries of the distribution of branches, this measure should be used with caution. Also, many banking services can be done online rather than at a physical branch, requiring fewer physical branches in a country. This feature of modern banking is not captured by this measure. Firms with a bank loan or line of credit. Firms, compared with other economic actors, have a greater preference for and hence demand for financial services. The advantage of using this measure is that the variations arising from varying preferences for loans across countries would be lower. Once again, however, the measure is determined by many other factors which may vary across countries. Small firms with a bank loan or line of credit (%). Smaller firms, including those that are starting up, are more likely to face credit rationing or exclusion for the reasons discussed in Unit 10 of Economy, Society, and Public Policy, making this measure of particular interest to policymakers. Stability Bank Z-score. This is a measure of the probability of a default in a country’s banking system and is calculated as a weighted average (using the total assets of individual banks as the weights) of the Z-scores of the individual banks. The Z-score links a bank’s capitalization with the rate of return it is making and the volatility of those returns. A higher Z-score indicates greater banking stability. This measure does not, however, take account of the interconnectedness of banks. Bank regulatory capital to risk-weighted assets (%). The more regulatory capital banks have, relative to their assets, the more capable they are of withstanding negative shocks. However, due to cross-country differences in accounting and policies, the data is not directly comparable across countries. The box and whisker plots are shown in Solution figures 10.1–10.8. For most indicators, the data are quite tightly clustered together (as shown by the narrow width of the box). Extreme values appear to be an issue for most indicators (except ‘Firms with a bank loan or line of credit’, and ‘Small firms with a bank loan or line of credit’). One possible reason for a large number of outliers is that the way banking is done can vary greatly across countries, for example, countries that rely heavily on online banking would have far fewer bank branches per 100,000 adults than countries in which transactions are mostly done in-person. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Box and whisker plot: Private credit by deposit money banks to GDP (%). '' /> Box and whisker plot: Private credit by deposit money banks to GDP (%). Solution figure 10.1 Box and whisker plot: Private credit by deposit money banks to GDP (%). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Box and whisker plot: Deposit money banks’ assets to GDP (%). '' /> Box and whisker plot: Deposit money banks’ assets to GDP (%). Solution figure 10.2 Box and whisker plot: Deposit money banks’ assets to GDP (%). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Box and whisker plot: Bank accounts per 1,000 adults. '' /> Box and whisker plot: Bank accounts per 1,000 adults. Solution figure 10.3 Box and whisker plot: Bank accounts per 1,000 adults. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Box and whisker plot: Bank branches per 100,000 adults. '' /> Box and whisker plot: Bank branches per 100,000 adults. Solution figure 10.4 Box and whisker plot: Bank branches per 100,000 adults. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-08.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-08-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-08-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-08-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-08.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Box and whisker plot: Firms with a bank loan or line of credit (%). '' /> Box and whisker plot: Firms with a bank loan or line of credit (%). Solution figure 10.5 Box and whisker plot: Firms with a bank loan or line of credit (%). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-09.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-09-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-09-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-09-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-09.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Box and whisker plot: Small firms with a bank loan or line of credit (%). '' /> Box and whisker plot: Small firms with a bank loan or line of credit (%). Solution figure 10.6 Box and whisker plot: Small firms with a bank loan or line of credit (%). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-10.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-10-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-10-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-10-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-10.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Box and whisker plot: Bank Z-score. '' /> Box and whisker plot: Bank Z-score. Solution figure 10.7 Box and whisker plot: Bank Z-score. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-11.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-11-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-11-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-11-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-11.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Box and whisker plot: Bank regulatory capital to risk-weighted assets (%). '' /> Box and whisker plot: Bank regulatory capital to risk-weighted assets (%). Solution figure 10.8 Box and whisker plot: Bank regulatory capital to risk-weighted assets (%). ‘Deposit money banks’ assets to GDP (%)’ and ‘Bank accounts per 1,000 adults’ are used as examples here. Averages are rounded to two decimal places and the number of observations is listed underneath the average value. High income: non-OECD High income: OECD Low income Lower middle income Upper middle income 2000 63.89 89.67 15.26 28.10 45.60 25 32 26 45 44 2001 67.17 90.06 14.96 27.85 46.79 25 32 26 47 45 2002 68.69 91.16 14.98 27.61 45.17 26 32 27 48 46 2003 67.29 92.75 15.71 27.76 44.80 26 32 27 48 46 2004 63.35 94.17 15.20 28.61 44.92 27 32 27 48 46 2005 62.19 98.79 15.25 30.36 46.68 27 32 27 48 45 2006 62.77 105.78 15.86 30.35 48.37 27 32 26 48 46 2007 65.25 110.92 16.57 32.02 50.24 27 31 26 48 47 2008 68.73 117.73 18.28 34.73 54.25 27 31 26 48 47 2009 79.37 123.16 19.11 37.65 58.59 25 30 25 47 47 2010 78.77 120.75 20.30 37.23 58.52 26 30 25 48 47 2011 78.00 118.81 21.58 37.88 58.91 26 29 25 46 47 2012 78.85 117.64 21.19 38.63 59.95 26 29 23 47 47 2013 80.12 115.07 22.87 40.28 61.48 26 29 23 46 47 2014 83.81 112.49 23.56 42.46 64.68 25 29 22 44 45 Deposit money banks’ assets to GDP (%), 2000–2014, by income group. Solution figure 10.9 Deposit money banks’ assets to GDP (%), 2000–2014, by income group. East Asia and Pacific Europe and Central Asia Latin America and Caribbean Middle East and North Africa North America South Asia Sub-Saharan Africa 2000 67.56 58.61 46.93 60.92 68.46 28.12 20.64 25 45 33 17 2 7 43 2001 67.25 58.10 48.89 62.51 83.10 29.39 20.53 25 46 33 18 2 7 44 2002 62.32 58.67 49.10 61.92 94.57 32.06 20.50 26 47 33 19 2 7 45 2003 61.24 60.47 48.25 59.39 92.08 33.15 21.43 26 47 33 19 2 7 45 2004 60.62 62.51 46.69 57.02 91.04 36.02 20.99 26 47 33 20 2 7 45 2005 62.69 67.64 47.28 56.14 94.11 38.77 21.48 25 46 33 20 2 7 46 2006 63.32 73.50 47.17 56.31 100.84 34.78 22.12 25 47 33 20 2 8 44 2007 64.10 79.04 48.88 55.71 101.98 37.70 22.78 25 46 33 21 2 8 44 2008 68.48 85.71 51.88 57.94 104.46 41.53 24.38 25 46 33 21 2 8 44 2009 74.95 92.90 55.29 65.27 66.73 43.88 26.32 25 44 33 21 1 8 42 2010 75.56 91.18 54.30 64.12 60.50 45.36 26.15 25 46 32 21 1 8 43 2011 75.97 90.08 54.39 65.47 59.34 45.63 26.94 23 46 32 20 1 8 43 2012 75.11 89.98 55.87 65.36 58.29 45.79 27.45 24 46 32 20 1 8 41 2013 79.25 88.83 56.44 66.47 58.08 46.44 28.39 24 46 32 20 1 8 40 2014 86.66 87.06 57.33 75.13 60.28 45.50 29.01 23 46 31 18 1 8 38 Deposit money banks’ assets to GDP (%), 2000–2014, by region. Solution figure 10.10 Deposit money banks’ assets to GDP (%), 2000–2014, by region. High income: non-OECD High income: OECD Low income Lower middle income Upper middle income 2000 2001 21.80 0.36 265.15 9.96 1 2 1 1 2002 28.47 38.91 285.32 10.46 1 2 1 1 2003 39.22 366.82 10.52 2 1 1 2004 592.08 1,095.45 78.59 243.98 406.29 9 2 15 22 12 2005 607.45 1,055.63 79.36 366.78 466.04 10 3 16 26 17 2006 657.89 1,172.50 80.45 394.09 485.32 10 3 18 27 20 2007 697.26 1,258.88 70.21 405.92 546.63 10 3 19 29 21 2008 778.63 1,511.63 89.02 412.18 594.75 11 4 20 31 21 2009 830.02 1,579.37 96.29 427.92 623.42 12 4 21 32 22 2010 956.33 1,590.49 121.95 475.07 634.92 14 4 22 31 22 2011 973.43 1,429.95 105.51 519.34 745.30 14 3 21 32 23 2012 982.11 1,184.27 122.68 546.57 692.47 13 5 20 30 22 2013 1,058.99 1,211.82 122.68 509.85 714.89 13 5 18 31 21 2014 1,101.30 1,230.74 101.69 632.79 805.33 13 5 11 23 18 Bank accounts per 1,000 adults, 2000–2014, by income group. Solution figure 10.11 Bank accounts per 1,000 adults, 2000–2014, by income group. East Asia and Pacific Europe and Central Asia Latin America and Caribbean Middle East and North Africa North America South Asia Sub-Saharan Africa 2000 2001 265.15 8.12 1 4 2002 285.32 29.19 1 4 2003 366.82 29.65 1 3 2004 580.53 537.83 521.01 368.07 425.56 126.12 5 8 8 6 4 29 2005 575.46 843.54 473.32 380.89 452.45 139.72 6 12 11 8 4 31 2006 516.70 919.94 528.12 388.46 487.81 145.62 9 12 12 8 4 33 2007 557.99 982.22 600.95 420.53 525.75 150.74 9 12 12 9 4 36 2008 713.03 1,092.63 651.44 463.48 458.24 167.85 10 12 12 11 5 37 2009 738.29 1,053.40 695.64 523.62 428.60 185.96 10 13 13 13 4 38 2010 764.47 1,116.05 750.63 523.34 431.42 216.89 10 15 13 13 5 37 2011 973.88 1,159.48 647.61 531.78 545.29 235.62 10 15 13 14 4 37 2012 796.89 1,156.74 695.38 536.23 560.50 277.71 11 14 12 14 4 35 2013 744.74 1,097.12 730.90 529.63 580.13 320.58 12 14 12 13 4 33 2014 863.20 1,161.06 797.02 542.29 672.26 413.40 11 14 11 8 4 22 Bank accounts per 1,000 adults, 2000–2014, by region. Solution figure 10.12 Bank accounts per 1,000 adults, 2000–2014, by region. Solution figures 10.13 to 10.16 show the line charts, and comments on these are provided. Richer countries tend to have larger financial institutions and markets. Depth measure by deposit money banks’ assets to GDP has been increasing in all countries except in non-OECD high-income countries. Richer countries have better access as measured by bank accounts per 1,000 adults. Access displays an upward trend in all groups except in high-income OECD countries. The values of both indicators increase over time for all groups except the high-income OECD group in which access and depth fell for several years after the global financial crisis. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-16.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-16-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-16-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-16-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-16.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Deposit money banks’ assets to GDP (%), 2000–2014, by income group. '' /> Deposit money banks’ assets to GDP (%), 2000–2014, by income group. Solution figure 10.13 Deposit money banks’ assets to GDP (%), 2000–2014, by income group. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-17.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-17-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-17-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-17-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-17.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Deposit money banks’ assets to GDP (%), 2000–2014, by region. '' /> Deposit money banks’ assets to GDP (%), 2000–2014, by region. Solution figure 10.14 Deposit money banks’ assets to GDP (%), 2000–2014, by region. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-18.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-18-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-18-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-18-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-18.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Bank accounts per 1,000 adults, 2000–2014, by income group. '' /> Bank accounts per 1,000 adults, 2000–2014, by income group. Solution figure 10.15 Bank accounts per 1,000 adults, 2000–2014, by income group. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-19.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-19-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-19-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-19-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-19.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Bank accounts per 1,000 adults, 2000–2014, by region. '' /> Bank accounts per 1,000 adults, 2000–2014, by region. Solution figure 10.16 Bank accounts per 1,000 adults, 2000–2014, by region. Note: No data is available for North America over this period. No solution is provided. No solution is provided. Values are rounded to two decimal places, and shown in Solution figure 10.17. North America is omitted due to missing data in all years. East Asia and Pacific Europe and Central Asia Latin America and Caribbean Middle East and North Africa South Asia Sub-Saharan Africa 2004 253.86 637.40 504.48 301.03 529.82 64.48 2005 336.63 1,262.14 471.72 324.82 531.27 76.57 2006 81.43 1,324.44 484.82 351.77 549.06 79.62 2007 85.34 1,380.43 520.49 393.35 573.51 132.21 2008 87.54 1,525.15 561.82 418.40 614.92 149.09 2009 90.62 1,501.88 633.35 436.77 227.03 195.23 2010 94.60 1,371.62 686.92 431.48 267.47 220.34 2011 105.83 1,419.89 688.62 420.16 334.88 236.72 2012 96.85 1,269.52 736.57 426.43 373.07 277.92 2013 101.48 1,043.82 755.75 427.68 399.85 309.17 2014 106.33 1,059.82 780.49 524.85 411.35 350.80 Population-weighted averages of the indicator ‘Bank accounts per 1,000 adults’, 2004–2014. Solution figure 10.17 Population-weighted averages of the indicator ‘Bank accounts per 1,000 adults’, 2004–2014. Where the weighted average is smaller than the simple average, countries with larger populations will generally have poorer access than countries with smaller populations, as is the case with East Asia and Pacific. The reverse holds when the weighted average is larger than the simple average, as is generally the case for Latin America and the Caribbean. The number of bank accounts per 1,000 adults is used as an example: In 2010, the 5th percentile is 27.59, and the 95th percentile is 1,604.69. No solution is provided. Solution figure 10.18 is provided for income groups. Values are rounded to two decimal places. The simple averages of Winsorized values are lower. The differences are large, suggesting that the average values before the Winsorization were driven by extreme values. Income group 2010 average (Winsorized) 2010 average (non-Winsorized) High income: non-OECD 916.90 956.33 High income: OECD 1,356.47 1,590.49 Low income 123.06 121.96 Lower middle income 422.65 475.07 Upper middle income 635.71 643.92 Bank accounts per 1,000 adults: Winsorized averages for 2010. Solution figure 10.18 Bank accounts per 1,000 adults: Winsorized averages for 2010. Part 10.2 Comparing financial stability before and after the 2008 global financial crisis There has been a rapid increase in the number of regulations since the global financial crisis. Banks are now required to hold more capital and liquid assets against the risks they take. Investment banks are forced to focus on facilitating client trades rather than on trading using their own capital. In many countries, regulators require banks to be prepared to survive future financial crises. These changes have raised the capital–asset ratio and lowered the probability of default. Solution figures 10.19 to 10.22 provide the separate tables for the relevant indicators by region and income group. Values are rounded to two decimal places. 2007 2014 Income group Mean N SD Mean N SD Diff in means SD (diff in means) CI width High income: non-OECD 12.14 32 6.67 11.57 25 6.60 –0.57 9.38 3.87 High income: OECD 11.66 32 8.27 11.83 31 6.81 0.17 10.71 3.62 Low income 7.75 25 4.74 9.49 9 4.50 1.73 6.54 3.98 Lower middle income 12.88 46 8.12 12.82 31 8.92 –0.05 12.07 4.06 Upper middle income 11.90 47 8.33 11.42 32 9.35 –0.49 12.52 4.16 Bank Z-score, by income group. Solution figure 10.19 Bank Z-score, by income group. 2007 2014 Income group Mean N SD Mean N SD Diff in means SD (diff in means) CI width High income: non-OECD 15.07 15 2.74 17.30 19 2.71 2.23 3.86 2.00 High income: OECD 12.01 32 1.43 16.79 31 4.27 4.79 4.50 1.70 Low income 21.58 4 7.97 21.0 6 5.18 –0.57 9.50 13.70 Lower middle income 18.36 24 6.23 17.03 30 4.67 –1.34 7.78 3.20 Upper middle income 15.83 27 3.58 16.25 32 2.29 0.42 4.25 1.60 Capital to asset ratio, by income group. Solution figure 10.20 Capital to asset ratio, by income group. 2007 2014 Region Mean N SD Mean N SD Diff in means SD (diff in means) CI width East Asia and Pacific 12.81 24 7.66 12.48 18 7.16 –0.34 10.48 4.77 Europe and Central Asia 8.92 51 7.12 7.98 43 6.29 –0.94 9.50 2.78 Latin America and Caribbean 13.09 35 6.64 12.60 28 6.80 –0.49 9.50 3.47 Middle East and North Africa 20.17 20 7.41 21.76 9.13 15 1.59 11.76 6.11 North America 19.37 3 3.15 17.78 3 6.26 –1.59 7.01 15.93 South Asia 10.85 8 8.49 10.38 6 2.53 –0.47 8.86 7.74 Sub-Saharan Africa 8.25 41 5.16 9.46 15 5.10 1.21 7.25 3.27 Bank Z-score, by region. Solution figure 10.21 Bank Z-score, by region. 2007 2014 Region Mean N SD Mean N SD Diff in means SD (diff in means) CI width East Asia and Pacific 14.58 12 4.98 16.45 15 4.17 1.86 6.49 3.88 Europe and Central Asia 15.12 44 5.48 17.85 45 3.79 2.73 6.66 2.02 Latin America and Caribbean 14.93 17 2.34 15.28 17 1.77 0.35 2.93 1.50 Middle East and North Africa 15.21 10 3.65 15.27 15 2.68 0.06 4.53 3.01 North America 13.80 2 1.00 14.30 2 0.10 0.50 1.00 12.19 South Asia 12.30 2 0.00 15.36 5 3.19 3.06 3.19 4.43 Sub-Saharan Africa 17.76 15 5.65 19.05 19 4.83 1.29 7.43 3.88 Capital to asset ratio, by region. Solution figure 10.22 Capital to asset ratio, by region. Solution figures 10.23 to 10.26 show the column charts. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-26.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-26-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-26-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-26-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-26.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Confidence intervals for Bank Z-score, by income group. '' /> Confidence intervals for Bank Z-score, by income group. Solution figure 10.23 Confidence intervals for Bank Z-score, by income group. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-27.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-27-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-27-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-27-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-27.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Confidence intervals for Capital to asset ratio, by income group. '' /> Confidence intervals for Capital to asset ratio, by income group. Solution figure 10.24 Confidence intervals for Capital to asset ratio, by income group. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-28.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-28-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-28-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-28-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-28.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Confidence intervals for Bank Z-score, by region. '' /> Confidence intervals for Bank Z-score, by region. Solution figure 10.25 Confidence intervals for Bank Z-score, by region. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-29.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-29-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-29-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-29-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-10-29.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Confidence intervals for Capital to asset ratio, by region. '' /> Confidence intervals for Capital to asset ratio, by region. Solution figure 10.26 Confidence intervals for Capital to asset ratio, by region. The means for bank Z-scores were estimated quite imprecisely (wide confidence intervals) so it is likely that the observed differences between 2007 and 2014 are due to chance. The size of the difference is also small for all countries and regions (recall that the vertical axis is measured in percentage points). In high-income countries, the bank regulatory capital to risk-weighted assets ratio increased between 2007 and 2014, especially for OECD countries, and Europe and Central Asia. For these countries/regions, the mean is quite precisely estimated so we can be fairly confident that the observed differences are not due to chance. For all other countries, the observed differences are small and imprecisely estimated (either because there are few countries in that group, or because there is huge variation within the group). The global financial crisis affected some countries more than others (for example, in North America, the US economy was affected more severely than the Canadian economy). As an extension, you could instead group countries according to how affected they were by the crisis (e.g. not much, moderately, severely) and recalculate the difference in means before and after the crisis. We might expect that countries that were affected more severely would also place stricter banking system regulations than less-affected countries."
});
index.addDoc({
    id: 57,
    title: "Doing Economics: Empirical Project 11: Measuring willingness to pay for climate change abatement",
    content: "Empirical project 11 Measuring willingness to pay for climate change mitigation Learning objectives In this project you will: construct indices to measure attitudes or opinions (Part 11.1) use Cronbach’s alpha to assess indices for internal consistency (Part 11.1) practise recoding and creating new variables (Part 11.1) compare survey measures of willingness to pay (Part 11.2). Key concepts Concepts needed for this project: mean, standard deviation, correlation/correlation coefficient, confidence interval (for difference in means). Concepts introduced in this project: Likert scale (frequency scale) and Cronbach’s alpha. Introduction CORE projects This empirical project is related to material in: Unit 11 of Economy, Society, and Public Policy Unit 12 and Unit 20 of The Economy. When designing policies to reduce carbon emissions or air pollution, or to save an endangered species or preserve biodiversity, economists face the problem that markets for environmental amenities are missing. How can the value to people of the abatement of environmental damage be calculated and set against the cost of implementing any abatement? contingent valuationA survey-based technique used to assess the value of non-market resources. Also known as: stated-preference model. The contingent valuation method asks people their willingness to pay (WTP), and so is called a stated-preference method. Alternative methods, including ‘revealed preference’, are explained in Section 20.6 of The Economy. As explained in The Economy, a number of methods can be used to estimate the value of abatement. One method, called contingent valuation, involves asking people directly—for example, through a survey—how much they value the good. Two common ways of obtaining information about willingness to pay (WTP) are: dichotomous choice (DC): presenting individuals with an amount, to which they respond with either ‘yes/willing to pay’ or ‘no/not willing to pay’ (sometimes a ‘no response’ option is also offered) a two-way payment ladder (TWPL): asking individuals to state the minimum and maximum amount they are willing to pay (selecting from a pre-specified list of amounts). As with all subjective measures, both of these methods face different kinds of response biases. In this project, we ask whether they give the same results on average. The issue of how to measure WTP for non-market goods, such as abatement of pollution, is important for policymaking. Incorrectly estimating the WTP may result in too much or too little abatement. For estimates of the cost of abatement of greenhouse gases, see The Economy Figures 20.9 and 20.26 in Sections 20.3 and 20.10 respectively. We will look at climate change mitigation as an example. Since tackling climate change may entail short-term costs such as reforestation of degraded forests, governments may want to know how much their citizens are willing to pay to reduce carbon emissions as a method of mitigating climate change. The German government sponsored a nationwide online survey that investigated the effect of question format (DC or TWPL) on WTP responses. A representative sample of participants aged 18–69 were randomly assigned to either question format, and were asked their willingness to pay for a 10 percentage point increase in Germany’s carbon emissions reduction target (from 30% to 40%) by 2020 (compared to 1990). This scenario closely corresponds to Germany’s current climate change mitigation strategy. In this survey, the list of WTP amounts for both question formats ranged from very low values (48 euros per household per year) to very high values (1,440 euros per household per year). We will be using this survey data to compare WTP (mean and median) under each method and assess whether WTP responses differ according to question formats. Working in Excel Working in R Working in Google Sheets"
});
index.addDoc({
    id: 58,
    title: "Doing Economics: Empirical Project 11: Working in Excel",
    content: "Empirical Project 11 Working in Excel Part 11.1 Summarizing the data Learning objectives for this part construct indices to measure attitudes or opinions use Cronbach’s alpha to assess indices for internal consistency practise recoding and creating new variables. We will be using data collected from an internet survey sponsored by the German government. First, download the survey data and documentation: Download the data. Read the ‘Data dictionary’ tab and make sure you know what each variable represents. (Later we will discuss exactly how some of these variables were coded.) Download a short description of the dataset, which explains the survey design and questions asked. You may find it helpful to read it before starting on the questions below. While contingent valuation methods can be useful, they also have shortcomings. Read Section 5 of the paper ‘Introduction to economic valuation methods’ (pages 16–19), and explain which limitations you think apply particularly to the survey we are looking at. You may also find it useful to look at Table 2 of that paper, which compares stated-preference with revealed-preference methods. Before comparing between question formats (dichotomous choice (DC) and two-way payment ladder (TWPL)), we will first compare the people assigned to each question format to see if they are similar in demographic characteristics and attitudes towards related topics (such as beliefs about climate change and the need for government intervention). If the groups are vastly dissimilar then any observed differences in answers between the groups might be due to differences in attitudes and/or demographics rather than the question format. Likert scaleA numerical scale (usually ranging from 1–5 or 1–7) used to measure attitudes or opinions, with each number representing the individual’s level of agreement or disagreement with a particular statement. Attitudes were assessed using a 1–5 Likert scale, where 1 = strongly disagree, and 5 = strongly agree. The way the questions were asked was not consistent, so an answer of ‘strongly agree’ might mean high climate change skepticism for one question, but low skepticism for another question. In order to combine these questions into an index we need to recode (in this case, reverse-code) some of the variables. Recode or create the variables as specified: Reverse-code the following variables (so that 1 is now 5, 2 is now 4, and so on): cog_2, cog_5, scepticism_6, scepticism_7. (Hint: One way to do this is to create a new variable and use Excel’s IF function to fill in the values of the new variable based on the values of the original variable. For help on using the IF function, see Excel walk-through 6.1.) For the variables ‘WTP_plmin’ and ‘WTP_plmax’, create new variables with the values replaced as shown in Figure 11.1 (these are the actual amounts, in euros, that individuals selected in the survey, and will be useful for calculating summary measures later). In order to produce the correct summary statistics, you will need to use Excel’s IF function to preserve the blank cells and only fill in values for the new variable if the original variable is non-blank. Original value New value 1 48 2 72 3 84 4 108 5 156 6 192 7 252 8 324 9 432 10 540 11 720 12 960 13 1,200 14 1,440 WTP survey categories (original value) and euro amounts (new value). Figure 11.1 WTP survey categories (original value) and euro amounts (new value). Create the following indices, giving them an appropriate name in your spreadsheet (make sure to use the reverse-coded variable where relevant): Belief that climate change is a real phenomenon: Take the mean of scepticism_2, scepticism_6, and scepticism_7. Preferences for government intervention to solve problems in society: Take the mean of cog_1, cog_2, cog_3, cog_4, cog_5, and cog_6. Feeling of personal responsibility to act pro-environmentally: Take the mean of PN_1, PN_2, PN_3, PN_4, PN_6, and PN_7. Cronbach’s alphaA measure used to assess the extent to which a set of items is a reliable or consistent measure of a concept. This measure ranges from 0–1, with 0 meaning that all of the items are independent of one another, and 1 meaning that all of the items are perfectly correlated with each other. When creating indices, we may be interested to see if each item used in the index measures the same underlying variable of interest (known as reliability or consistency). There are two common ways to assess reliability: either look at the correlation between items in the index, or use a summary measure called Cronbach’s alpha (this measure is used in the social sciences). We will be calculating and interpreting both of these measures. Cronbach’s alpha is a way to summarize the correlations between many variables, and ranges from 0 to 1, with 0 meaning that all of the items are independent of one another, and 1 meaning that all of the items are perfectly correlated with each other. While higher values of this measure indicate that the items are closely related and therefore measure the same concept, with values that are very close to 1 (or 1) we could be concerned that our index contains redundant items (for example, two items that tell us the same information, so we would only want to use one or the other, but not both). You can read more about this in the paper ‘Using and interpreting Cronbach’s Alpha’. Calculate correlation coefficients and interpret Cronbach’s alpha: For one of the indices you created in Question 3, create a correlation table to show the correlation between each of the items in the index. Remember to give the variables meaningful names in your table (refer to the ‘Data dictionary’ tab for descriptions of each variable). (For help on calculating correlation coefficients, see Excel walk-through 1.7.) Figure 11.2 shows an example for Question 3(a). (Remem­ber that the correlation between A and B is the same as the correlation between B and A, so you only need to calculate the correlation for each pair of items once.) Are the items in that index strongly correlated? exaggeration not.human.activity no.evidence exaggeration 1 – – not.human.activity 1 – no.evidence 1 Correlation table for survey items on climate change scepticism: Climate change is exaggerated (exaggeration), Human activity is not the main cause of climate change (not.human.activity), No evidence of global warming (no.evidence). Figure 11.2 Correlation table for survey items on climate change scepticism: Climate change is exaggerated (exaggeration), Human activity is not the main cause of climate change (not.human.activity), No evidence of global warming (no.evidence). The Cronbach’s alpha for these indices are 0.66, 0.71, and 0.85 respectively. Interpret these values in terms of index reliability. Now we will compare characteristics of people in the dichotomous choice (DC) group and two-way payment ladder (TWPL) group (the variable ‘abst_format’ indicates which group an individual belongs to). Since the groups are of different sizes, we will use percentages instead of frequencies. For each group (DC and TWPL), create tables to summarize the distribution of the following variables (a separate table for each variable): gender (‘sex’) age (‘age’) number of children (‘kids_nr’) household net income per month in euros (‘hhnetinc’) membership in environmental organization (‘member’) highest educational attainment (‘education’). For help on creating tables, see Excel walk-through 3.1. Using the tables you have created, and without doing formal calculations, discuss any similarities/differences in demographic characteristics between the two groups. Create a separate summary table as shown in Figure 11.3 for each of the three indices you created in Question 3. Without doing formal calculations, do the two groups of individuals look similar in the attitudes specified? Mean Standard deviation Min Max DC format TWPL format Summary table for indices. Figure 11.3 Summary table for indices. Part 11.2 Comparing willingness to pay across methods and individual characteristics Learning objectives for this part compare survey measures of willingness to pay. Before comparing WTP across question formats, we will summarize the distribution of WTP within each question format. For individuals who answered the TWPL question: Use the variables ‘WTP_plmin’ and ‘WTP_plmax’ to create column charts (one for each variable) with frequency on the vertical axis and category (the numbers 1–14) on the horizontal axis. Describe characteristics of the distributions shown on the charts. Using the variables you created in Question 2(b) in Part 11.1 (showing the actual WTP amounts), make a new variable that contains the average of the two variables (i.e. for each individual, calculate the average of the minimum and maximum willingness to pay). Calculate the mean and median of the variable you created in Question 1(b). Using the variable from Question 1(b), calculate the correlation between individuals’ average WTP and the demographic and attitudinal variables (see Questions 3 and 5 from Part 11.1 for a list of these variables). Interpret the relationships implied by the coefficients. For individuals who answered the DC question: Each individual was given one amount and had to decide ‘yes’, ‘no’, or ‘no vote/abstain from deciding’. Make a pivot table showing the frequency of ‘DC_ref_outcome’, with ‘costs’ as the row variable and ‘DC_ref_outcome’ as the column variable. Use this table to calculate the percentage of individuals who voted ‘no’ and ‘yes’ for each amount (in other words as a percentage of the row total, not the overall total). Count individuals who chose ‘abstain’ as voting ‘no’. Make a scatterplot showing the ‘demand curve’, with percentage of individuals who voted ‘yes’ as the vertical axis variable and amount (in euros) as the horizontal axis variable. (To connect the points, use the chart option ‘Scatter with Straight Lines and Markers’.) Describe features of this ‘demand curve’ that you find interesting. Repeat Question 2(b), this time excluding individuals who chose ‘abstain’ from the calculations. Plot this new ‘demand curve’ on the chart created for Question 2(c). Do your results change qualitatively, depending on how you count individuals who did not vote? Compare the mean and median WTP under both question formats: Complete Figure 11.4 and use it to calculate the difference in means (DC minus TWPL), the standard deviation of these differences, and the number of observations. (The mean of DC is the mean of ‘DC_ref_outcome’ for individuals who voted ‘yes’.) Format Mean Standard deviation Number of observations DC TWPL Summary table for WTP. Figure 11.4 Summary table for WTP. Use Excel’s CONFIDENCE.T function and the calculated values for Question 3(a) to make a 95% confidence interval for the difference in means. Discuss your findings. (For help on calculating confidence intervals for the difference in means, see Excel walk-through 6.4 and the discussion in Part 8.3.). Does the median WTP look different across question formats? (You do not need to do any formal calculations.) Using your answers to Questions 3(a)–(c), would you recommend that governments use the mean or median WTP in policy-making decisions? (That is, which measure appears to be more robust to changes in the question format?) Leading think tanks estimate that the world needs 20 trillion USD of investment in low-carbon energy supplies and energy efficient technologies by 2030 to meet the Paris Agreement targets. This amount roughly corresponds to 3,273 euros total per adult (aged 15 and above), or 298 euros per adult per year (2020 to 2030 inclusive). Compare this approximate number with the WTP estimates you have found. Assuming people around the world have the same attitudes towards climate change as the Germans surveyed, would a tax equal to the median WTP be enough to finance climate change mitigation? Discuss how governments could increase public support for and involvement in climate change mitigation activities."
});
index.addDoc({
    id: 59,
    title: "Doing Economics: Empirical Project 11: Working in R",
    content: "Empirical Project 11 Working in R Download the code To download the code chunks used in this project, right-click on the download link and select ‘Save Link As…’. You’ll need to save the code download to your working directory, and open it in RStudio. Don’t forget to also download the data into your working directory by following the steps in this project. Getting started in R For this project you will need the following packages: tidyverse, to help with data manipulation readxl, to import an Excel spreadsheet knitr, to format tables psych, to compute Cronbach’s alpha. If you need to install any of these packages, run the following code: install.packages(c(''tidyverse'', ''readxl'', ''knitr'', ''psych'')) You can import the libraries now, or when they are used in the R walk-throughs below. library(tidyverse) library(readxl) library(knitr) library(psych) Part 11.1 Summarizing the data Learning objectives for this part construct indices to measure attitudes or opinions use Cronbach’s alpha to assess indices for internal consistency practise recoding and creating new variables. We will be using data collected from an internet survey sponsored by the German government. First, download the survey data and documentation: Download the data. Open the file in Excel and read the ‘Data dictionary’ tab and make sure you know what each variable represents. (Later we will discuss exactly how some of these variables were coded.) The paper ‘Data in Brief’ gives a summary of how the survey was conducted. You may find it helpful to read it before starting on the questions below. While contingent valuation methods can be useful, they also have shortcomings. Read Section 5 of the paper ‘Introduction to economic valuation methods’ (pages 16–19), and explain which limitations you think apply particularly to the survey we are looking at. You may also find it useful to look at Table 2 of that paper, which compares stated-preference with revealed-preference methods. Before comparing between question formats (dichotomous choice (DC) and two-way payment ladder (TWPL)), we will first compare the people assigned to each question format to see if they are similar in demographic characteristics and attitudes towards related topics (such as beliefs about climate change and need for government intervention). If the groups are vastly dissimilar then any observed differences in answers between the groups might be due to differences in attitudes and/or demographics rather than the question format. Likert scaleA numerical scale (usually ranging from 1–5 or 1–7) used to measure attitudes or opinions, with each number representing the individual’s level of agreement or disagreement with a particular statement. Attitudes were assessed using a 1–5 Likert scale, where 1 = strongly disagree, and 5 = strongly agree. The way the questions were asked was not consistent, so an answer of ‘strongly agree’ might mean high climate change skepticism for one question, but low skepticism for another question. In order to combine these questions into an index we need to recode (in this case, reverse-code) some of the variables. Recode or create the variables as specified: Reverse-code the following variables (so that 1 is now 5, 2 is now 4, and so on): cog_2, cog_5, scepticism_6, scepticism_7. For the variables WTP_plmin and WTP_plmax, create new variables with the values replaced as shown in Figure 11.1 (these are the actual amounts, in euros, that individuals selected in the survey, and will be useful for calculating summary measures later). Original value New value 1 48 2 72 3 84 4 108 5 156 6 192 7 252 8 324 9 432 10 540 11 720 12 960 13 1,200 14 1,440 WTP survey categories (original value) and euro amounts (new value). Figure 11.1 WTP survey categories (original value) and euro amounts (new value). R walk-through 11.1 Importing data and recoding variables Before importing data in Excel or .csv format, open it in a spreadsheet program (such as Excel) to ensure you understand the structure of the data and check if any additional options are required for the read_excel function in order to import the data correctly. In this case, the data is in a worksheet called ‘Data’, there are no missing values to worry about, and the first row contains the variable names. This format is straightforward to import. We can therefore import the data using the read_excel function without any additional options. library(tidyverse) library(readxl) library(knitr) # Set your working directory to the correct folder. # Insert your file path for 'YOURFILEPATH'. setwd(''YOURFILEPATH'') WTP <- read_excel( ''Project 11 datafile.xlsx'', sheet = ''Data'') Reverse-code variables The first task is to recode variables related to the respondents’ views on certain aspects of government behaviour and attitudes about global warming (cog_2, cog_5, scepticism_6, and scepticism_7). This coding makes the interpretation of high and low values consistent across all questions, since the survey questions do not have this consistency. To recode all of these variables in one go, we use the piping operator (%>%), which can perform the same sequence of commands on a number of variables at once. (For a more detailed introduction to piping, see the University of Manchester’s Econometric Computing Learning Resource.) Note that even though the value of 3 for these variables will stay the same, for the recode function to work properly we have to specify how each new value corresponds to a previous value. WTP <- WTP %>% mutate_at(c(''cog_2'', ''cog_5'', ''scepticism_6'', ''scepticism_7''), funs(recode(., ''1'' = 5, ''2'' = 4, ''3'' = 3, ''4'' = 2, ''5'' = 1))) Create new variables containing WTP amounts Although we could employ the same technique as above to recode the value for the minimum and maximum willingness to pay variables, an alternative is to use the merge function. This function allows us to combine two dataframes via values given in a particular variable. We start by creating a new dataframe (category_amount) that has two variables: the original category value and the corresponding new euro amount. We then apply the merge function to the WTP dataframe and the new dataframe, specifying the variables that link the data in each dataframe together (by.x indicates which variable in the first dataframe, here WTP, is to be matched to by.y, the variable in the second dataframe, here category_amount). We also use the all.x = TRUE option to keep all observations, otherwise the merge function will drop any observations with missing values for the WTP_plmin and WTP_plmax variables. Finally we have to rename the column of the merged new values to something more meaningful (WTP_plmin_euro and WTP_plmax_euro respectively). # Vector containing the Euro amounts wtp_euro_levels <- c(48, 72, 84, 108, 156, 192, 252, 324, 432, 540, 720, 960, 1200, 1440) # Create mapping dataframe category_amount <- data.frame(original = 1:14, new = wtp_euro_levels) # Create a new variable for the minimum WTP WTP <- merge(WTP, category_amount, by.x = ''WTP_plmin'', by.y = ''original'', all.x = TRUE) %>% rename(., ''WTP_plmin_euro'' = ''new'') # Create a new variable for the maximum WTP WTP <- merge(WTP, category_amount, by.x = ''WTP_plmax'', by.y = ''original'', all.x = TRUE) %>% rename(., ''WTP_plmax_euro'' = ''new'') Create the following indices, giving them an appropriate name in your spreadsheet (make sure to use the reverse-coded variable wherever relevant): Belief that climate change is a real phenomenon: Take the mean of scepticism_2, scepticism_6, and scepticism_7. Preferences for government intervention to solve problems in society: Take the mean of cog_1, cog_2, cog_3, cog_4, cog_5, and cog_6. Feeling of personal responsibility to act pro-environmentally: Take the mean of PN_1, PN_2, PN_3, PN_4, PN_6, and PN_7. R walk-through 11.2 Creating indices We can create all of the required indices in three steps using the rowMeans function. In each step we use the cbind function to join the required variables (columns) together as a matrix. As the data is stored as a single observation per row, the index value is the average of the values in each row of this matrix, which we calculate using the rowMeans function. WTP <- WTP %>% # Ensure subsequent operations are applied by row rowwise() %>% mutate(., climate = rowMeans(cbind( scepticism_2, scepticism_6, scepticism_7))) %>% mutate(., gov_intervention = rowMeans(cbind( cog_1, cog_2, cog_3, cog_4, cog_5, cog_6))) %>% mutate(., pro_environment = rowMeans(cbind( PN_1, PN_2, PN_3, PN_4, PN_6, PN_7))) %>% # Return the dataframe to the original format ungroup() Cronbach’s alphaA measure used to assess the extent to which a set of items is a reliable or consistent measure of a concept. This measure ranges from 0–1, with 0 meaning that all of the items are independent of one another, and 1 meaning that all of the items are perfectly correlated with each other. When creating indices, we may be interested to see if each item used in the index measures the same underlying concept of interest (known as reliability or consistency). There are two common ways to assess reliability: either look at the correlation between items in the index, or use a summary measure called Cronbach’s alpha (this measure is used in the social sciences). We will be calculating and interpreting both of these measures. Cronbach’s alpha is a way to summarize the correlations between many variables, and ranges from 0 to 1, with 0 meaning that all of the items are independent of one another, and 1 meaning that all of the items are perfectly correlated with each other. While higher values of this measure indicate that the items are closely related and therefore measure the same concept, with values that are very close to 1 (or 1), we could be concerned that our index contains redundant items (for example, two items that tell us the same information, so we would only want to use one or the other, but not both). You can read more about this in the paper ‘Using and interpreting Cronbach’s Alpha’. Calculate correlation coefficients and interpret Cronbach’s alpha: For one of the indices you created in Question 3, create a correlation table to show the correlation between each of the items in the index. Remember to give the variables meaningful names in your table (refer to the ‘Data dictionary’ tab for descriptions of each variable). Figure 11.2 shows an example for Question 3(a). (Remember that the correlation between A and B is the same as the correlation between B and A, so you only need to calculate the correlation for each pair of items once). Are the items in that index strongly correlated? exaggeration not.human.activity no.evidence exaggeration 1 – – not.human.activity 1 – no.evidence 1 Correlation table for survey items on climate change scepticism: Climate change is exaggerated (exaggeration), Human activity is not the main cause of climate change (not.human.activity), No evidence of global warming (no.evidence). Figure 11.2 Correlation table for survey items on climate change scepticism: Climate change is exaggerated (exaggeration), Human activity is not the main cause of climate change (not.human.activity), No evidence of global warming (no.evidence). Use the alpha function (part of the psych package included with R) to compute the Cronbach’s alpha for these indices. Interpret these values in terms of index reliability. R walk-through 11.3 Calculating correlation coefficients Calculate correlation coefficients and Cronbach’s alpha We covered calculating correlation coefficients in R walk-through 10.1. In this case, since there are no missing values we can use the cor function without any additional options. For the questions on climate change: cor(cbind(WTP$scepticism_2, WTP$scepticism_6, WTP$scepticism_7)) ## [,1] [,2] [,3] ## [1,] 1.0000000 0.3904296 0.4167478 ## [2,] 0.3904296 1.0000000 0.4624211 ## [3,] 0.4167478 0.4624211 1.0000000 For the questions on government behaviour: cor(cbind(WTP$cog_1, WTP$cog_2, WTP$cog_3, WTP$cog_4, WTP$cog_5, WTP$cog_6)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1.0000000 0.2509464 0.32358783 0.6823385 0.28925672 0.4141992 ## [2,] 0.2509464 1.0000000 0.11761093 0.2771883 0.40794667 0.0828661 ## [3,] 0.3235878 0.1176109 1.00000000 0.3347662 0.01818617 0.3128608 ## [4,] 0.6823385 0.2771883 0.33476619 1.0000000 0.27424993 0.4597244 ## [5,] 0.2892567 0.4079467 0.01818617 0.2742499 1.00000000 0.1045843 ## [6,] 0.4141992 0.0828661 0.31286082 0.4597244 0.10458434 1.0000000 For the questions on personal behaviour: cor(cbind(WTP$PN_1, WTP$PN_2, WTP$PN_3, WTP$PN_4, WTP$PN_6, WTP$PN_7)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1.0000000 0.4824823 0.4282149 0.4226534 0.4138090 0.4584007 ## [2,] 0.4824823 1.0000000 0.6315015 0.4375971 0.4994126 0.6542377 ## [3,] 0.4282149 0.6315015 1.0000000 0.4596711 0.5219712 0.5894731 ## [4,] 0.4226534 0.4375971 0.4596711 1.0000000 0.5668642 0.3947270 ## [5,] 0.4138090 0.4994126 0.5219712 0.5668642 1.0000000 0.4551294 ## [6,] 0.4584007 0.6542377 0.5894731 0.3947270 0.4551294 1.0000000 Calculate Cronbach’s alpha It is straightforward to compute the Cronbach’s alpha using the alpha function from the psych package. This function calculates Cronbach’s alpha and stores it in $total$std.alpha. psych::alpha(WTP[c(''scepticism_2'', ''scepticism_6'', ''scepticism_7'')])$total$std.alpha ## [1] 0.6876079 psych::alpha(WTP[c(''cog_1'', ''cog_2'', ''cog_3'', ''cog_4'', ''cog_5'', ''cog_6'')])$total$std.alpha ## [1] 0.7102249 psych::alpha(WTP[c(''PN_1'', ''PN_2'', ''PN_3'', ''PN_4'', ''PN_6'', ''PN_7'')])$total$std.alpha ## [1] 0.8543827 Now we will compare characteristics of people in the dichotomous choice (DC) group and two-way payment ladder (TWPL) group (the variable abst_format indicates which group an individual belongs to). Since the groups are of different sizes, we will use percentages instead of frequencies. For each group (DC and TWPL), create tables to summarize the distribution of the following variables (a separate table for each variable): gender (sex) age (age) number of children (kids_nr) household net income per month in euros (hhnetinc) membership in environmental organization (member) highest educational attainment (education). Using the tables you have created, and without doing formal calculations, discuss any similarities/differences in demographic characteristics between the two groups. R walk-through 11.4 Using loops to obtain summary statistics The two different formats (DC and TWPL) are recorded in the variable abst_format, and take the values ref and ladder respectively. We will store all the variables of interest into a list called variables, and use a ‘for’ loop to calculate summary statistics for each variable and present it in a table. variables <- list(quo(sex), quo(age), quo(kids_nr), quo(hhnetinc), quo(member), quo(education)) for (i in seq_along(variables)){ WTP %>% group_by(abst_format, !!variables[[i]]) %>% summarize (n = n()) %>% mutate(freq = n / sum(n)) %>% select(-n) %>% spread(abst_format, freq) %>% print() } ## # A tibble: 2 x 3 ## sex ladder ref ## <chr> <dbl> <dbl> ## 1 female 0.518 0.523 ## 2 male 0.482 0.477 ## # A tibble: 6 x 3 ## age ladder ref ## <chr> <dbl> <dbl> ## 1 18 - 24 0.0949 0.0964 ## 2 25 - 29 0.0830 0.0865 ## 3 30 - 39 0.178 0.172 ## 4 40 - 49 0.223 0.226 ## 5 50 - 59 0.241 0.239 ## 6 60 - 69 0.180 0.181 ## # A tibble: 5 x 3 ## kids_nr ladder ref ## <chr> <dbl> <dbl> ## 1 four or more children 0.00988 0.00895 ## 2 no children 0.646 0.657 ## 3 one child 0.204 0.176 ## 4 three children 0.0296 0.0348 ## 5 two children 0.111 0.123 ## # A tibble: 12 x 3 ## hhnetinc ladder ref ## <chr> <dbl> <dbl> ## 1 1100 bis unter 1500 Euro 0.142 0.132 ## 2 1500 bis unter 2000 Euro 0.150 0.146 ## 3 2000 bis unter 2600 Euro 0.115 0.148 ## 4 2600 bis unter 3200 Euro 0.107 0.107 ## 5 3200 bis unter 4000 Euro 0.111 0.0815 ## 6 4000 bis unter 5000 Euro 0.0514 0.0497 ## 7 500 bis unter 1100 Euro 0.134 0.142 ## 8 5000 bis unter 6000 Euro 0.0277 0.0169 ## 9 6000 bis unter 7500 Euro 0.00791 0.00398 ## 10 7500 und mehr 0.00395 0.00497 ## 11 bis unter 500 Euro 0.0296 0.0417 ## 12 do not want to answer 0.121 0.125 ## # A tibble: 2 x 3 ## member ladder ref ## <chr> <dbl> <dbl> ## 1 no 0.923 0.914 ## 2 yes 0.0771 0.0865 ## # A tibble: 6 x 3 ## education ladder ref ## <dbl> <dbl> <dbl> ## 1 1 0.0119 0.0129 ## 2 2 0.0198 0.0209 ## 3 3 0.342 0.328 ## 4 4 0.263 0.269 ## 5 5 0.0692 0.0686 ## 6 6 0.294 0.300 The output above gives the required tables, but is not easy to read. You may want to tidy up the results, for example by translating (from German to English) and reordering the options in the household net income variable (hhnetinc). Create a separate summary table as shown in Figure 11.3 for each of the three indices you created in Question 3. Without doing formal calculations, do the two groups of individuals look similar in the attitudes specified? Mean Standard deviation Min Max DC format TWPL format Summary table for indices. Figure 11.3 Summary table for indices. R walk-through 11.5 Calculating summary statistics The summarize_at function can provide multiple statistics for a number of variables in one command. Simply provide a list of the variables you want to summarize and then use the funs() option to specify the summary statistics you need. Here, we need the mean, sd, mean, and max for the variables climate, gov_intervention, and pro_environment. WTP %>% group_by(abst_format) %>% summarise_at(c(''climate'', ''gov_intervention'', ''pro_environment''), funs(mean, sd, min, max)) %>% # Use gather and spread functions to reformat output # for aesthetic reasons gather(index, value, climate_mean:pro_environment_max) %>% spread(abst_format, value) %>% kable(., format = ''markdown'', digits = 2) |index | ladder| ref| |:---------------------|------:|----:| |climate_max | 5.00| 5.00| |climate_mean | 2.29| 2.37| |climate_min | 1.00| 1.00| |climate_sd | 0.84| 0.85| |gov_intervention_max | 5.00| 5.00| |gov_intervention_mean | 3.15| 3.19| |gov_intervention_min | 1.00| 1.00| |gov_intervention_sd | 0.70| 0.66| |pro_environment_max | 5.00| 5.00| |pro_environment_mean | 3.03| 3.01| |pro_environment_min | 1.00| 1.00| |pro_environment_sd | 0.79| 0.82| Part 11.2 Comparing willingness to pay across methods and individual characteristics Learning objectives for this part compare survey measures of willingness to pay. Before comparing WTP across question formats, we will summarize the distribution of WTP within each question format. For individuals who answered the TWPL question: Use the variables WTP_plmin and WTP_plmax to create column charts (one for each variable) with frequency on the vertical axis and category (the numbers 1–14) on the horizontal axis. Describe characteristics of the distributions shown on the charts. Using the variables you created in Question 2(b) in Part 11.1 (showing the actual WTP amounts), make a new variable that contains the average of the two variables (i.e. for each individual, calculate the average of the minimum and maximum willingness to pay). Calculate the mean and median of the variable you created in Question 1(b). Using the variable from Question 1(b), calculate the correlation between individuals’ average WTP and the demographic and attitudinal variables (see Questions 3 and 5 from Part 11.1 for a list of these variables). Interpret the relationships implied by the coefficients. R walk-through 11.6 Summarizing willingness to pay variables Create column charts for minimum and maximum WTP Before we can plot a column chart, we need to compute frequencies (number of observations) for each value of the willingness to pay (1–14). We do this separately for the minimum and maximum willingness to pay. In each case we select the relevant variable and remove any observations with missing values using the na.omit function. We can then separate the data by level (WTP amount) of the WTP_plmin_euro or WTP_PLmax_euro variables (using group_by), then obtain a frequency count using the summarize function. We also use the factor function to set this variable’s type to factor, to get the correct horizontal axis labels in the column chart. Once we have the frequency count stored as a dataframe, we can plot the column charts. For the minimum willingness to pay: df.plmin <- WTP %>% select(WTP_plmin_euro) %>% na.omit() %>% group_by(WTP_plmin_euro) %>% summarize(n = n()) %>% mutate(WTP_plmin_euro = factor(WTP_plmin_euro, levels = wtp_euro_levels)) ggplot(df.plmin, aes(WTP_plmin_euro, n)) + geom_bar(stat = ''identity'', position = ''identity'') + xlab(''Minimum WTP (euros)'') + ylab(''Frequency'') + theme_bw() <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Minimum WTP (euros). '' /> Minimum WTP (euros). Figure 11.4 Minimum WTP (euros). For the maximum willingness to pay: df.plmax <- WTP %>% select(WTP_plmax_euro) %>% na.omit() %>% group_by(WTP_plmax_euro) %>% summarize(n = n()) %>% mutate(WTP_plmax_euro = factor(WTP_plmax_euro, levels = wtp_euro_levels)) ggplot(df.plmax, aes(WTP_plmax_euro, n)) + geom_bar(stat = ''identity'', position = ''identity'') + xlab(''Maximum WTP (euros)'') + ylab(''Frequency'') + theme_bw() <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Maximum WTP (euros). '' /> Maximum WTP (euros). Figure 11.5 Maximum WTP (euros). Calculate average WTP for each individual We can use the rowMeans function to obtain the average of the minimum and maximum willingness to pay. WTP <- WTP %>% rowwise() %>% mutate(., WTP_average = rowMeans(cbind( WTP_plmin_euro, WTP_plmax_euro))) %>% ungroup() Calculate mean and median WTP across individuals The mean and median of this average value can be obtained using the mean and median functions, although we have to use the na.rm = TRUE option to handle missing values correctly. mean(WTP$WTP_average, na.rm = TRUE) ## [1] 268.5345 median(WTP$WTP_average, na.rm = TRUE) ## [1] 132 Calculate correlation coefficients We showed how to obtain a matrix of correlation coefficients for a number of variables in R walk-through 8.8. We use the same process here, storing the coefficients in an object called M. WTP %>% # Create the gender variable mutate(gender = as.numeric(ifelse(sex == ''female'', 0, 1))) %>% select(WTP_average, education, gender, climate, gov_intervention, pro_environment) %>% cor(., use = ''pairwise.complete.obs'') -> M M[, ''WTP_average''] ## WTP_average education gender climate ## 1.00000000 0.13817368 0.03694972 -0.14462072 ## gov_intervention pro_environment ## -0.18845205 0.18750331 For individuals who answered the DC question: Each individual was given an amount and had to decide ‘yes’, ‘no’, or ‘no vote/abstain from deciding’. Make a table showing the frequency of DC_ref_outcome, with costs as the row variable and DC_ref_outcome as the column variable. Use this table to calculate the percentage of individuals who voted ‘no’ and ‘yes’ for each amount (in other words as a percentage of the row total, not the overall total). Count individuals who chose ‘abstain’ as voting ‘no’. Make a line chart showing the ‘demand curve’, with percentage of individuals who voted ‘yes’ as the vertical axis variable and amount (in euros) as the horizontal axis variable. Describe features of this ‘demand curve’ that you find interesting. Repeat Question 2(b), this time excluding individuals who chose ‘abstain’ from the calculations. Plot this new ‘demand curve’ on the chart created for Question 2(c). Do your results change qualitatively depending on how you count individuals who did not vote? R walk-through 11.7 Summarizing Dichotomous Choice (DC) variables Create frequency table for DC_ref_outcome We can group by costs and DC_ref_outcome to obtain the number of observations for each combination of amount and vote response. We can also recode the voting options to ‘Yes’, ‘No’, and ‘Abstain’. WTP_DC <- WTP %>% group_by(costs, DC_ref_outcome) %>% summarize(n = n()) %>% na.omit() %>% mutate_at(''DC_ref_outcome'', funs(recode(., ''do not support referendum and no pay'' = ''No'', ''support referendum and pay'' = ''Yes'', ''would not vote'' = ''Abstain''))) %>% spread(DC_ref_outcome, n) kable(WTP_DC, format = ''markdown'', digits = 2) | costs| Abstain| No| Yes| |-----:|-------:|--:|---:| | 48| 12| 21| 32| | 72| 11| 30| 40| | 84| 12| 24| 45| | 108| 7| 35| 31| | 156| 13| 31| 40| | 192| 11| 25| 25| | 252| 9| 32| 28| | 324| 16| 41| 27| | 432| 11| 35| 29| | 540| 9| 31| 22| | 720| 12| 39| 13| | 960| 14| 28| 15| | 1200| 11| 42| 21| | 1440| 19| 42| 15| Add column showing proportion voting yes or no We can extend the table from Question 2(a) to include the proportion voting yes or no (to obtain percentages, multiply the values by 100). WTP_DC <- WTP_DC %>% mutate(total = Abstain + No + Yes, prop_no = (Abstain + No) / total, prop_yes = Yes / total) %>% # Round all numbers to 2 decimal places mutate_if(is.numeric, funs(round(., 2))) kable(WTP_DC, format = ''markdown'', digits = 2) | costs| Abstain| No| Yes| total| prop_no| prop_yes| |-----:|-------:|--:|---:|-----:|-------:|--------:| | 48| 12| 21| 32| 65| 0.51| 0.49| | 72| 11| 30| 40| 81| 0.51| 0.49| | 84| 12| 24| 45| 81| 0.44| 0.56| | 108| 7| 35| 31| 73| 0.57| 0.42| | 156| 13| 31| 40| 84| 0.52| 0.48| | 192| 11| 25| 25| 61| 0.59| 0.41| | 252| 9| 32| 28| 69| 0.59| 0.41| | 324| 16| 41| 27| 84| 0.68| 0.32| | 432| 11| 35| 29| 75| 0.61| 0.39| | 540| 9| 31| 22| 62| 0.64| 0.36| | 720| 12| 39| 13| 64| 0.80| 0.20| | 960| 14| 28| 15| 57| 0.74| 0.26| | 1200| 11| 42| 21| 74| 0.72| 0.28| | 1440| 19| 42| 15| 76| 0.80| 0.20| Make a line chart of WTP Using the dataframe generated for Questions 2(a) and (b) (WTP_DC), we can plot the ‘demand curve’ as a scatterplot with connected points by using the geom_point and geom_line options for ggplot. Adding the extra option scale_x_continuous changes the default labeling on the horizontal axis to display ticks at every 100 euros, enabling us to read the chart more easily. p <- ggplot(WTP_DC, aes(y = prop_yes, x = costs)) + geom_point() + geom_line(size = 1) + ylab(''% Voting 'Yes''') + xlab(''Amount (euros)'') + scale_x_continuous(breaks = seq(0, 1500, 100)) + theme_bw() print(p) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-06.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-06-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-06-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-06-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-06.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Demand curve (in euros), DC method. '' /> Demand curve (in euros), DC method. Figure 11.6 Demand curve (in euros), DC method. Calculate new proportions and add them to the table and chart It is straightforward to calculate the new proportions and add them to the existing dataframe, however, we will need to reshape the data (using gather) to plot multiple lines on the same scatterplot. WTP_DC <- WTP_DC %>% mutate(total_ex = No + Yes, prop_no_ex = No / total_ex, prop_yes_ex = Yes / total_ex) %>% # Round all numbers to 2 decimal places mutate_if(is.numeric, funs(round(., 2))) kable(WTP_DC, format = ''markdown'', digits = 2) | costs| Abstain| No| Yes| total| prop_no| prop_yes| total_ex| prop_no_ex| prop_yes_ex| |-----:|-------:|--:|---:|-----:|-------:|--------:|--------:|----------:|-----------:| | 48| 12| 21| 32| 65| 0.51| 0.49| 53| 0.40| 0.60| | 72| 11| 30| 40| 81| 0.51| 0.49| 70| 0.43| 0.57| | 84| 12| 24| 45| 81| 0.44| 0.56| 69| 0.35| 0.65| | 108| 7| 35| 31| 73| 0.57| 0.42| 66| 0.53| 0.47| | 156| 13| 31| 40| 84| 0.52| 0.48| 71| 0.44| 0.56| | 192| 11| 25| 25| 61| 0.59| 0.41| 50| 0.50| 0.50| | 252| 9| 32| 28| 69| 0.59| 0.41| 60| 0.53| 0.47| | 324| 16| 41| 27| 84| 0.68| 0.32| 68| 0.60| 0.40| | 432| 11| 35| 29| 75| 0.61| 0.39| 64| 0.55| 0.45| | 540| 9| 31| 22| 62| 0.64| 0.36| 53| 0.58| 0.42| | 720| 12| 39| 13| 64| 0.80| 0.20| 52| 0.75| 0.25| | 960| 14| 28| 15| 57| 0.74| 0.26| 43| 0.65| 0.35| | 1200| 11| 42| 21| 74| 0.72| 0.28| 63| 0.67| 0.33| | 1440| 19| 42| 15| 76| 0.80| 0.20| 57| 0.74| 0.26| WTP_DC %>% select(costs, prop_yes, prop_yes_ex) %>% gather(Vote, value, prop_yes:prop_yes_ex) %>% ggplot(., aes(y = value, x = costs, color = Vote)) + geom_line(size = 1) + geom_point() + ggtitle('''Demand curve' from DC respondents, under different treatments for 'Abstain' responses.'') + scale_color_manual(values = c(''blue'', ''red''), labels = c(''counted as no'', ''excluded'')) + ylab(''% voting 'yes''') + xlab(''Costs (euros)'') + theme_bw() <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-11-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Demand curve from DC respondents, under different treatments for ‘Abstain’ responses. '' /> Demand curve from DC respondents, under different treatments for ‘Abstain’ responses. Figure 11.7 Demand curve from DC respondents, under different treatments for ‘Abstain’ responses. Compare the mean and median WTP under both question formats: Complete Figure 11.8 and use it to calculate the difference in means (DC minus TWPL), the standard deviation of these differences, and the number of observations. (The mean of DC is the mean of DC_ref_outcome for individuals who voted yes.) Format Mean Standard deviation Number of observations DC TWPL Summary table for WTP. Figure 11.8 Summary table for WTP. Obtain 95% confidence intervals for the difference of means for each question format. Discuss your findings. Does the median WTP look different across question formats? (You do not need to do any formal calculations.) Using your answers to Questions 3(a)–(c), would you recommend that governments use the mean or median WTP in policy making decisions? (That is, which measure appears to be more robust to changes in the question format?) R walk-through 11.8 Calculating confidence intervals for differences in means Calculate the difference in means, standard deviations, and number of observations We first create two vectors that will contain the WTP values for each of the two question methods. For the DC format, willingness to pay is recorded in the costs variable, so we select all observations where the DC_ref_outcome variable indicates the individual voted ‘yes’ and drop any missing observations. For the TWPL format we use the WTP_average variable that we created in R walk-through 11.6. DC_WTP <- WTP %>% subset( DC_ref_outcome == ''support referendum and pay'') %>% select(costs) %>% filter(!is.na(costs)) %>% as.matrix() # Print out the mean, sd, and count cat(sprintf(''DC Format - mean: %.1f, standard deviation %.1f, count %dn'', mean(DC_WTP), sd(DC_WTP), length((DC_WTP)))) ## DC Format - mean: 348.2, standard deviation 378.6, count 383 TWPL_WTP <- WTP %>% select(WTP_average) %>% filter(!is.na(WTP_average)) %>% as.matrix() cat(sprintf(''TWPL Format - mean: %.1f, standard deviation %.1f, count %dn'', mean(TWPL_WTP), sd(TWPL_WTP), length((TWPL_WTP)))) ## TWPL Format - mean: 268.5, standard deviation 287.7, count 348 Calculate 95% confidence intervals Using the t.test function to obtain 95% confidence intervals was covered in R walk-throughs 8.10 and 10.6. As we have already separated the data for the two different question formats in Question 3(a), we can obtain the confidence interval directly. t.test(DC_WTP, TWPL_WTP, conf.level = 0.05)$conf.int ## [1] 78.10141 81.20560 ## attr(,''conf.level'') ## [1] 0.05 Calculate median WTP for the DC format In R walk-through 11.6 we obtained the median WTP for the TWPL format (132). We now obtain the WTP using the DC format. median(DC_WTP) ## [1] 192 Leading think tanks estimate that the world needs 20 trillion USD of investment in low-carbon energy supplies and energy efficient technologies by 2030 to meet the Paris Agreement targets. This amount roughly corresponds to 3,273 euros total per adult (aged 15 and above), or 298 euros per adult per year (2020 to 2030 inclusive). Compare this approximate number with the WTP estimates you have found. Assuming people around the world have the same attitudes towards climate change as the Germans surveyed, would a tax equal to the median WTP be enough to finance climate change mitigation? Discuss how governments could increase public support for and involvement in climate change mitigation activities."
});
index.addDoc({
    id: 60,
    title: "Doing Economics: Empirical Project 11: Working in Google Sheets",
    content: "Empirical Project 11 Working in Google Sheets Part 11.1 Summarizing the data Learning objectives for this part construct indices to measure attitudes or opinions use Cronbach’s alpha to assess indices for internal consistency practise recoding and creating new variables. We will be using data collected from an internet survey sponsored by the German government. First, download the survey data and documentation: Download the data. Read the ‘Data dictionary’ tab and make sure you know what each variable represents. (Later we will discuss exactly how some of these variables were coded.) Download a short description of the dataset, which explains the survey design and questions asked. You may find it helpful to read it before starting on the questions below. While contingent valuation methods can be useful, they also have shortcomings. Read Section 5 of the paper ‘Introduction to economic valuation methods’ (pages 16–19), and explain which limitations you think apply particularly to the survey we are looking at. You may also find it useful to look at Table 2 of that paper, which compares stated-preference with revealed-preference methods. Before comparing between question formats (dichotomous choice (DC) and two-way payment ladder (TWPL)), we will first compare the people assigned to each question format to see if they are similar in demographic characteristics and attitudes towards related topics (such as beliefs about climate change and the need for government intervention). If the groups are vastly dissimilar then any observed differences in answers between the groups might be due to differences in attitudes and/or demographics rather than the question format. Likert scaleA numerical scale (usually ranging from 1–5 or 1–7) used to measure attitudes or opinions, with each number representing the individual’s level of agreement or disagreement with a particular statement. Attitudes were assessed using a 1–5 Likert scale, where 1 = strongly disagree, and 5 = strongly agree. The way the questions were asked was not consistent, so an answer of ‘strongly agree’ might mean high climate change skepticism for one question, but low skepticism for another question. In order to combine these questions into an index we need to recode (in this case, reverse-code) some of the variables. Recode or create the variables as specified: Reverse-code the following variables (so that 1 is now 5, 2 is now 4, and so on): cog_2, cog_5, scepticism_6, scepticism_7. (Hint: One way to do this is to create a new variable and use Google Sheets’ IF function to fill in the values of the new variable based on the values of the original variable. For help on using the IF function, see Google Sheets walk-through 6.1.) For the variables ‘WTP_plmin’ and ‘WTP_plmax’, create new variables with the values replaced as shown in Figure 11.1 (these are the actual amounts, in euros, that individuals selected in the survey, and will be useful for calculating summary measures later). In order to produce the correct summary statistics, you will need to use Google Sheet’s IF function to preserve the blank cells and only fill in values for the new variable if the original variable is non-blank. Original value New value 1 48 2 72 3 84 4 108 5 156 6 192 7 252 8 324 9 432 10 540 11 720 12 960 13 1,200 14 1,440 WTP survey categories (original value) and euro amounts (new value). Figure 11.1 WTP survey categories (original value) and euro amounts (new value). Create the following indices, giving them an appropriate name in your spreadsheet (make sure to use the reverse-coded variable where relevant): Belief that climate change is a real phenomenon: Take the mean of scepticism_2, scepticism_6, and scepticism_7. Preferences for government intervention to solve problems in society: Take the mean of cog_1, cog_2, cog_3, cog_4, cog_5, and cog_6. Feeling of personal responsibility to act pro-environmentally: Take the mean of PN_1, PN_2, PN_3, PN_4, PN_6, and PN_7. Cronbach’s alphaA measure used to assess the extent to which a set of items is a reliable or consistent measure of a concept. This measure ranges from 0–1, with 0 meaning that all of the items are independent of one another, and 1 meaning that all of the items are perfectly correlated with each other. When creating indices, we may be interested to see if each item used in the index measures the same underlying variable of interest (known as reliability or consistency). There are two common ways to assess reliability: either look at the correlation between items in the index, or use a summary measure called Cronbach’s alpha (this measure is used in the social sciences). We will be calculating and interpreting both of these measures. Cronbach’s alpha is a way to summarize the correlations between many variables, and ranges from 0 to 1, with 0 meaning that all of the items are independent of one another, and 1 meaning that all of the items are perfectly correlated with each other. While higher values of this measure indicate that the items are closely related and therefore measure the same concept, with values that are very close to 1 (or 1) we could be concerned that our index contains redundant items (for example, two items that tell us the same information, so we would only want to use one or the other, but not both). You can read more about this in the paper ‘Using and interpreting Cronbach’s Alpha’. Calculate correlation coefficients and interpret Cronbach’s alpha: For one of the indices you created in Question 3, create a correlation table to show the correlation between each of the items in the index. Remember to give the variables meaningful names in your table (refer to the ‘Data dictionary’ tab for descriptions of each variable). For help on calculating correlation coefficients, see Google Sheets walk-through 1.7. Figure 11.2 shows an example for Question 3(a). (Remember that the correlation between A and B is the same as the correlation between B and A, so you only need to calculate the correlation for each pair of items once.) Are the items in that index strongly correlated? exaggeration not.human.activity no.evidence exaggeration 1 – – not.human.activity 1 – no.evidence 1 Correlation table for survey items on climate change scepticism: Climate change is exaggerated (exaggeration), Human activity is not the main cause of climate change (not.human.activity), No evidence of global warming (no.evidence). Figure 11.2 Correlation table for survey items on climate change scepticism: Climate change is exaggerated (exaggeration), Human activity is not the main cause of climate change (not.human.activity), No evidence of global warming (no.evidence). The Cronbach’s alpha for these indices are 0.66, 0.71, and 0.85 respectively. Interpret these values in terms of index reliability. Now we will compare characteristics of people in the dichotomous choice (DC) group and two-way payment ladder (TWPL) group (the variable ‘abst_format’ indicates which group an individual belongs to). Since the groups are of different sizes, we will use percentages instead of frequencies. For each group (DC and TWPL), create tables to summarize the distribution of the following variables (a separate table for each variable): gender (‘sex’) age (‘age’) number of children (‘kids_nr’) household net income per month in euros (‘hhnetinc’) membership in environmental organization (‘member’) highest educational attainment (‘education’). For help on creating tables, see Google Sheets walk-through 3.1. Using the tables you have created, and without doing formal calculations, discuss any similarities/differences in demographic characteristics between the two groups. Create a separate summary table as shown in Figure 11.3 for each of the three indices you created in Question 3. Without doing formal calculations, do the two groups of individuals look similar in the attitudes specified? Mean Standard deviation Min Max DC format TWPL format Summary table for indices. Figure 11.3 Summary table for indices. Part 11.2 Comparing willingness to pay across methods and individual characteristics Learning objectives for this part compare survey measures of willingness to pay. Before comparing WTP across question formats, we will summarize the distribution of WTP within each question format. For individuals who answered the TWPL question: Use the variables ‘WTP_plmin’ and ‘WTP_plmax’ to create column charts (one for each variable) with frequency on the vertical axis and category (the numbers 1–14) on the horizontal axis. Describe characteristics of the distributions shown on the charts. Using the variables you created in Question 2(b) in Part 11.1 (showing the actual WTP amounts), make a new variable that contains the average of the two variables (i.e. for each individual, calculate the average of the minimum and maximum willingness to pay). Calculate the mean and median of the variable you created in Question 1(b). Using the variable from Question 1(b), calculate the correlation between individuals’ average WTP and the demographic and attitudinal variables (see Questions 3 and 5 from Part 11.1 for a list of these variables). Interpret the relationships implied by the coefficients. For individuals who answered the DC question: Each individual was given one amount and had to decide ‘yes’, ‘no’, or ‘no vote/abstain from deciding’. Make a pivot table showing the frequency of ‘DC_ref_outcome’, with ‘costs’ as the row variable and ‘DC_ref_outcome’ as the column variable. Use this table to calculate the percentage of individuals who voted ‘no’ and ‘yes’ for each amount (in other words as a percentage of the row total, not the overall total). Count individuals who chose ‘abstain’ as voting ‘no’. Make a scatterplot showing the ‘demand curve’, with percentage of individuals who voted ‘yes’ as the vertical axis variable and amount (in euros) as the horizontal axis variable. (To connect the points, use the chart option ‘Scatter with Straight Lines and Markers’.) Describe features of this ‘demand curve’ that you find interesting. Repeat Question 2(b), this time excluding individuals who chose ‘abstain’ from the calculations. Plot this new ‘demand curve’ on the chart created for Question 2(c). Do your results change qualitatively, depending on how you count individuals who did not vote? Compare the mean and median WTP under both question formats: Complete Figure 11.4 and use it to calculate the difference in means (DC minus TWPL), the standard deviation of these differences, and the number of observations. (The mean of DC is the mean of ‘DC_ref_outcome’ for individuals who voted ‘yes’.) Format Mean Standard deviation Number of observations DC TWPL Summary table for WTP. Figure 11.4 Summary table for WTP. Use Google Sheets’ CONFIDENCE.T function and the calculated values for Question 3(a) to make a 95% confidence interval for the difference in means. Discuss your findings. (For help on calculating confidence intervals for the difference in means, see Google Sheets walk-through 6.4 and the discussion in Part 8.3.). Does the median WTP look different across question formats? (You do not need to do any formal calculations.) Using your answers to Questions 3(a)–(c), would you recommend that governments use the mean or median WTP in policy-making decisions? (That is, which measure appears to be more robust to changes in the question format?) Leading think tanks estimate that the world needs 20 trillion USD of investment in low-carbon energy supplies and energy efficient technologies by 2030 to meet the Paris Agreement targets. This amount roughly corresponds to 3,273 euros total per adult (aged 15 and above), or 298 euros per adult per year (2020 to 2030 inclusive). Compare this approximate number with the WTP estimates you have found. Assuming people around the world have the same attitudes towards climate change as the Germans surveyed, would a tax equal to the median WTP be enough to finance climate change mitigation? Discuss how governments could increase public support for and involvement in climate change mitigation activities."
});
index.addDoc({
    id: 61,
    title: "Doing Economics: Empirical Project 11 Solutions",
    content: "Empirical project 11 Solutions These are not model answers. They are provided to help students, including those doing the project outside a formal class, to check their progress while working through the questions using the Excel, R, or Google Sheets walk-throughs. There are also brief notes for the more interpretive questions. Students taking courses using Doing Economics should follow the guidance of their instructors. Part 11.1 Summarizing the data Surveys rely on respondents’ subjective opinions, which are subject to biases. These biases may affect the two types of answers differently. Public awareness of climate change issues has increased significantly over the past few decades, and people may feel morally obligated to conform to social norms and value the environmental costs highly. As a result, for dichotomous choice (DC) surveys, respondents may be inclined to choose the maximum amount. For two-way payment ladder (TWPL) surveys, respondents may be inclined to report overly large values, shifting the distribution to the right and leading to large variations in reported values. There are many other sources of bias for the surveys. For example, the subset of the population that participates in the survey may not be representative of the population. The design of questionnaires may also affect responses. However, for the purpose of comparing the differences between survey types, these biases may not be as significant. No solution is provided. No solution is provided. (a)–(c) No solution is provided. Solution figures 11.1, 11.2, and 11.3 provide solutions for all the indices. Values are rounded to two decimal places. exaggeration not.human.activity no.evidence exaggeration 1.00 not.human.activity 0.39 1.00 no.evidence 0.42 0.46 1.00 Correlation table for survey items on climate change scepticism: Climate change is exaggerated (exaggeration), Human activity is not the main cause of climate change (not.human.activity), No evidence of global warming (no.evidence). Solution figure 11.1 Correlation table for survey items on climate change scepticism: Climate change is exaggerated (exaggeration), Human activity is not the main cause of climate change (not.human.activity), No evidence of global warming (no.evidence). too.much not.pass.laws minimal.intervention not.dictate indiv.freedom personal.responsibility too.much 1.00 not.pass.laws 0.25 1.00 minimal.intervention 0.32 0.12 1.00 not.dictate 0.68 0.28 0.33 1.00 indiv.freedom 0.29 0.41 0.02 0.27 1.00 personal.responsibility 0.41 0.08 0.31 0.46 0.10 1.00 Correlation table for survey items on government intervention: the government interferes too much (too.much), governments should not pass laws so that people can act to their own advantage (not.pass.laws), governments should intervene as little as possible in economic matters (minimal.intervention), governments should stop dictating to people how they should live (not.dictate), the government should not do more to achieve social goals even if it restricts individual freedom (indiv.freedom), individuals should have more personal responsibility (personal.responsibility). Solution figure 11.2 Correlation table for survey items on government intervention: the government interferes too much (too.much), governments should not pass laws so that people can act to their own advantage (not.pass.laws), governments should intervene as little as possible in economic matters (minimal.intervention), governments should stop dictating to people how they should live (not.dictate), the government should not do more to achieve social goals even if it restricts individual freedom (indiv.freedom), individuals should have more personal responsibility (personal.responsibility). buy.local indiv.impact feel.better public.transport conserve.energy reduce.emissions buy.local 1.00 indiv.impact 0.48 1.00 feel.better 0.43 0.63 1.00 public.transport 0.42 0.44 0.46 1.00 conserve.energy 0.41 0.50 0.52 0.57 1.00 reduce.emissions 0.46 0.65 0.59 0.39 0.46 1.00 Correlation table for survey items on ‘personal responsibility for the environment’: I buy locally to reduce emissions (buy.local), I am obliged to take impact of daily activities on climate (individual.impact), I feel better when reducing emissions (feel.better), I prefer to use public transport (public.transport), I feel uncomfortable when consuming energy (conserve.energy), I try to reduce emissions as much as possible (reduce.emissions). Solution figure 11.3 Correlation table for survey items on ‘personal responsibility for the environment’: I buy locally to reduce emissions (buy.local), I am obliged to take impact of daily activities on climate (individual.impact), I feel better when reducing emissions (feel.better), I prefer to use public transport (public.transport), I feel uncomfortable when consuming energy (conserve.energy), I try to reduce emissions as much as possible (reduce.emissions). The correlations are all positive and most are moderately strong. Cronbach’s alpha assesses the internal reliability or consistency of a set of measures. Cronbach’s alpha ranges between 0 and 1 with 1 indicating maximum reliability. The coefficient values in question are high, suggesting that the indicators within each category measure the same underlying concept. Solution figures 11.4 to 11.9 summarize the distribution of the variables. Values are rounded to two decimal places. These two groups look very similar in terms of demographic characteristics. Gender TWPL (%) DC (%) Female 51.78 52.29 Male 48.22 47.71 Gender of participants, by group. Solution figure 11.4 Gender of participants, by group. Age range TWPL (%) DC (%) 18–24 9.49 9.64 25–29 8.30 8.65 30–39 17.79 17.20 40–49 22.33 22.56 50–59 24.11 23.86 60–69 17.98 18.09 Age of participants, by group. Solution figure 11.5 Age of participants, by group. Level of Education TWPL (%) DC (%) 1 (In school) 1.19 1.29 2 (Without school degree) 1.98 2.09 3 (Secondary general school) 34.19 32.80 4 (Intermediate general school) 26.28 26.94 5 (Polytechnic school) 6.92 6.86 6 (University preparatory school) 29.45 30.02 Highest educational attainment, by group. Solution figure 11.6 Highest educational attainment, by group. Number of children TWPL (%) DC (%) No children 64.62 65.71 One 20.36 17.59 Two 11.07 12.33 Three 2.96 3.48 Four or more 0.99 0.89 Number of children, by group. Solution figure 11.7 Number of children, by group. Membership TWPL (%) DC (%) No 92.29 91.35 Yes 7.71 8.65 Environmental organization membership, by group. Solution figure 11.8 Environmental organization membership, by group. Income range (euros) TWPL (%) DC (%) Less than 500 2.96 4.17 500–1,100 13.44 14.21 1,100–1,500 14.23 13.22 1,500–2,000 15.02 14.61 2,000–2,600 11.46 14.81 2,600–3,200 10.67 10.74 3,200–4,000 11.07 8.15 4,000–5,000 5.14 4.97 5,000–6,000 2.77 1.69 6,000–7,500 0.79 0.40 7,500 or more 0.40 0.50 Do not want to answer 12.06 12.52 Household net income per month in euros, by group. Solution figure 11.9 Household net income per month in euros, by group. Solution figures 11.10, 11.11, and 11.12 provide summary tables for the indices. The two groups are quite similar in attitudes. Combined with the answer to Question 5, we can be reasonably confident that any differences in survey responses is due to the question format rather than differences in attitudes or demographics. Mean Std Min Max Ref (DC) 2.37 0.85 1.00 5.00 Ladder (TWPL) 2.29 0.84 1.00 5.00 Summary table for ‘climate change beliefs’ index. Solution figure 11.10 Summary table for ‘climate change beliefs’ index. Mean Std Min Max Ref (DC) 3.19 0.66 1.00 5.00 Ladder (TWPL) 3.15 0.70 1.00 5.00 Summary table for ‘preferences for government intervention’ index. Solution figure 11.11 Summary table for ‘preferences for government intervention’ index. Mean Std Min Max Ref (DC) 3.01 0.82 1.00 5.00 Ladder (TWPL) 3.03 0.79 1.00 5.00 Summary table for ‘personal responsibility for the environment’ index. Solution figure 11.12 Summary table for ‘personal responsibility for the environment’ index. Part 11.2 Comparing willingness to pay across methods and individual characteristics Solution figures 11.13 and 11.14 provide column charts for the variables. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-13.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-13-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-13-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-13-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-13.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Column charts of minimum WTP. '' /> Column charts of minimum WTP. Solution figure 11.13 Column charts of minimum WTP. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-14.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-14-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-14-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-14-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-14.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Column charts of maximum WTP. '' /> Column charts of maximum WTP. Solution figure 11.14 Column charts of maximum WTP. The distributions are more left-heavy (higher number of lower values). No solution is provided. Mean WTP is 268.53 and median WTP is 132.00 (rounded to two decimal places). Solution figure 11.15 provides the correlation between the average WTP and the demographic and attitudinal variables. The correlation coefficients are very low, suggesting that average WTP may not be strongly correlated with the variables. Variable Correlation Education 0.14 Gender 0.04 Belief –0.14 Preferences –0.19 Feelings 0.19 Correlation table of average WTP and other variables. Solution figure 11.15 Correlation table of average WTP and other variables. Solution figure 11.16 shows the table. Amount (euros) No Yes Abstain Total 48 21 32 12 65 72 30 40 11 81 84 24 45 12 81 108 35 31 7 73 156 31 40 13 84 192 25 25 11 61 252 32 28 9 69 324 41 27 16 84 432 35 29 11 75 540 31 22 9 62 720 39 13 12 64 960 28 15 14 57 1,200 42 21 11 74 1,440 42 15 19 76 **Total** **456** **383** **167** **1,006** DC format: Responses for each amount. Solution figure 11.16 DC format: Responses for each amount. Solution figure 11.17 shows the percentage of individuals who voted ‘no’ and ‘yes’ for each amount. Values are rounded to two decimal places. Amount (euros) No Yes 48 50.77 49.23 72 50.62 49.38 84 44.44 55.56 108 57.53 42.47 156 52.38 47.62 192 59.02 40.98 252 59.42 40.58 324 67.86 32.14 432 61.33 38.67 540 64.52 35.48 720 79.69 20.31 960 73.69 26.32 1,200 71.62 28.38 1,440 80.26 19.74 DC format: Reponses (in percentages), with ‘abstain’ counted as ‘no’. Solution figure 11.17 DC format: Reponses (in percentages), with ‘abstain’ counted as ‘no’. Solution figure 11.18 shows the demand curve. The demand curve is generally downward sloping, with the percentage voting ‘yes’ decreasing as the amount increases, though this relationship is not perfect due to the nature of the survey, with each individual only being asked their willingness to pay for one amount. (This curve therefore does not represent the sum of individuals’ willingness to pay at a given price, so is not a ‘demand curve’ in the strict sense.) <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-18.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-18-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-18-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-18-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-18.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''‘Demand curve’ from DC respondents. '' /> ‘Demand curve’ from DC respondents. Solution figure 11.18 ‘Demand curve’ from DC respondents. Solution figure 11.19 shows the percentage of individuals who voted ‘no’ and ‘yes’ with ‘abstain’ excluded. Solution figure 11.20 provides the demand curve. The results do not appear to change drastically after excluding ‘abstain’ respondents. Amount (euros) No Yes 48 39.62 60.38 72 42.86 57.14 84 34.78 65.22 108 53.03 46.97 156 43.66 56.34 192 50.00 50.00 252 53.33 46.67 324 60.29 39.71 432 54.69 45.31 540 58.49 41.51 720 75.00 25.00 960 65.12 34.88 1,200 66.67 33.33 1,440 73.68 26.32 DC format: Responses (in percentages), with ‘abstain’ responses excluded. Solution figure 11.19 DC format: Responses (in percentages), with ‘abstain’ responses excluded. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-20.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-20-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-20-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-20-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-11-20.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''‘Demand curve’ from DC respondents, under different treatments for ‘abstain’ responses. '' /> ‘Demand curve’ from DC respondents, under different treatments for ‘abstain’ responses. Solution figure 11.20 ‘Demand curve’ from DC respondents, under different treatments for ‘abstain’ responses. In Solution figure 11.21, the extra row shows the calculated values for the difference in means. Values are rounded to two decimal places. Format Mean Standard deviation Number of observations DC 348.19 378.65 383 TWPL 268.54 287.70 348 Diff in means 79.65 475.50 731 Summary table for WTP. Solution figure 11.21 Summary table for WTP. The ‘width’ of the confidence interval is 1.55, so the confidence interval is [79.65 – 1.55, 79.65 + 1.55], which is [78.10, 81.21]. The difference in means is large (around 80 euros) and also precisely estimated, so we can be confident that the observed difference is not due to chance i.e. WTP is higher under the DC format than the TWPL format. The median for the DC format is 192, which is similar to the median for the TWPL format (132). In contrast, the mean for the DC format is almost double the mean of the TWPL format. The median is therefore more robust to changes in question format, and governments may want to use the median instead of mean WTP. The median WTP is slightly lower than what is needed to completely fund the required mitigation activities. There are many ways to increase support/involvement in climate change mitigation activities. While most people now believe that climate change is real, they may underestimate its severity and/or believe that their individual actions cannot help address the problem. Governments can help change these attitudes through public information campaigns or funding organizations that publicize the effects of climate change (for example, the BBC’s Blue Planet II documentary not only helped raise public awareness of human impact on the environment, but also resulted in lifestyle changes for the majority of people who watched it)."
});
index.addDoc({
    id: 62,
    title: "Doing Economics: Empirical Project 12: Government policies and popularity: Hong Kong cash handout",
    content: "Empirical Project 12 Government policies and popularity: Hong Kong cash handout Learning objectives In this project you will: draw Lorenz curves and calculate Gini coefficients (Part 12.1) assess the effect of a policy on income inequality (Part 12.1) and government popularity (Part 12.2) convert nominal values to real values (extension) (Part 12.1). Key concepts Concepts needed for this project: percentile, Gini coefficient, and Lorenz curve. Concepts introduced in this project: nominal and real values. Note Inequality, Lorenz curves, and Gini coefficients are discussed in more detail in Project 5. Introduction CORE projects This empirical project is related to material in: Unit 12 of Economy, Society, and Public Policy Unit 19 and Unit 22 of The Economy. An important role of the government is to use tax revenue to provide goods and services for its citizens. When governments have an unanticipated budget surplus (taxes exceed government spending and interest payments on government debt), they may choose to increase spending on public infrastructure programs or improve the publicly funded goods and services provided to their citizens. An alternative way to handle a budget surplus is to simply make a payment to citizens. While this policy may seem unconventional, it is exactly what the Hong Kong Government did in 2011. In every year from 2004 to 2010, the Hong Kong Government had a budget surplus, so they decided to distribute part of this surplus to the public by giving a one-off payment of $6,000 HKD (approximately 770 USD) to every citizen aged 18 or above, irrespective of need. This program, known as Scheme $6,000, was announced at the start of 2011, and after a year-long registration process, transfers were made to citizens’ bank accounts in 2012. You can read a brief summary of Scheme $6,000 in the article ‘Government to start next phase of Scheme $6,000’. While it may seem odd that citizens would fail to claim a handout from the government that is granted to all, 120,000 residents (around 2% of all eligible residents) did not register to receive the handout. Various arguments were raised against using the scheme, including its failure to address income inequality effectively. We will assess the effects that this policy could have on inequality, and discuss some reasons why governments may choose this policy over other redistributive policies. Working in Excel Working in R Working in Google Sheets"
});
index.addDoc({
    id: 63,
    title: "Doing Economics: Empirical Project 12: Working in Excel",
    content: "Empirical Project 12 Working in Excel Excel-specific learning objectives In addition to the learning objectives for this project, in this section you will learn how to convert cells from text to number format. Part 12.1 Inequality Learning objectives for this part draw Lorenz curves and calculate Gini coefficients assess the effect of a policy on income inequality convert nominal values to real values (extension). One reason cited for Scheme $6,000 was to share the gains from economic growth among everyone in the society. We will be using household income data collected by the Hong Kong government to assess the potential effects of this scheme. Download the data. The spreadsheet contains information about household incomes for certain percentiles of the population. These incomes are ‘pre-intervention’, meaning that they do not include the effects of handouts or policy interventions from the government. (If you are curious about the difference between the two tables in this spreadsheet, see the extension section ‘Nominal and real values’ at the end of Part 12.1.) Using the table ‘Monthly real household income (pre-tax, $HKD)’, plot a separate line chart for each percentile, with year on the horizontal axis and income on the vertical axis. Describe any patterns you see over time. Now we will use this data to draw Lorenz curves and compare changes in the income distribution for 2011–2012. One way to do this is to make the following simplifying assumptions: There are 100 households in the economy (so we can think of each percentile as corresponding to one household). Households between the 15th and 25th percentile have the same income as the household in the 15th percentile, households between the 25th and 50th percentile have the same income as the household in the 25th percentile, and so on. (Households below the 15th percentile earn nothing.) Draw Lorenz curves for 2011 and 2012 by carrying out the following: Create a new column for 2012 only, showing incomes following the $6,000 handout. Assume that nothing else has changed except for the cash handout. (Remember that this amount was given to all households, including those with no income.) Calculate the economy-wide earnings in 2011 and 2012 (with the $6,000 cash handout included). (Hint: Multiply the income of a given percentile by the number of households assumed to earn that amount.) Use your answer to Question 2(b) to complete the table in Figure 12.1 below. (The second row also shows zeros in 2011 because the bottom 15% of households earned nothing.) For help on how to calculate cumulative shares, see Excel walk-through 5.1. Cumulative share of the population (%) Perfect equality line Cumulative share of income in 2011 (%) Cumulative share of income in 2012 (%) 0 0 0 0 15 15 0 25 25 50 50 75 75 85 85 100 100 100 100 Cumulative share of income, for some percentiles of the population. Figure 12.1 Cumulative share of income, for some percentiles of the population. Draw the Lorenz curves for 2011 and 2012 in the same chart, with cumulative share of population on the horizontal axis and cumulative share of income on the vertical axis. In order for the chart to be drawn to scale, use the scatterplot option ‘Scatter with Straight Lines and Markers’ instead of the usual line chart option. Add the perfect equality line to your chart, and add a chart legend. Now we will compare the Gini coefficients in 2011, 2012 (with the cash handout), and 2013 (a year after the policy came into effect). Instead of calculating the coefficients manually, we will use an online Gini coefficient calculator, which takes a list of values and calculates the Gini coefficient. Calculate the Gini coefficient by carrying out the following: Create a table as in Figure 12.2, and fill in the remaining values (some values for 2011 have been filled in for you). Remember that the 2012 data should include the cash handout. The first column should contain the numbers 1 to 100, in intervals of 1, and the remaining columns should contain the incomes earned by a household at that percentile (using the same assumption as in Question 2). To obtain more accurate answers, use two decimal places instead of rounding incomes to the nearest dollar. Percentile 2011 2012 2013 1 0.00 2 0.00 3 0.00 … 98 44,516.67 99 44,516.67 100 44,516.67 Incomes earned by each percentile of the population. Figure 12.2 Incomes earned by each percentile of the population. Create a new column for each year, containing the incomes with a comma added at the end (e.g. ‘40,000’ would be ‘40,000,’). Then, for each year, copy and paste the income values for all percentiles into the Gini coefficient calculator to obtain the Gini coefficient for that year. Hint: The CONCATENATE function can combine the contents of cells and/or text together. The formula =CONCATENATE(cell1, “,”) adds a comma after the contents of cell1. Based on the Gini coefficients from Question 3(b), what effect did the $6,000 handout appear to have on income inequality in the short run, and in the long run? Suggest some explanations for what you observe. In our analysis we assumed that the $6,000 handout was the only policy that affected households in 2012. In reality a household’s disposable income will also depend on taxes and transfers. Without doing additional calculations, explain what would happen to the shape of the Lorenz curve and inequality in 2012 if: households in and below the 15th percentile received cash transfers from the government households in and above the 75th percentile had to pay income tax. Extension Nominal and real values In this extension section, we will discuss how the table ‘Monthly nominal household income (pre-tax, $HKD)’ taken from the ‘Hong Kong poverty situation report 2016’ was used to create the table ‘Monthly real household income (pre-tax, $HKD)’, and why we needed to make this conversion. inflationAn increase in the general price level in the economy. Usually measured over a year. See also: deflation, disinflation. The difference between real and nominal income is that real income takes inflation into account. You may be familiar with the concept of inflation, which is an increase in the general price level in the economy. Usually inflation is measured by taking a fixed bundle of goods and services and looking at how much it would cost to buy that bundle, compared to a reference year. (For more details about real vs nominal variables, see the Einstein ‘Comparing income at different times, and across different countries’ in Section 1.2 of The Economy). If the bundle of goods and services has become more expensive, then we conclude that the price level in the economy has increased. In this case, the values from 2010 onwards have been adjusted to account for the fact that prices have increased since 2009, so the same income would be able to purchase fewer goods and services. Without making this adjustment, we would conclude that households in the 15th percentile had the same purchasing power in 2009 and 2010, when in fact they do not as they can buy fewer goods and services in 2010 because of the overall price increase. Convert nominal values to real values, using 2009 as the reference year: To understand what happens to a given nominal value over time due to inflation, create a table as in Figure 12.3 below, and fill it in according to the percentage increases shown. (These percentages were taken from the Monthly Digest of Statistics.) (For example, $1 in 2009 would be $1 × (1 + (2.40/100)) = $1.024 in 2010.) For greater accuracy, round your answers to three decimal places. With a starting value of $1 in 2009, what would the value be in 2016? Year Percentage increase (from previous year) Inflation index 2009 1.000 2010 2.40 1.024 2011 5.30 2012 4.10 2013 4.30 2014 4.40 2015 3.00 2016 2.40 Creating an index-based series from percentage increases. Figure 12.3 Creating an index-based series from percentage increases. Use this table to convert nominal incomes to real incomes by dividing the nominal income by the corresponding value in the third column (for example, divide nominal incomes in 2010 by 1.024 to get the value in 2009 terms). You should get the same values as in the ‘real household income’ table. Part 12.2 Government popularity Learning objectives for this part assess the effect of a policy on government popularity. One possible reason why the government implemented Scheme $6,000 was to gain public approval, since there was some pressure on the government to spend the surplus on alleviating current social issues rather than reinvesting it (for example, in pension schemes). We will use a public opinion poll conducted by the University of Hong Kong to assess whether this scheme could have improved public satisfaction with the government. Think about the groups who would be affected by this scheme (for example, the government or members of the public). Who would be likely to support or oppose this scheme, and why? Download the data: Go to the overall performance results page on the HKU POP website, which contains half-yearly survey data on the overall performance of the government. Under the subheading ‘Collapsed data’, copy and paste the entire table directly into a new tab in Excel. (The data is not available in Excel format, so you will have to resort to this method.) Excel will recognize the data you have pasted as text, but to make charts, the data needs to be in number format. Follow the steps in Excel walk-through 12.1 below to reformat the column called ‘Net Value’. Read the HKU POP survey methods page for a description of how the survey data was collected. Explain whether you think the sample is representative of the target population, and discuss some limitations of the survey method. Excel walk-through 12.1 Using SUBSTITUTE to clean text in cells <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to use SUBSTITUTE to clean text in cells by removing spaces before and after the text. '' /> How to use SUBSTITUTE to clean text in cells by removing spaces before and after the text. Figure 12.4 How to use SUBSTITUTE to clean text in cells by removing spaces before and after the text. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : We will use the data in Column I as an example. '' /> The data We will use the data in Column I as an example. Figure 12.4a We will use the data in Column I as an example. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Numbers stored as text : Excel currently recognizes the values in Column I as text, so when you try to plot a line chart, it will come out like this, with no actual data being plotted. '' /> Numbers stored as text Excel currently recognizes the values in Column I as text, so when you try to plot a line chart, it will come out like this, with no actual data being plotted. Figure 12.4b Excel currently recognizes the values in Column I as text, so when you try to plot a line chart, it will come out like this, with no actual data being plotted. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Remove unnecessary spaces : Excel thinks the numbers are text because there is a space before and after the number. To remove the spaces, we use the SUBSTITUTE function (to remove non-breaking spaces, i.e. spaces that do not result in a new line), then the TRIM function (which removes spaces before and after text). Note: For Mac users, type ‘CHAR(202)’ in the formula instead of ‘CHAR(160)’. '' /> Remove unnecessary spaces Excel thinks the numbers are text because there is a space before and after the number. To remove the spaces, we use the SUBSTITUTE function (to remove non-breaking spaces, i.e. spaces that do not result in a new line), then the TRIM function (which removes spaces before and after text). Note: For Mac users, type ‘CHAR(202)’ in the formula instead of ‘CHAR(160)’. Figure 12.4c Excel thinks the numbers are text because there is a space before and after the number. To remove the spaces, we use the SUBSTITUTE function (to remove non-breaking spaces, i.e. spaces that do not result in a new line), then the TRIM function (which removes spaces before and after text). Note: For Mac users, type ‘CHAR(202)’ in the formula instead of ‘CHAR(160)’. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Copy and paste into a new column : The numbers in Column J are still stored as text, but we cannot convert Column J to numbers because there is a formula in those cells. Instead, we need to copy and paste (values only) into a new column, then convert the pasted data to numbers. '' /> Copy and paste into a new column The numbers in Column J are still stored as text, but we cannot convert Column J to numbers because there is a formula in those cells. Instead, we need to copy and paste (values only) into a new column, then convert the pasted data to numbers. Figure 12.4d The numbers in Column J are still stored as text, but we cannot convert Column J to numbers because there is a formula in those cells. Instead, we need to copy and paste (values only) into a new column, then convert the pasted data to numbers. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Convert text to numbers : Now Excel will recognize that numbers are stored as text, and will give you the option to convert them to numbers. '' /> Convert text to numbers Now Excel will recognize that numbers are stored as text, and will give you the option to convert them to numbers. Figure 12.4e Now Excel will recognize that numbers are stored as text, and will give you the option to convert them to numbers. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/figure-12-04-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Numbers correctly stored in Excel : Now Excel recognizes the values in Column K as numbers, so you can use them to plot a line chart. '' /> Numbers correctly stored in Excel Now Excel recognizes the values in Column K as numbers, so you can use them to plot a line chart. Figure 12.4f Now Excel recognizes the values in Column K as numbers, so you can use them to plot a line chart. Assess public satisfaction with the government by carrying out the following: Make a line chart with overall public satisfaction (net value, which is the difference between percentage of positive and negative responses) on the vertical axis, and time (Jan–June 2006 to the latest period available) on the horizontal axis. Remember to sort your data in ascending date order (earliest to latest) before making your chart (for help, see Excel walk-through 5.3). Comment on any trends in overall public satisfaction over this time period. Go to the POP polls main data page and choose one or two other indicators that are directly related to the policy (for example, improving people’s ‘Degree of prosperity’ or ‘Degree of equality’). Find a table of the data by clicking ‘Content’, then ‘Table’ (if half-yearly data is not available, choose the most similar time interval). Copy and paste the data into a new tab, and reformat the variable of interest as in Question 2. For each of your chosen indicators, make a separate line chart as in Question 3(a) and comment on any similarities to or differences from the chart in Question 3(a). (Since some indicators may be measured on a different scale, focus on changes over time.) Do you think the scheme had the intended effect on government popularity? Besides the scheme, what other factors or events could explain the observed patterns? In 2018, the government decided to do another cash handout. Read the article ‘Hong Kong cash handout scheme will cost government HK$330 million to administer’ and discuss how this scheme differs from the 2011 scheme. Explain whether you think this policy is an improvement over the 2011 scheme. Suppose you are a policymaker in a developed country with a large budget surplus, and one of the government’s aims is to reduce income inequality. Would you recommend that the government implement a scheme similar to either the 2011 or 2018 scheme? If you recommend a cash handout, suggest some modifications that could make the scheme more effective. If not, suggest other policies that may be more effective in reducing inequality. (You may find it helpful to research policies aimed at reducing income inequality, for example Universal Basic Income, which some countries have tried.)"
});
index.addDoc({
    id: 64,
    title: "Doing Economics: Empirical Project 12: Working in R",
    content: "Empirical Project 12 Working in R Download the code To download the code chunks used in this project, right-click on the download link and select ‘Save Link As…’. You’ll need to save the code download to your working directory, and open it in RStudio. Don’t forget to also download the data into your working directory by following the steps in this project. Getting started in R For this project you will need the following packages: tidyverse, to help with data manipulation readxl, to import an Excel spreadsheet knitr, to format tables ineq, to calculate inequality measures rvest, to import tables directly from websites zoo, to format times and dates. If you need to install any of these packages, run the following code: install.packages(c(''tidyverse'', ''readxl'', ''knitr'', ''ineq'', ''rvest'', ''zoo'')) You can import the libraries now, or when they are used in the R walk-throughs below. library(tidyverse) library(readxl) library(knitr) library(ineq) library(rvest) library(zoo) Part 12.1 Inequality Learning objectives for this part draw Lorenz curves and calculate Gini coefficients assess the effect of a policy on income inequality convert nominal values to real values (extension). One reason cited for Scheme $6,000 was to share the gains from economic growth among everyone in the society. We will be using household income data collected by the Hong Kong government to assess the potential effects of this scheme. Download the data. The spreadsheet contains information about household incomes for certain percentiles of the population. These incomes are ‘pre-intervention’, meaning that they do not include the effects of handouts or policy interventions from the government. (If you are curious about the difference between the two tables in this spreadsheet, see the extension section ‘Nominal and real values’ at the end of Part 12.1.) Using the table ‘Monthly real household income (pre-tax, $HKD)’, plot a separate line chart for each percentile, with year on the horizontal axis and income on the vertical axis. Describe any patterns you see over time. R walk-through 12.1 Importing a specified range of data from a spreadsheet We start by importing the data. The data provided in the Excel spreadsheet is not in the usual format of one variable per column (known as ‘long’ format). Instead the first tab contains two separate tables, and we need the second table. We can therefore use the range = option of the read_excel function to specify the cells in the spreadsheet to import (note that variable headers for the years are included). library(tidyverse) library(knitr) library(readxl) # Set your working directory to the correct folder. # Insert your file path for 'YOURFILEPATH'. setwd(''YOURFILEPATH'') # Import the data, stating the cell range income <- read_excel(''Project-12-datafile.xlsx'', range = ''A13:I18'') # Rename the first column names(income)[1] <- ''percentile'' We can use the ggplot function to make a line chart showing the change in real income on the vertical axis (y = real_inc) and year on the horizontal axis (x = Year) for each of the percentile groups (group = percentile) on a single chart as follows. income %>% gather(Year, real_inc, `2009`:`2016`) %>% ggplot(., aes(y = real_inc, x = Year, group = percentile, color = percentile)) + geom_line(size = 1) + theme_bw() + ylab(''Monthly real household income (HKD)'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-01.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-01-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-01-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-01-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-01.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Monthly real household income over time. '' /> Monthly real household income over time. Figure 12.1 Monthly real household income over time. However, plotting all percentile groups using the same scale hides a lot of variation within each percentile group. So, we should create a separate chart for each percentile group. As an example, we will plot the chart for the 15th percentile. First, we use the gather function to reshape the data into the format that R uses to plot charts. Then we use the filter function to select data for the 15th percentile only and use mutate to convert Year to a numeric variable (otherwise, R will not know the years are numbers and cannot plot the chart). Finally, we use gglot to make the line chart as before. income %>% # Reshape the data gather(Year, real_inc, `2009`:`2016`) %>% # Select only data for 15th percentile filter(percentile == ''15th'') %>% # Set 'Year' as a numeric variable so we can plot a line mutate(Year = as.numeric(Year)) %>% ggplot(., aes(y = real_inc, x = Year)) + geom_point(size = 2) + geom_line(size = 1) + theme_bw() + ylab( ''Monthly real household income for 15th percentile'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-02.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-02-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-02-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-02-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-02.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Monthly real household income for 15th percentile. '' /> Monthly real household income for 15th percentile. Figure 12.2 Monthly real household income for 15th percentile. Now we will use this data to draw Lorenz curves and compare changes in the income distribution for 2011–2012. One way to do this is to make the following simplifying assumptions: There are 100 households in the economy (so we can think of each percentile as corresponding to one household). Households between the 15th and 25th percentile have the same income as the household in the 15th percentile, households between the 25th and 50th percentile have the same income as the household in the 25th percentile, and so on. (Households below the 15th percentile earn nothing.) Draw Lorenz curves for 2011 and 2012 by carrying out the following: Create a new column for 2012 only, showing incomes following the $6,000 handout. Assume that nothing else has changed except for the cash handout. (Remember that this amount was given to all households, including those with no income.) Calculate the economy-wide earnings in 2011 and 2012 (with the cash handout included). (Hint: Multiply the income of a given percentile by the number of households assumed to earn that amount.) Use your answers to Question 2(b) to complete the table in Figure 12.3 below. (The second row also shows zeros in 2011 because the bottom 15% of households earned nothing.) For help on how to calculate cumulative shares, see R walk-through 5.1. Cumulative share of the population (%) Perfect equality line Cumulative share of income in 2011 (%) Cumulative share of income in 2012 (%) 0 0 0 0 15 15 0 25 25 50 50 75 75 85 85 100 100 100 100 Cumulative share of income, for some percentiles of the population. Figure 12.3 Cumulative share of income, for some percentiles of the population. Draw the Lorenz curves for 2011 and 2012 in the same chart, with cumulative share of population on the horizontal axis and cumulative share of income on the vertical axis. Add the perfect equality line to your chart, and add a chart legend. R walk-through 12.2 Calculating cumulative income shares and plotting a Lorenz curve Questions 2(a)–(c) can be completed in one stage using the piping technique, although there are a number of steps. The piping operator (%>%) from the tidyverse package allows us to perform multiple functions, one after another. (For more information on piping operators, see The University of Manchester’s Econometric Computing Learning Resource.) First, we use the select function to get the relevant variables. Then, we remove the ‘th’ suffix in the percentile values using the separate function, and convert them to numeric values using the mutate function (as in R walk-through 12.1). We then use rbind (which stands for ‘row bind’) to add an extra row for the 0th percentile, and cbind (‘column bind’) to add a column representing the number of households in each percentile group (assuming 100 households in the economy). Now, we use the mutate function to create new variables containing the information we need to draw the Lorenz curve. We create a new variable called 2012_handout that adds $6,000 to each value for the year 2012. We calculate the economy-wide income for each percentile group by multiplying the income values with the number of households, storing these in the variables 2011_income and 2012_income. To complete Figure 12.3, we need to calculate the cumulative income for each percentile group. To sort the data from the lowest to highest percentile groups we use the arrange function. Next we use the cumsum function to calculate the cumulative sum of economy-wide incomes for each year, storing these in the variables 2011_sum and 2012_sum. Finally we tidy up the data to reflect Figure 12.3 by making sure that the 0th percentile has no share of the income and the 100th percentile has 100% of the cumulative income. df.new <- income %>% # Select the required variables select(percentile, `2011`, `2012`) %>% # Separate words from numbers separate(percentile, ''percentile'', ''th'') %>% # Change character variable to numeric variable mutate(percentile = as.numeric(percentile)) %>% # Add observation for 0th percentile rbind(c(0, 0, 0)) %>% # Order percentile values from low to high arrange(percentile) %>% # Add column for the number of households in each group cbind(households = c(15, 10, 25, 25, 10, 15)) %>% mutate(`2012_handout` = `2012` + 6000) %>% # Economy-wide income mutate(`2011_income` = `2011` * households, `2012_income` = `2012_handout` * households) %>% # Relative share in 2011 mutate(`2011_cum` = cumsum(`2011_income`) / sum(`2011_income`) * 100) %>% # Relative share in 2012 mutate(`2012_cum` = cumsum(`2012_income`) / sum(`2012_income`) * 100) %>% # Drop interim variables select(percentile, households, `2011_cum`, `2012_cum`) %>% # Tidy up the table to normalize 0 and 100 percentiles mutate(percentile = percentile + households) %>% select(-households) %>% rbind(c(0, 0, 0), .) kable(df.new, format = ''markdown'', digits = 2) | percentile| 2011_sum| 2012_sum| |----------:|--------:|--------:| | 0| 0.00| 0.00| | 15| 0.00| 3.91| | 25| 2.74| 8.44| | 50| 15.09| 24.53| | 75| 41.42| 50.37| | 85| 60.50| 67.09| | 100| 100.00| 100.00| Using the data from Questions 2(a)–(c) we can plot the Lorenz curve using the ggplot function. Note that we use the percentile variable to draw the line of perfect equality. df.new %>% # Create variable for perfect equality mutate(equality = percentile) %>% # Reshape into long format for plotting gather(year, cum_income, `2011_cum`:equality) %>% ggplot(., aes(y = cum_income, x = percentile, group = year, color = year)) + geom_line(size = 1) + geom_point(size = 2) + theme_bw() + ylab(''Cumulative share of income (%)'') + xlab(''Cumulative share of population (%)'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Lorenz curve (2011). '' /> Lorenz curve (2011). Figure 12.4 Lorenz curve (2011). Now we will compare the Gini coefficients in 2011, 2012, and 2013 (a year after the policy came into effect). Calculate the Gini coefficient by carrying out the following: Create a table as in Figure 12.5, and fill in the remaining values (some values for 2011 have been filled in for you). The first column should contain the numbers 1 to 100, in intervals of 1, and the remaining columns should contain the incomes earned by a household at that percentile (using the same assumption as in Question 2). Remember that the 2012 data should include the cash handout. To obtain more accurate answers, use two decimal places instead of rounding incomes to the nearest dollar. Percentile 2011 2012 2013 1 0.00 2 0.00 3 0.00 … 98 44,516.67 99 44,516.67 100 44,516.67 Incomes earned by each percentile of the population. Figure 12.5 Incomes earned by each percentile of the population. Using the ineq function, calculate the Gini coefficient for each year. Based on the Gini coefficients from Question 3(b), what effect did the $6,000 handout appear to have on income inequality in the short run, and in the long run? Suggest some explanations for what you observe. R walk-through 12.3 Generating Gini coefficients Create table containing percentiles To create the percentiles for every percentile in our 100 household economy we need to take the income for each percentile group and expand that for every household in the respective percentile group. For example, there are 15 households in the bottom percentile group having zero income for 2011 and 2013, and $6,000 in 2012. For the 15th percentile group there are 15 households that will share the same income value, and so on for the other percentile groups. To achieve this expansion, we first create a new variable (households) that contains the number of households in each percentile group. Then for each group we can use the slice and rep functions together to copy the values for 2011, 2012, and 2013 within each percentile group. The rep function takes a sequence of 1 to 6 (in other words the number of groups) and copies each number in the sequence by the corresponding value in the households variable. The result is a variable containing 15 1s, 10 2s, 25 3s, and so on. The splice function then uses this index to reference each observation in the data, for example where the index value is 1 the bottom percentile group is selected, where the index value is 2 the 15th percentile group is selected, and so on. Once the expansion is complete we order the data by row and generate a new sequence from 1 to 100 to represent the percentile. The final result is the dataframe called df.new. df.new <- income %>% select(`2011`, `2012`, `2013`) %>% rbind(c(0, 0, 0)) %>% # Apply the $6,000 handout to 2012 mutate(`2012` = `2012` + 6000) %>% # Arrange by ascending incomes in 2011 arrange(`2011`) %>% # Specify number of households in each group mutate(households = c(15, 10, 25, 25, 10, 15)) %>% group_by(households) %>% # Repeat values within each percentile group slice(rep(1:n(), each = households)) %>% rowwise() %>% arrange(`2011`) %>% # Create a sequence from 1 to 100 for percentiles mutate(percentile = seq(1:n())) %>% select(percentile, `2011`, `2012`, `2013`) # Check the first and last five percentiles to compare # with Figure 12.5. head(df.new, 5) ## # A tibble: 5 x 4 ## percentile `2011` `2012` `2013` ## <int> <dbl> <dbl> <dbl> ## 1 1 0 6000 0 ## 2 2 0 6000 0 ## 3 3 0 6000 0 ## 4 4 0 6000 0 ## 5 5 0 6000 0 tail(df.new, 5) ## # A tibble: 5 x 4 ## percentile `2011` `2012` `2013` ## <int> <dbl> <dbl> <dbl> ## 1 96 44516. 50544. 45270. ## 2 97 44516. 50544. 45270. ## 3 98 44516. 50544. 45270. ## 4 99 44516. 50544. 45270. ## 5 100 44516. 50544. 45270. Calculate Gini coefficients Using the ineq function from the ineq package, it is straightforward to obtain the Gini coefficient for each year. library(ineq) ineq(df.new$`2011`) ## [1] 0.4687603 ineq(df.new$`2012`) ## [1] 0.3440174 ineq(df.new$`2013`) ## [1] 0.4698187 In our analysis we assumed that the $6,000 handout was the only policy that affected households in 2012. In reality a household’s disposable income will also depend on taxes and transfers. Without doing additional calculations, explain what would happen to the shape of the Lorenz curve and inequality in 2012 if: households in and below the 15th percentile received cash transfers from the government households in and above the 75th percentile had to pay income tax. Extension Nominal and real values In this extension section, we will discuss how the table ‘Monthly nominal household income (pre-tax, $HKD)’ taken from the Hong Kong Poverty Situation Report 2016 was used to create the table ‘Monthly real household income (pre-tax, $HKD)’, and why we needed to make this conversion. inflationAn increase in the general price level in the economy. Usually measured over a year. See also: deflation, disinflation. The difference between real and nominal income is that real income takes inflation into account. You may be familiar with the concept of inflation, which is an increase in the general price level in the economy. Usually inflation is measured by taking a fixed bundle of goods and services and looking at how much it would cost to buy that bundle, compared to a reference year. (For more details about real vs nominal variables, see the Einstein ‘Comparing income at different times, and across different countries’ in Section 1.2 of The Economy.) If the bundle has become more expensive, then we conclude that the price level in the economy has increased. In this case, the values from 2010 onwards have been adjusted to account for the fact that prices have increased since 2009, so the same income would be able to purchase fewer goods and services. Without making this adjustment, we would conclude that households in the 15th percentile had the same purchasing power in 2009 and 2010, when in fact they do not as they can buy fewer goods in 2010 because of the overall price increase. Convert nominal values to real values, using 2009 as the reference year: To understand what happens to a given nominal value over time due to inflation, create a table as in Figure 12.6 below, and fill it in according to the percentage increases shown. (These percentages were taken from the Monthly Digest of Statistics.) (For example, $1 in 2009 would be $1 × (1 + (2.40/100)) = $1.024 in 2010.) For greater accuracy, round your answers to three decimal places. With a starting value of $1 in 2009, what would the value be in 2016? Year Percentage increase (from previous year) Inflation index 2009 1.000 2010 2.40 1.024 2011 5.30 2012 4.10 2013 4.30 2014 4.40 2015 3.00 2016 2.40 Creating an index-based series from percentage increases. Figure 12.6 Creating an index-based series from percentage increases. Use this table to convert nominal incomes to real incomes by dividing the nominal income by the corresponding value in the third column (for example, divide nominal incomes in 2010 by 1.024 to get the value in 2009 terms). You should get the same values as in the ‘real household income’ table. Extension R walk-through 12.4 Converting nominal incomes to real incomes To obtain the real income values, we need to divide the income for each percentile group by the inflation index created in Question 5(a). Recall that we have only imported the real income data from the Excel spreadsheet and not the nominal income data, so we will first import the nominal income data (nom_income). Note that in the code below, the inflation index is entered as a vector (list of numbers) called inflation (with the same number of elements as the number of years in the data) and this is multiplied (element-wise) by each row of the income data using the sweep function. The result is the dataframe called real_income. We use the kable function to make a neat table showing the values of this dataframe. # Import the nominal income data nom_income <- read_excel(''Project 12 datafile.xlsx'', range = ''A3:I8'') inflation <- c(1, 1.024, 1.078, 1.122, 1.171, 1.222, 1.259, 1.289) # Only multiply columns with income data by the inflation # index real_income <- sweep(nom_income[, -1], MARGIN = 2, inflation, ''/'') %>% # Reattach the percentile column cbind(percentile = c(85, 75, 50, 25, 15), .) kable(real_income, format = ''markdown'', digits = 2) | percentile| 2009| 2010| 2011| 2012| 2013| 2014| 2015| 2016| |----------:|-----:|-------:|--------:|--------:|--------:|--------:|--------:|--------:| | 85| 43300| 43945.31| 44515.67| 44563.28| 45260.46| 45171.85| 47656.87| 47090.77| | 75| 31000| 31250.00| 32273.86| 32531.19| 34158.84| 33306.06| 34789.52| 34910.78| | 50| 17400| 17578.12| 17806.27| 17825.31| 18616.57| 18494.27| 19062.75| 19394.88| | 25| 8000| 8203.12| 8346.69| 8823.53| 8539.71| 8592.47| 8737.09| 8533.75| | 15| 4500| 4394.53| 4637.05| 4456.33| 4355.25| 4091.65| 3971.41| 3878.98| Part 12.2 Government popularity Learning objectives for this part assess the effect of a policy on government popularity. One possible reason why the government implemented Scheme $6,000 was to gain public approval, since there was some pressure on the government to spend the surplus on alleviating current social issues rather than reinvesting it (for example, in pension schemes). We will use a public opinion poll conducted by the University of Hong Kong to assess whether this scheme could have improved public satisfaction with the government. Think about the groups who would be affected by this scheme (for example, the government or members of the public). Who would be likely to support or oppose this scheme, and why? Download the data: Go to the overall performance results page on the HKU POP website, which contains half-yearly survey data on the overall performance of the government. Under the subheading ‘Collapsed data’, import the data contained in this table using R. (The data is not available in Excel format, so you will have to import the data manually.) R walk-through 12.5 Importing data directly from a website It is quite common to use data from publicly-available websites that is presented in a table, but is not available in a spreadsheet to download. Although we could copy and paste this data into a spreadsheet and then import that into R, we could instead use the rvest package to read the data in such a table directly into a dataframe. The R-bloggers site provides details on how to do this in a guide to using rvest. We will store this table in a dataframe called overall. library(rvest) # When copying this code to R you need to ensure that the # url is presented in one line. url <- ''https://www.hkupop.hku.hk/english/popexpress/sargperf/sarg/halfyr/datatables.html'' overall <- url %>% html() %>% html_nodes(xpath = '//*[@id=''popexpress'']/table[2]') %>% html_table(header = TRUE) overall <- overall[[1]] Note that if the technicalities of the procedure described above are too complicated, you can still proceed by manually copying and pasting the data into a spreadsheet and then importing it into R. R will recognize the data you have imported as text, but to make charts, the data needs to be in number format (the % signs need to be removed). Follow the steps in R walk-through 12.6 below to reformat all columns. Read the HKU POP survey methods page for a description of how the survey data was collected. Explain whether you think the sample is representative of the target population, and discuss some limitations of the survey method. R walk-through 12.6 Cleaning imported data The names of each column from the imported data are long and contain a mixture of alphabet sets; we can rename the variables using the names property of a dataframe. Then we use the gsub function to find and replace all instances of the ‘%’ sign in the data with an empty string (''''), and use as.numeric to convert all variables (except the date variable) to be numeric variables (currently R thinks these are text instead of numbers because they had a ‘%’ sign when imported). names(overall) <- c(''Date'', ''Total Sample'', ''Sub-sample'', ''Positive'', ''Half-half'', ''Negative'', ''DKHS'', ''Total'', ''Net Value'') overall <- overall %>% mutate_all(funs(gsub(''%'', '''', .))) %>% mutate_at(vars(-Date), funs(as.numeric)) str(overall) ## 'data.frame': 43 obs. of 9 variables: ## $ Date : chr ''7-12/2018'' ''1-6/2018'' ''7-12/2017'' ''1-6/2017'' ... ## $ Total Sample: num 3025 12092 12201 12181 12074 ... ## $ Sub-sample : num 1761 7225 8750 7380 7167 ... ## $ Positive : num 36 35.7 39.6 28.5 24.3 25.1 24.2 27.1 27 27.6 ... ## $ Half-half : num 22.1 21.1 22.2 21.3 24.7 22.5 27.3 25.7 24.6 27.2 ... ## $ Negative : num 40.7 42.2 35.7 49.1 49.3 51.2 47.4 45.9 47.1 43.7 ... ## $ DKHS : num 1.1 1 2.5 1.1 1.7 1.2 1 1.2 1.3 1.5 ... ## $ Total : num 100 100 100 100 100 100 100 100 100 100 ... ## $ Net Value : num -4.7 -6.6 3.9 -20.7 -25 -26.1 -23.2 -18.8 -20 -16.1 ... Assess public satisfaction with the government by carrying out the following: Make a line chart with overall public satisfaction (net value, which is the difference between percentage of positive and negative responses) on the vertical axis, and time (Jan–June 2006 to the latest period available) on the horizontal axis. Comment on any trends in overall public satisfaction over this time period. Go to the POP polls main data page and choose one or two other indicators that are directly related to the policy (for example, improving people’s Degree of prosperity or Degree of equality). Find a table of the data by clicking ‘Content’, then ‘Table’ (if half-yearly data is not available, choose the most similar time interval). Import the data into R, and reformat the variable of interest as in Question 2. For each of your chosen indicators, make a separate line chart as in Question 3(a) and comment on any similarities to or differences from the chart in Question 3(a). (Since some indicators may be measured on a different scale, focus on changes over time.) Do you think the scheme had the intended effect on government popularity? Besides the scheme, what other factors or events could explain the observed patterns? R walk-through 12.7 Cleaning data and setting dates For Question 3(a), before we can plot the imported data in R, we need to format the data variable so that R understands the chronological order of the data. The imported data has the Date variable, which is a text string using ‘1-6’ to represent January to June and ‘7-12’ for July to December. This format is not useful for ordering the data because R cannot determine the sequential order of numbers that are stored as text. There are a number of ways that R can deal with times and dates. In this example we are going to use the yearmon function from the zoo library as this focuses on using monthly data. Although we don’t have monthly data, we can use the gsub function to replace the ‘1-6’ text string with ‘jan-’ and ‘7-12’ with ‘july-’, in other words we are going to use January and July to represent the first and second half of each year respectively. Once we have the months in a format that R can understand, we use the as.yearmon function to set the Date variable to the date format (using the %b-%Y option; further details on date formats in R can be found in the Dates and Times in R page). We can use the scale_x_yearmon function when plotting the line chart to ensure that the dates on the horizontal axis are labelled correctly. library(zoo) overall %>% mutate_at(vars(Date), funs( gsub(''1-6/'', ''jan-'', .))) %>% mutate_at(vars(Date), funs( gsub(''7-12/'', ''july-'', .))) %>% mutate(Date = as.yearmon(Date, ''%b-%Y'')) %>% subset(Date >= ''Jan 2006'') %>% ggplot(., aes(x = Date, y = `Net Value`)) + geom_line(size = 1) + theme_bw() + scale_x_yearmon(format = ''%b-%Y'') + ylab(''Net satisfaction'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Net public satisfaction with the government’s performance over time. '' /> Net public satisfaction with the government’s performance over time. Figure 12.7 Net public satisfaction with the government’s performance over time. For Question 3(b), in this example we use the variable Improving People's Livelihood. Repeating the import and cleaning processes from R walk-through 12.5, we can directly read in the relevant data. # When copying this code to R you need to ensure that the # url is presented in one line. url <- ''https://www.hkupop.hku.hk/english/popexpress/sargperf/live/halfyr/datatables.html'' improvement <- url %>% html() %>% html_nodes(xpath = '//*[@id=''popexpress'']/table[2]') %>% html_table(header = TRUE) improvement <- improvement [[1]] names(improvement) <- c(''Date'', ''Total Sample'', ''Sub-sample'', ''Positive'', ''Half-half'', ''Negative'', ''DKHS'', ''Total'', ''Net Value'') improvement <- improvement %>% mutate_all(funs(gsub(''%'', '''', .)))%>% mutate_at(vars(-Date), funs(as.numeric)) For Question 3(c), repeat the steps taken for 3(a). improvement %>% mutate_at(vars(Date), funs( gsub(''1-6/'', ''jan-'', .))) %>% mutate_at(vars(Date), funs( gsub(''7-12/'', ''july-'', .))) %>% mutate(Date = as.yearmon(Date, ''%b-%Y'')) %>% subset(Date >= ''Jan 2006'' ) %>% ggplot(., aes(x = Date, y = `Net Value`)) + geom_line(size = 1) + theme_bw() + scale_x_yearmon(format = ''%b-%Y'') + ylab(''Net satisfaction'') <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-08.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-08-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-08-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-08-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/r-figure-12-08.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Net public satisfaction with the government’s ability to improve people’s livelihood over time. '' /> Net public satisfaction with the government’s ability to improve people’s livelihood over time. Figure 12.8 Net public satisfaction with the government’s ability to improve people’s livelihood over time. In 2018, the government decided to do another cash handout. Read the article ‘Hong Kong cash handout scheme will cost government HK$330 million to administer’ and discuss how this scheme differs from the 2011 scheme. Explain whether you think this policy is an improvement over the 2011 scheme. Suppose you are a policymaker in a developed country with a large budget surplus, and one of the government’s aims is to reduce income inequality. Would you recommend that the government implement a scheme similar to either the 2011 or 2018 scheme? If you recommend a cash handout, suggest some modifications that could make the scheme more effective. If not, suggest other policies that may be more effective in reducing inequality. You may find it helpful to research policies aimed at reducing income inequality, for example Universal Basic Income, which some countries have tried.)"
});
index.addDoc({
    id: 65,
    title: "Doing Economics: Empirical Project 12: Working in Google Sheets",
    content: "Empirical Project 12 Working in Google Sheets Google Sheets-specific learning objectives In addition to the learning objectives for this project, in this section you will learn how to convert cells from text to number format. Part 12.1 Inequality Learning objectives for this part draw Lorenz curves and calculate Gini coefficients assess the effect of a policy on income inequality convert nominal values to real values (extension). One reason cited for Scheme $6,000 was to share the gains from economic growth among everyone in the society. We will be using household income data collected by the Hong Kong government to assess the potential effects of this scheme. Download the data. The spreadsheet contains information about household incomes for certain percentiles of the population. These incomes are ‘pre-intervention’, meaning that they do not include the effects of handouts or policy interventions from the government. (If you are curious about the difference between the two tables in this spreadsheet, see the extension section ‘Nominal and real values’ at the end of Part 12.1.) Using the table ‘Monthly real household income (pre-tax, $HKD)’, plot a separate line chart for each percentile, with year on the horizontal axis and income on the vertical axis. Describe any patterns you see over time. Now we will use this data to draw Lorenz curves and compare changes in the income distribution for 2011–2012. One way to do this is to make the following simplifying assumptions: There are 100 households in the economy (so we can think of each percentile as corresponding to one household). Households between the 15th and 25th percentile have the same income as the household in the 15th percentile, households between the 25th and 50th percentile have the same income as the household in the 25th percentile, and so on. (Households below the 15th percentile earn nothing.) Draw Lorenz curves for 2011 and 2012 by carrying out the following: Create a new column for 2012 only, showing incomes following the $6,000 handout. Assume that nothing has changed except for the cash handout. (Remember that this amount was given to all households, including those with no income.) Calculate the economy-wide earnings in 2011 and 2012 (with the $6,000 cash handout included). (Hint: Multiply the income of a given percentile by the number of households assumed to earn that amount.) Use your answer to Question 2(b) to complete the table in Figure 12.1 below. (The second row also shows zeros in 2011 because the bottom 15% of households earned nothing.) (For help on how to calculate cumulative shares, see Google Sheets walk-through 5.1). Cumulative share of the population (%) Perfect equality line Cumulative share of income in 2011 (%) Cumulative share of income in 2012 (%) 0 0 0 0 15 15 0 25 25 50 50 75 75 85 85 100 100 100 100 Cumulative share of income, for some percentiles of the population. Figure 12.1 Cumulative share of income, for some percentiles of the population. Draw the Lorenz curves for 2011 and 2012 in the same chart, with cumulative share of population on the horizontal axis and cumulative share of income on the vertical axis. In order for the chart to be drawn to scale, use the scatterplot option ‘Scatter with Straight Lines and Markers’ instead of the usual line chart option. Add the perfect equality line to your chart, and add a chart legend. Now we will compare the Gini coefficients in 2011, 2012 (with the cash handout), and 2013 (a year after the policy came into effect). Instead of calculating the coefficients manually, we will use an online Gini coefficient calculator, which takes a list of values and calculates the Gini coefficient. Calculate the Gini coefficient by carrying out the following: Create a table as in Figure 12.2, and fill in the remaining values (some values for 2011 have been filled in for you). The first column should contain the numbers 1 to 100, in intervals of 1, and the remaining columns should contain the incomes earned by a household at that percentile (using the same assumption as in Question 2). Remember that the 2012 data should include the cash handout. To obtain more accurate answers, use two decimal places instead of rounding incomes to the nearest dollar. Percentile 2011 2012 2013 1 0.00 2 0.00 3 0.00 … 98 44,516.67 99 44,516.67 100 44,516.67 Incomes earned by each percentile of the population. Figure 12.2 Incomes earned by each percentile of the population. Create a new column for each year, containing the incomes with a comma added at the end (e.g. ‘40,000’ would be ‘40,000,’). Then, for each year, copy and paste the income values for all percentiles into the Gini coefficient calculator to obtain the Gini coefficient for that year. Hint: The CONCATENATE function can combine the contents of cells and/or text together. The formula =CONCATENATE(cell1, “,”) adds a comma after the contents of cell1. Based on the Gini coefficients from Question 3(b), what effect did the $6,000 handout appear to have on income inequality in the short run, and in the long run? Suggest some explanations for what you observe. In our analysis we assumed that the $6,000 handout was the only policy that affected households in 2012. In reality a household’s disposable income will also depend on taxes and transfers. Without doing additional calculations, explain what would happen to the shape of the Lorenz curve and inequality in 2012 if: households in and below the 15th percentile received cash transfers from the government households in and above the 75th percentile had to pay income tax. Extension Nominal and real values In this extension section, we will discuss how the table ‘Monthly nominal household income (pre-tax, $HKD)’ taken from the ‘Hong Kong poverty situation report 2016’ was used to create the table ‘Monthly real household income (pre-tax, $HKD)’, and why we needed to make this conversion. inflationAn increase in the general price level in the economy. Usually measured over a year. See also: deflation, disinflation. The difference between real and nominal income is that real income takes inflation into account. You may be familiar with the concept of inflation, which is an increase in the general price level in the economy. Usually inflation is measured by taking a fixed bundle of goods and services and looking at how much it would cost to buy that bundle, compared to a reference year. (For more details about real vs nominal variables, see the Einstein ‘Comparing income at different times, and across different countries’ in Section 1.2 of The Economy). If the bundle of goods and services has become more expensive, then we conclude that the price level in the economy has increased. In this case, the values from 2010 onwards have been adjusted to account for the fact that prices have increased since 2009, so the same income would be able to purchase fewer goods and services. Without making this adjustment, we would conclude that households in the 15th percentile had the same purchasing power in 2009 and 2010, when in fact they do not as they can buy fewer goods and services in 2010 because of the overall price increase. Convert nominal values to real values, using 2009 as the reference year: To understand what happens to a given nominal value over time due to inflation, create a table as in Figure 12.3 below, and fill it in according to the percentage increases shown. (These percentages were taken from the Monthly Digest of Statistics.) (For example, $1 in 2009 would be $1 × (1 + (2.40/100)) = $1.024 in 2010.) For greater accuracy, round your answers to three decimal places. With a starting value of $1 in 2009, what would the value be in 2016? Year Percentage increase (from previous year) Inflation index 2009 1.000 2010 2.40 1.024 2011 5.30 2012 4.10 2013 4.30 2014 4.40 2015 3.00 2016 2.40 Creating an index-based series from percentage increases. Figure 12.3 Creating an index-based series from percentage increases. Use this table to convert nominal incomes to real incomes by dividing the nominal income by the corresponding value in the third column (for example, divide nominal incomes in 2010 by 1.024 to get the value in 2009 terms). You should get the same values as in the ‘real household income’ table. Part 12.2 Government popularity Learning objectives for this part assess the effect of a policy on government popularity. One possible reason why the government implemented Scheme $6,000 was to gain public approval, since there was some pressure on the government to spend the surplus on alleviating current social issues rather than reinvesting it (for example, in pension schemes). We will use a public opinion poll conducted by the University of Hong Kong to assess whether this scheme could have improved public satisfaction with the government. Think about the groups who would be affected by this scheme (for example, the government or members of the public). Who would be likely to support or oppose this scheme, and why? Download the data: Go to the overall performance results page on the HKU POP website, which contains half-yearly survey data on the overall performance of the government. Under the subheading ‘Collapsed data’, copy and paste the entire table directly into a new tab in Google Sheets. (The data is not available in Google Sheets format, so you will have to resort to this method.) Google Sheets will recognize the data you have pasted as text, but to make charts, the data needs to be in number format. Follow the steps in Google Sheets walk-through 12.1 below to reformat the column called ‘Net Value’. Read the HKU POP survey methods page for a description of how the survey data was collected. Explain whether you think the sample is representative of the target population, and discuss some limitations of the survey method. Google Sheets walk-through 12.1 Using SUBSTITUTE to clean text in cells <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''How to use SUBSTITUTE to clean text in cells by removing spaces before and after the text. '' /> How to use SUBSTITUTE to clean text in cells by removing spaces before and after the text. Figure 12.4 How to use SUBSTITUTE to clean text in cells by removing spaces before and after the text. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-a.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-a-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-a-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-a-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-a.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''The data : We will use the data in Column I as an example. '' /> The data We will use the data in Column I as an example. Figure 12.4a We will use the data in Column I as an example. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-b.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-b-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-b-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-b-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-b.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Numbers stored as text : Google Sheets currently recognizes the values in Column I as text rather than numbers, so when you try to plot a line chart, it will come out like this, with no actual data being plotted. '' /> Numbers stored as text Google Sheets currently recognizes the values in Column I as text rather than numbers, so when you try to plot a line chart, it will come out like this, with no actual data being plotted. Figure 12.4b Google Sheets currently recognizes the values in Column I as text rather than numbers, so when you try to plot a line chart, it will come out like this, with no actual data being plotted. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-c.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-c-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-c-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-c-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-c.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Remove unnecessary spaces : Google Sheets thinks the numbers are text because there is a space before and after the number. To remove the spaces, we use the SUBSTITUTE function (which removes non-breaking spaces i.e. spaces that do not result in a new line), then the TRIM function (which removes spaces before and after text). '' /> Remove unnecessary spaces Google Sheets thinks the numbers are text because there is a space before and after the number. To remove the spaces, we use the SUBSTITUTE function (which removes non-breaking spaces i.e. spaces that do not result in a new line), then the TRIM function (which removes spaces before and after text). Figure 12.4c Google Sheets thinks the numbers are text because there is a space before and after the number. To remove the spaces, we use the SUBSTITUTE function (which removes non-breaking spaces i.e. spaces that do not result in a new line), then the TRIM function (which removes spaces before and after text). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-d.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-d-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-d-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-d-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-d.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Copy and paste values into a new column : The numbers in Column J are still stored as text, but we cannot convert Column J to numbers because there is a formula in those cells. Instead, we need to copy and paste (values only) into a new column, then convert the pasted data to numbers. '' /> Copy and paste values into a new column The numbers in Column J are still stored as text, but we cannot convert Column J to numbers because there is a formula in those cells. Instead, we need to copy and paste (values only) into a new column, then convert the pasted data to numbers. Figure 12.4d The numbers in Column J are still stored as text, but we cannot convert Column J to numbers because there is a formula in those cells. Instead, we need to copy and paste (values only) into a new column, then convert the pasted data to numbers. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-e.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-e-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-e-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-e-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-e.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Convert text to numbers : Now Google Sheets will recognize that numbers are stored as text, and will give you the option to convert them to numbers. '' /> Convert text to numbers Now Google Sheets will recognize that numbers are stored as text, and will give you the option to convert them to numbers. Figure 12.4e Now Google Sheets will recognize that numbers are stored as text, and will give you the option to convert them to numbers. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-f.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-f-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-f-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-f-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/gs-figure-12-04-f.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Numbers correctly stored in Google Sheets : Now Google Sheets recognizes the values in Column K as numbers, so you can use them to plot a line chart. '' /> Numbers correctly stored in Google Sheets Now Google Sheets recognizes the values in Column K as numbers, so you can use them to plot a line chart. Figure 12.4f Now Google Sheets recognizes the values in Column K as numbers, so you can use them to plot a line chart. Assess public satisfaction with the government by carrying out the following: Make a line chart with overall public satisfaction (net value, which is the difference between percentage of positive and negative responses) on the vertical axis, and time (Jan–June 2006 to the latest period available) on the horizontal axis. Remember to sort your data in ascending date order (earliest to latest) before making your chart (for help, see Google Sheets walk-through 5.3). Comment on any trends in overall public satisfaction over this time period. Go to the POP polls main data page and choose one or two other indicators that are directly related to the policy (for example, improving people’s ‘Degree of prosperity’ or ‘Degree of equality’). Find a table of the data by clicking ‘Content’, then ‘Table’ (if half-yearly data is not available, choose the most similar time interval). Copy and paste the data into a new tab, and reformat the variable of interest as in Question 2. For each of your chosen indicators, make a separate line chart as in Question 3(a) and comment on any similarities to or differences from the chart in Question 3(a). (Since some indicators may be measured on a different scale, focus on changes over time.) Do you think the scheme had the intended effect on government popularity? Besides the scheme, what other factors or events could explain the observed patterns? In 2018, the government decided to do another cash handout. Read the article ‘Hong Kong cash handout scheme will cost government HK$330 million to administer’ and discuss how this scheme differs from the 2011 scheme. Explain whether you think this policy is an improvement over the 2011 scheme. Suppose you are a policymaker in a developed country with a large budget surplus, and one of the government’s aims is to reduce income inequality. Would you recommend that the government implement a scheme similar to either the 2011 or 2018 scheme? If you recommend a cash handout, suggest some modifications that could make the scheme more effective. If not, suggest other policies that may be more effective in reducing inequality. (You may find it helpful to research policies aimed at reducing income inequality, for example Universal Basic Income, which some countries have tried.)"
});
index.addDoc({
    id: 66,
    title: "Doing Economics: Empirical Project 12 Solutions",
    content: "Empirical project 12 Solutions These are not model answers. They are provided to help students, including those doing the project outside a formal class, to check their progress while working through the questions using the Excel, R, or Google Sheets walk-throughs. There are also brief notes for the more interpretive questions. Students taking courses using Doing Economics should follow the guidance of their instructors. Part 12.1 Inequality Separate line charts for each percentile are provided in Solution figures 12.1 to 12.5. Real incomes have generally increased over time for all percentiles except the 15th percentile. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-01.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-01-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-01-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-01-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-01.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''15th percentile of incomes. '' /> 15th percentile of incomes. Solution figure 12.1 15th percentile of incomes. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-02.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-02-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-02-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-02-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-02.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''25th percentile of incomes. '' /> 25th percentile of incomes. Solution figure 12.2 25th percentile of incomes. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-03.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-03-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-03-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-03-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-03.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''50th percentile of incomes. '' /> 50th percentile of incomes. Solution figure 12.3 50th percentile of incomes. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-04.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-04-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-04-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-04-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-04.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''75th percentile of incomes. '' /> 75th percentile of incomes. Solution figure 12.4 75th percentile of incomes. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-05.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-05-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-05-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-05-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-05.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''85th percentile of incomes. '' /> 85th percentile of incomes. Solution figure 12.5 85th percentile of incomes. No solution is provided. Assuming 100 households, the economy-wide income is $1,690,668 in 2011 and $2,203,815 in 2012. Values are rounded to two decimal places, and shown in Solution figure 12.6. Cumulative share of the population (%) Perfect equality line Cumulative share of income in 2011 (%) Cumulative share of income in 2012 (%) 0.00 0.00 0.00 0.00 15.00 15.00 0.00 3.91 25.00 25.00 2.74 8.44 50.00 50.00 15.09 24.53 75.00 75.00 41.42 50.37 85.00 85.00 60.50 67.09 100.00 100.00 100.00 100.00 Cumulative share of income, for some percentiles of the population. Solution figure 12.6 Cumulative share of income, for some percentiles of the population. The Lorenz curves are shown in Solution figure 12.7. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-07.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-07-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-07-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-07-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-07.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Lorenz curves for 2011 and 2012. '' /> Lorenz curves for 2011 and 2012. Solution figure 12.7 Lorenz curves for 2011 and 2012. The completed table is too large to provide here, but some values for 2012 and 2013 are provided in Solution figure 12.8. Percentile 2011 2012 2013 1 0.00 6,000.00 0.00 2 0.00 6,000.00 0.00 3 0.00 6,000.00 0.00 … 98 44,516.67 50,544.18 45,270.21 99 44,516.67 50,544.18 45,270.21 100 44,516.67 50,544.18 45,270.21 Incomes earned by each percentile of the population. Solution figure 12.8 Incomes earned by each percentile of the population. The Gini coefficient is 0.47 in 2011, 0.34 in 2012, and 0.46 in 2013. The $6,000 handout had a noticeable impact on income inequality in 2012, but these impacts did not last until 2013. One reason that the scheme only had a one-year effect on income inequality after 2012 is that a one-off lump-sum payment did not address the root causes of income inequality (for example, differences in education or qualifications, which determine the job opportunities and wages available to households, or institutions that prevent social mobility). The fact that it was a one-off handout meant that low income households, who are typically credit constrained and unable to borrow were likely to spend it on consumption. The amount was not large enough to pay for training or qualifications or to allow a household to set up a small business. It is also unlikely that the changes in household income in 2012 were entirely due to the scheme. The incomes of the richer households may have increased more than those of the poorer households, for example due to receiving larger end-of-year bonuses or receiving tax rebates, which could outweigh the $6,000 given in the scheme. If this were the case, the fall in the Gini coefficient would have been moderated by these effects. If households currently receiving zero income received additional cash transfers from the government, their cumulative share of income would increase, so the Lorenz curve at that part would move closer to the perfect equality line. Inequality (and the Gini coefficient) would decrease. If the richer households had to pay income tax, their cumulative share of income would decrease, so the Lorenz curve at that part would move closer to the perfect equality line. Inequality (and the Gini coefficient) would decrease. Solution figure 12.9 shows the percentage increases over time. Values are rounded to three decimal places. For the percentage changes in the price level in each year given, an index set at 1 in 2009 would have increased to 1.289, an increase of 28.9%. Year Percentage increase (from previous year) Inflation index 2009 1.000 2010 2.40 1.024 2011 5.30 1.078 2012 4.10 1.122 2013 4.30 1.171 2014 4.40 1.222 2015 3.00 1.259 2016 2.40 1.289 Creating an index-based series from percentage increases. Solution figure 12.9 Creating an index-based series from percentage increases. No solution is provided. Part 12.2 Government popularity Supporters of the scheme could include: citizens who receive the payment, especially the lower-income households, for whom $6,000 would be a substantial increase in annual income government officials and politicians, since the scheme is a visible way to address inequality in society, which could help boost their popularity and help to get them re-elected social welfare groups or related organizations, who could view this scheme as one step towards reducing inequality. Opponents of the scheme could include: lower-income people, who may believe that a pay-out independent of need is unfair wealthier or more powerful members of society, such as businesspeople, who may believe they made a larger contribution to economic growth and may therefore not perceive an equal split as being fair policymakers, who may believe that other programs or policies could be more effective at addressing inequality than a one-off lump-sum payment (for example, additional subsidies for lower-income households, or transfers to lower-income households only). No solution is provided. Some measures were taken to ensure the sample was representative, for example: The sample is selected according to the gender–age distribution of the population of interest. Telephone numbers were randomly generated. Limitations of the survey method include: The website does not specify whether landline numbers or mobile phone numbers are used (or both). Certain age and income groups are more likely to have and use a mobile phone (likewise for landlines), which affects the likelihood of being successfully contacted. Certain groups of people are more likely to have the time to complete the survey. Only Cantonese people over the age of 18 were surveyed, so this survey does not capture the opinions of other groups of the population (e.g. immigrants who cannot speak Cantonese). The line chart showing overall public satisfaction is provided in Solution figure 12.10. Overall public satisfaction was initially high from 2006 to the first half of 2008, but dropped in the second half of 2008 and remained low throughout 2009 (which may have prompted the government to take measures to improve their popularity). Overall public satisfaction dropped further from 2010 to 2012, remaining at negative values until the second half of 2017. <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-10.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-10-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-10-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-10-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-10.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Overall satisfaction with the government (2006–2017). '' /> Overall satisfaction with the government (2006–2017). Solution figure 12.10 Overall satisfaction with the government (2006–2017). No solution is provided. Improvement of people’s prosperity is used as an example in Solution figure 12.11. The general pattern is similar to that of overall satisfaction with the government, with net satisfaction generally decreasing until the second half of 2012, and again in the second half of 2017. The increase in the second half of 2012 is larger, which may be expected since the policy is more directly targeted at this aspect of governance (though net satisfaction is still negative). <img src=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-11.jpg'' srcset=''https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-11-320.jpg 320w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-11-640.jpg 640w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-11-1024.jpg 1024w, https://electricbookworks.github.io/empirical-projects-media/book/images/web/solution-figure-12-11.jpg 1280w'' sizes=''(min-width: 600px) 1300px, 100vw'' alt=''Satisfaction with government’s improvement of people’s prosperity (2006–2017). '' /> Satisfaction with government’s improvement of people’s prosperity (2006–2017). Solution figure 12.11 Satisfaction with government’s improvement of people’s prosperity (2006–2017). The scheme did not seem to improve government popularity. When the scheme was announced in 2011, popularity dropped even further, and only rose slightly in the second half of 2012 when the scheme’s implementation had finished. Satisfaction remained at negative net values until the second half of 2017. However, we cannot simply conclude that the scheme ‘caused’ the drop in government popularity. Other events, such as political scandals, could undermine public support for the government. For example, in 2015, the government was criticized for its handling of the widespread contamination of drinking water. The government could have pursued other policies that were unpopular with the public, for example lowering the minimum wage, or entering a trade agreement that would disadvantage its citizens. It is also possible for popularity to drop due to unfavourable statistical releases, such as bad economic performance data or high unemployment rates. There are many possible points to discuss, including: Compared to the 2011 scheme, this scheme is targeted more towards needy households (for example, those who are not homeowners or are not earning enough to pay income tax). Another difference is that the amount received depends on need rather than being the same amount for all eligible households. Administrative costs are higher than the 2011 scheme (possibly because of the need to verify and calculate the amount given to each eligible household), and the maximum amount distributed is smaller. An example of an argument for recommending the scheme is provided: The scheme is a visible and tangible way to reduce income inequality, and giving everyone the same lump-sum amount could be seen as being ‘fair’, which could improve public satisfaction with the government. However, when the scheme was actually implemented, there were huge administrative costs involved with registering citizens and arranging the payment. To reduce the administrative burden, instead of doing a direct transfer, the payment could take the form of a tax rebate (for taxpayers) and an increase in existing transfers or subsidies (for non-taxpayers). Also, the payment could be adjusted for the number of dependents that the adult has to support: even though the scheme seemed fair since every adult received the same amount, households with more children (who have greater financial needs) received the same amount as married couples without children. An example of an argument for not recommending the scheme is provided: The scheme does not address the root causes of income inequality, so would not affect inequality in the long run. It might be more effective to spend the surplus on providing educational subsidies for low-income households, scholarships for students from low-income households to study at university, or retraining programs for the unemployed."
});
index.addDoc({
    id: 67,
    title: "Doing Economics: Excel and R usage",
    content: "Technical reference Excel versions The Excel walk-throughs have been created using Excel version 16.16.8 on Windows. Some of the Excel instructions will vary for Mac or Linux users. For more on using Excel see the official Excel help centre. R versions The R walk-throughs have been created using R version 3.5.0 and RStudio version 1.1.453 on Windows. If you are inexperienced in the use of R, we recommend the free Beginner’s guide to R by Computerworld. This course also describes the main components of the RStudio user interface. For R support more specific to economists, you may want to consult the pages on the ECLR wiki. On the CRAN Task Views page you can find links to useful packages sorted by topic. Google Sheets versions The Google Sheets walk-throughs have been created using Google Sheets on Windows, prior to the Google Sheets interface update in early 2019. There are no functionality changes, so the instructions are still valid, but there may be some visual differences between the walk-through images and the current Google Sheets interface. For more on using Google Sheets, see the official G Suite Learning Center. Character encoding in R on Windows Windows machines use a limited character encoding set by default, which means you might see some symbols as ‘???’. To resolve this, open the file in which you see the unusual symbols in RStudio, select ‘File’ > ‘Reopen with encoding…’ > ‘UTF-8’, and select ‘Set as default encoding for source files’, which will prevent this from happening in the future in RStudio. Creating a folder on Mac To create a folder on Mac, on your desktop or in Finder, use Shift-Command-N. Finding your path on Mac Right-click on the file or folder for which you want to find the path and select ‘Get info’. You can copy the file path from the ‘Where’ section of the info box that appears. Finding your path on Windows Right-click on the file or folder for which you want to find the path and select ‘Properties’. You can copy the file path from the ‘Location’ section of the ‘General’ tab of the properties box that appears."
});
index.addDoc({
    id: 68,
    title: "Doing Economics: Glossary",
    content: "Glossary box and whisker plotA graphic display of the range and quartiles of a distribution, where the first and third quartile form the ‘box’ and the maximum and minimum values form the ‘whiskers’.causationA direction from cause to effect, establishing that a change in one variable produces a change in another. While a correlation gives an indication of whether two variables move together (either in the same or opposite directions), causation means that there is a mechanism that explains this association. Example: We know that higher levels of CO2 in the atmosphere lead to a greenhouse effect, which warms the Earth’s surface. Therefore we can say that higher CO2 levels are the cause of higher surface temperatures.conditional meanAn average of a variable, taken over a subgroup of observations that satisfy certain conditions, rather than all observations.confidence intervalA range of values that is centred around the sample value, and is defined so that there is a specified probability (usually 95%) that it contains the ‘true value’ of interest.contingent valuationA survey-based technique used to assess the value of non-market resources. Also known as: stated-preference model.correlation coefficientA numerical measure, ranging between 1 and −1, of how closely associated two variables are—whether they tend to rise and fall together, or move in opposite directions. A positive coefficient indicates that when one variable takes a high (low) value, the other tends to be high (low) too, and a negative coefficient indicates that when one variable is high the other is likely to be low. A value of 1 or −1 indicates that knowing the value of one of the variables would allow you to perfectly predict the value of the other. A value of 0 indicates that knowing one of the variables provides no information about the value of the other.correlationA measure of how closely related two variables are. Two variables are correlated if knowing the value of one variable provides information on the likely value of the other, for example high values of one variable being commonly observed along with high values of the other variable. Correlation can be positive or negative. It is negative when high values of one variable are observed with low values of the other. Correlation does not mean that there is a causal relationship between the variables. Example: When the weather is hotter, purchases of ice cream are higher. Temperature and ice cream sales are positively correlated. On the other hand, if purchases of hot beverages decrease when the weather is hotter, we say that temperature and hot beverage sales are negatively correlated.credit constrainedA description of individuals who are able to borrow only on unfavourable terms. See also: credit excluded.credit excludedA description of individuals who are unable to borrow on any terms. See also: credit constrained.Cronbach’s alphaA measure used to assess the extent to which a set of items is a reliable or consistent measure of a concept. This measure ranges from 0–1, with 0 meaning that all of the items are independent of one another, and 1 meaning that all of the items are perfectly correlated with each other.cross-sectional dataData that is collected from participants at one point in time or within a relatively short time frame. In contrast, time series data refers to data collected by following an individual (or firm, country, etc.) over a course of time. Example: Data on degree courses taken by all the students in a particular university in 2016 is considered cross-sectional data. In contrast, data on degree courses taken by all students in a particular university from 1990 to 2016 is considered time series data.decileA subset of observations, formed by ordering the full set of observations according to the values of a particular variable and then splitting the set into ten equally-sized groups. For example, the 1st decile refers to the smallest 10% of values in a set of observations. See also: percentile.deflationA decrease in the general price level. See also: inflation.differences-in-differencesA method that applies an experimental research design to outcomes observed in a natural experiment. It involves comparing the difference in the average outcomes of two groups, a treatment and control group, both before and after the treatment took place.disinflationA decrease in the rate of inflation. See also: inflation, deflation.dummy variable (indicator variable)A variable that takes the value 1 if a certain condition is met, and 0 otherwise.endogenousProduced by the workings of a model rather than coming from outside the model. See also: exogenousexogenousComing from outside the model rather than being produced by the workings of the model itself. See also: endogenous.frequency tableA record of how many observations in a dataset have a particular value, range of values, or belong to a particular category.geometric meanA summary measure calculated by multiplying N numbers together and then taking the Nth root of this product. The geometric mean is useful when the items being averaged have different scoring indices or scales, because it is not sensitive to these differences, unlike the arithmetic mean. For example, if education ranged from 0 to 20 years and life expectancy ranged from 0 to 85 years, life expectancy would have a bigger influence on the HDI than education if we used the arithmetic mean rather than the geometric mean. Conversely, the geometric mean treats each criteria equally. Example: Suppose we use life expectancy and mean years of schooling to construct an index of wellbeing. Country A has life expectancy of 40 years and a mean of 6 years of schooling. If we used the arithmetic mean to make an index, we would get (40 + 6)/2 = 23. If we used the geometric mean, we would get (40 × 6)1/2 = 15.5. Now suppose life expectancy doubled to 80 years. The arithmetic mean would be (80 + 6)/2 = 43, and the geometric mean would be (80 × 6)1/2 = 21.9. If, instead, mean years of schooling doubled to 12 years, the arithmetic mean would be (40 + 12)/2 = 26, and the geometric mean would be (40 × 12)1/2 = 21.9. This example shows that the arithmetic mean can be ‘unfair’ because proportional changes in one variable (life expectancy) have a larger influence over the index than changes in the other variable (years of schooling). The geometric mean gives each variable the same influence over the value of the index, so doubling the value of one variable would have the same effect on the index as doubling the value of another variable.Gini coefficientA measure of inequality of any quantity such as income or wealth, varying from a value of zero (if there is no inequality) to one (if a single individual receives all of it).hypothesis testA test in which a null (default) and an alternative hypothesis are posed about some characteristic of the population. Sample data is then used to test how likely it is that these sample data would be seen if the null hypothesis was true.incomplete contractA contract that does not specify, in an enforceable way, every aspect of the exchange that affects the interests of parties to the exchange (or of others).indexAn index is formed by aggregating the values of multiple items into a single value, and is used as a summary measure of an item of interest. Example: The HDI is a summary measure of wellbeing, and is calculated by aggregating the values for life expectancy, expected years of schooling, mean years of schooling, and gross national income per capita.inflationAn increase in the general price level in the economy. Usually measured over a year. See also: deflation, disinflation.leverage ratio (for banks or households)The value of assets divided by the equity stake (capital contributed by owners and shareholders) in those assets.leverage ratio (for non-bank companies)The value of total liabilities divided by total assets.Likert scaleA numerical scale (usually ranging from 1–5 or 1–7) used to measure attitudes or opinions, with each number representing the individual’s level of agreement or disagreement with a particular statement.logarithmic scaleA way of measuring a quantity based on the logarithm function, f(x) = log(x). The logarithm function converts a ratio to a difference: log (a/b) = log a – log b. This is very useful for working with growth rates. For instance, if national income doubles from 50 to 100 in a poor country and from 1,000 to 2,000 in a rich country, the absolute difference in the first case is 50 and in the second 1,000, but log(100) – log(50) = 0.693, and log(2,000) – log(1,000) = 0.693. The ratio in each case is 2 and log(2) = 0.693.Lorenz curveA graphical representation of inequality of some quantity such as wealth or income. Individuals are arranged in ascending order by how much of this quantity they have, and the cumulative share of the total is then plotted against the cumulative share of the population. For complete equality of income, for example, it would be a straight line with a slope of one. The extent to which the curve falls below this perfect equality line is a measure of inequality. See also: Gini coefficient.meanA summary statistic for a set of observations, calculated by adding all values in the set and dividing by the number of observations.medianThe middle number in a set of values, such that half of the numbers are larger than the median and half are smaller. Also known as: 50th percentile.natural experimentAn empirical study exploiting naturally occurring statistical controls in which researchers do not have the ability to assign participants to treatment and control groups, as is the case in conventional experiments. Instead, differences in law, policy, weather, or other events can offer the opportunity to analyse populations as if they had been part of an experiment. The validity of such studies depends on the premise that the assignment of subjects to the naturally occurring treatment and control groups can be plausibly argued to be random.natural logarithm See: logarithmic scale.nominal wageThe actual amount received in payment for work, in a particular currency. See also: real wage.p-valueThe probability of observing the data collected, assuming that any differences observed between the two groups of interest have happened by chance. The p-value ranges from 0 to 1, where lower values indicate a higher probability that the underlying assumption (differences observed have happened by chance) is false. The lower the probability (the lower the p-value), the less likely it is to observe the given data, and therefore the more likely it is that the assumption is false (the observed differences are unlikely to have happened by chance).percentileA subset of observations, formed by ordering the full set of observations according to the values of a particular variable and then splitting the set into ten equally-sized groups. For example, the 1st percentile refers to the smallest 1% of values in a set of observations. See also: decile.principal–agent relationshipThis is an asymmetrical relationship in which one party (the principal) benefits from some action or attribute of the other party (the agent) about which the principal’s information is not sufficient to enforce in a complete contract. See also: incomplete contract. Also known as: principal–agent problem.rangeThe interval formed by the smallest (minimum) and the largest (maximum) value of a particular variable. The range shows the two most extreme values in the distribution, and can be used to check whether there are any outliers in the data. (Outliers are a few observations in the data that are very different from the rest of the observations.)real wageThe nominal wage, adjusted to take account of changes in prices between different time periods. It measures the amount of goods and services the worker can buy. See also: nominal wage.selection biasAn issue that occurs when the sample or data observed is not representative of the population of interest. For example, individuals with certain characteristics may be more likely to be part of the sample observed (such as students being more likely than CEOs to participate in computer lab experiments).significance levelA cut-off probability that determines whether a p-value is considered statistically significant. If a p-value is smaller than the significance level, it is considered unlikely that the differences observed are due to chance, given the assumptions made about the variables (for example, having the same mean). Common significance levels are 1% (p-value of 0.01), 5% (p-value of 0.05), and 10% (p-value of 0.1). See also: statistically significant, p-value.simultaneityWhen the right-hand and left-hand variables in a model equation affect each other at the same time, so that the direction of causality runs both ways. For example, in supply and demand models, the market price affects the quantity supplied and demanded, but quantity supplied and demanded can in turn affect the market price.spurious correlationA strong linear association between two variables that does not result from any direct relationship, but instead may be due to coincidence or to another unseen factor.standard deviationA measure of dispersion in a frequency distribution, equal to the square root of the variance. The standard deviation has a similar interpretation to the variance. A larger standard deviation means that the data is more spread out. Example: The set of numbers 1, 1, 1 has a standard deviation of zero (no variation or spread), while the set of numbers 1, 1, 999 has a standard deviation of 46.7 (large spread).standard errorA measure of the degree to which the sample mean deviates from the population mean. It is calculated by dividing the standard deviation of the sample by the square root of the number of observations.statistically significantWhen a relationship between two or more variables is unlikely to be due to chance, given the assumptions made about the variables (for example, having the same mean). Statistical significance does not tell us whether there is a causal link between the variables.time series dataA time series is a set of time-ordered observations of a variable taken at successive, in most cases regular, periods or points of time. Example: The population of a particular country in the years 1990, 1991, 1992, … , 2015 is time series data.varianceA measure of dispersion in a frequency distribution, equal to the mean of the squares of the deviations from the arithmetic mean of the distribution. The variance is used to indicate how ‘spread out’ the data is. A higher variance means that the data is more spread out. Example: The set of numbers 1, 1, 1 has zero variance (no variation), while the set of numbers 1, 1, 999 has a high variance of 221,334 (large spread).weighted averageA type of average that assigns greater importance (weight) to some components than to others, in contrast with a simple average, which weights each component equally. Components with a larger weight can have a larger influence on the average."
});
index.addDoc({
    id: 69,
    title: "Doing Economics: Bibliography",
    content: "Bibliography Empirical Project 1 National Aeronautics and Space Administration Goddard Institute for Space Studies. ‘GISS surface temperature analysis frequently asked questions (FAQ)’. Accessed 8 August 2018. Popovich, Nadja, and Adam Pearce. 2017. ‘It’s not your imagination. Summers are getting hotter.’. The New York Times. Published 28 July 2017. Tans, Pieter, and Kirk Thoning. 2018. ‘How we measure background CO2 levels on Mauna Loa.’. NOAA ESRL Global Monitoring Division. Published March 2018. Empirical Project 2 Herrmann, Benedikt, Christian Thöni, and Simon Gächter. 2008. ‘Antisocial punishment across societies’. Science Magazine 319 (5868): pp. 1362–1367. Empirical Project 3 Silver, Lynn D., Shu Wen Ng, Suzanne Ryan-Ibarra, Lindsey Smith Taillie, Marta Induni, Donna R. Miles, Jennifer M. Poti, and Barry M. Popkin. 2017. ‘Changes in prices, sales, consumer spending, and beverage consumption one year after a tax on sugar-sweetened beverages in Berkeley, California, US: A before-and-after study’. PLoS Med 14 (4): e1002283. Empirical Project 4 Fox, Justin. 2012. ‘The economics of well-being’. Harvard Business Review January–February 2012. OECD. 2016. ‘Statistical insights: what does GDP per capita tell us about households’ material well-being?’. Published 6 October 2016. United Nations Development Programme. 2016. Technical notes: calculating the human development indices—graphical presentation. Empirical Project 5 Peltzman, Sam. 2009. ‘Mortality inequality’. Journal of Economic Perspectives 23 (4): pp. 175–90. Empirical Project 6 Bloom, Nicholas, Benn Eifert, Aprajit Mahajan, David McKenzie, and John Roberts. 2013. ‘Does management matter? Evidence from India’. The Quarterly Journal of Economics 128 (1): pp. 1–51. Bloom, Nicholas, Christos Genakos, Raffaella Sadun, and John Van Reenen. 2012. ‘Management practices across firms and countries’. The Academy of Management Perspectives 26 (1): pp. 12–33. University of Manchester’s Econometric Computing Learning Resource (ECLR). 2018. ‘R AnalysisTidy’. Updated 9 January 2018. Empirical Project 7 Stewart, Kenneth G. 2018. Suits’ watermelon model: The missing simultaneous equations empirical application. Suits, Daniel B. 1955. ‘An econometric model of the watermelon market’. Journal of Farm Economics 37 (2): pp. 237–251. Empirical Project 8 Stam, Kirsten, Inge Sieben, Ellen Verbakel, and Paul M. de Graaf. 2016. ‘Employment status and subjective well-being: the role of the social norm to work’. Work, employment and society 30 (2): pp. 309–333. Empirical Project 9 Kedir, Abbi. 2003. ‘Determinants of access to credit and loan amount: household-level evidence from urban Ethiopia’. International Conference on African Development Archives. Empirical Project 10 Levine, Ross, Martin Čihák, Aslı Demirgüç-Kunt, Erik Feyen. 2012. ‘Benchmarking financial systems around the world’. The World Bank Policy Research Working Paper No WPS6175. Empirical Project 11 Abdullah, Sabah, Anil Markandya, Paulo A. L. D. Nunes. 2011. ‘Introduction to economic valuation methods’. In Research Tools In Natural Resource And Environmental Economics: pp. 143–187. World Scientific Publishing. University of Virginia Library. 2015. ‘Using and interpreting Cronbach’s Alpha’. Published 16 November 2015. Empirical Project 12 Hong Kong Business. 2011. ‘Government to start next phase of Scheme $6,000’. Published 26 September 2011. Lau, Stuart. 2013. ‘120,000 residents forgo cash handouts’. South China Morning Post. Updated 12 January 2013. Lum, Alvin. 2018. ‘Hong Kong cash handout scheme will cost government HK$330 million to administer’. South China Morning Post. Updated 21 April 2018."
});
index.addDoc({
    id: 70,
    title: "Doing Economics: Copyright acknowledgements",
    content: "Copyright acknowledgements Cover: Waves: Tuncay Coskun / Flikr.com We would like to acknowledge everybody who granted us permission to reproduce images, figures and quotations throughout this text. Every effort was made to trace copyright holders, but we will make arrangements to clear permission for material reproduced in this book with any copyright holders whom it has not been possible to contact."
});
